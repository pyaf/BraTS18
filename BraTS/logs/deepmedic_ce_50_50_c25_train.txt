2018-05-24 01:12:04,934 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:13:05,945 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:14:24,120 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:15:14,902 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:17:54,214 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:19:17,527 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:20:36,371 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:20:57,851 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:22:36,046 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:23:10,715 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:23:43,473 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:24:04,644 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:24:45,579 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:26:39,459 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:27:37,575 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:30:55,557 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:31:33,660 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int16, np.int16)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:33:27,308 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 33
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 16

2018-05-24 01:39:20,480 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 10

2018-05-24 01:40:40,280 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 10

2018-05-24 01:41:45,708 Iter 1, Epoch 0.2193, Loss 1.7640
2018-05-24 01:41:50,462 Iter 2, Epoch 0.4386, Loss 0.7252
2018-05-24 01:41:56,201 Iter 3, Epoch 0.6579, Loss 0.6226
2018-05-24 01:42:05,878 Iter 4, Epoch 0.8772, Loss 0.5041
2018-05-24 01:42:14,091 Iter 5, Epoch 1.0965, Loss 3.5227
2018-05-24 01:42:19,347 Iter 6, Epoch 1.3158, Loss 2.4327
2018-05-24 01:42:24,604 Iter 7, Epoch 1.5351, Loss 2.0094
2018-05-24 01:42:29,845 Iter 8, Epoch 1.7544, Loss 1.7541
2018-05-24 01:42:35,104 Iter 9, Epoch 1.9737, Loss 1.5140
2018-05-24 01:45:31,069 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 10

2018-05-24 01:46:50,014 Iter 1, Epoch 0.2193, Loss 1.7716
2018-05-24 01:46:55,035 Iter 2, Epoch 0.4386, Loss 0.7528
2018-05-24 01:47:00,273 Iter 3, Epoch 0.6579, Loss 0.5890
2018-05-24 01:47:08,783 Iter 4, Epoch 0.8772, Loss 0.5031
2018-05-24 01:47:19,737 Iter 5, Epoch 1.0965, Loss 0.4994
2018-05-24 01:47:24,818 Iter 6, Epoch 1.3158, Loss 0.5363
2018-05-24 01:47:30,081 Iter 7, Epoch 1.5351, Loss 0.5245
2018-05-24 01:47:35,361 Iter 8, Epoch 1.7544, Loss 0.4874
2018-05-24 01:47:40,612 Iter 9, Epoch 1.9737, Loss 0.4598
2018-05-24 01:47:45,858 Iter 10, Epoch 2.1930, Loss 0.5177
2018-05-24 01:47:52,389 Iter 11, Epoch 2.4123, Loss 0.4731
2018-05-24 01:47:58,784 Iter 12, Epoch 2.6316, Loss 0.5027
2018-05-24 01:48:04,024 Iter 13, Epoch 2.8509, Loss 0.5214
2018-05-24 01:48:09,695 Iter 14, Epoch 3.0702, Loss 3.0287
2018-05-24 01:48:15,673 Iter 15, Epoch 3.2895, Loss 1.7228
2018-05-24 01:48:21,266 Iter 16, Epoch 3.5088, Loss 1.5209
2018-05-24 01:48:26,486 Iter 17, Epoch 3.7281, Loss 1.3336
2018-05-24 01:48:32,495 Iter 18, Epoch 3.9474, Loss 1.1986
2018-05-24 01:48:38,144 Iter 19, Epoch 4.1667, Loss 1.2116
2018-05-24 01:48:44,173 Iter 20, Epoch 4.3860, Loss 1.1473
2018-05-24 01:48:49,440 Iter 21, Epoch 4.6053, Loss 1.1220
2018-05-24 01:48:55,058 Iter 22, Epoch 4.8246, Loss 1.1108
2018-05-24 01:49:00,538 Iter 23, Epoch 5.0439, Loss 1.1172
2018-05-24 01:49:06,001 Iter 24, Epoch 5.2632, Loss 1.1128
2018-05-24 01:49:12,289 Iter 25, Epoch 5.4825, Loss 1.1136
2018-05-24 01:49:17,521 Iter 26, Epoch 5.7018, Loss 1.1177
2018-05-24 01:49:39,604 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 8

2018-05-24 01:50:32,084 Iter 1, Epoch 0.2193, Loss 1.7427
2018-05-24 01:50:37,081 Iter 2, Epoch 0.4386, Loss 0.7303
2018-05-24 01:50:42,360 Iter 3, Epoch 0.6579, Loss 0.5985
2018-05-24 01:50:47,634 Iter 4, Epoch 0.8772, Loss 0.5604
2018-05-24 01:50:55,264 Iter 5, Epoch 1.0965, Loss 0.5272
2018-05-24 01:51:03,717 Iter 6, Epoch 1.3158, Loss 0.5675
2018-05-24 01:51:09,005 Iter 7, Epoch 1.5351, Loss 0.5544
2018-05-24 01:51:14,291 Iter 8, Epoch 1.7544, Loss 0.4999
2018-05-24 01:51:19,571 Iter 9, Epoch 1.9737, Loss 0.6476
2018-05-24 01:51:24,829 Iter 10, Epoch 2.1930, Loss 2.0417
2018-05-24 01:51:41,124 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 4

2018-05-24 01:52:30,117 Iter 1, Epoch 0.2193, Loss 1.7705
2018-05-24 01:52:34,787 Iter 2, Epoch 0.4386, Loss 0.6991
2018-05-24 01:52:40,035 Iter 3, Epoch 0.6579, Loss 0.6379
2018-05-24 01:52:45,584 Iter 4, Epoch 0.8772, Loss 0.5558
2018-05-24 01:53:11,112 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 01:54:02,124 Iter 1, Epoch 0.2193, Loss 1.7353
2018-05-24 01:54:07,082 Iter 2, Epoch 0.4386, Loss 0.6791
2018-05-24 01:54:12,416 Iter 3, Epoch 0.6579, Loss 1.1162
2018-05-24 01:54:18,169 Iter 4, Epoch 0.8772, Loss 3.7677
2018-05-24 01:54:26,478 Iter 5, Epoch 1.0965, Loss 2.2366
2018-05-24 01:54:31,910 Iter 6, Epoch 1.3158, Loss 2.0200
2018-05-24 01:54:37,253 Iter 7, Epoch 1.5351, Loss 1.7389
2018-05-24 01:54:42,594 Iter 8, Epoch 1.7544, Loss 1.5122
2018-05-24 01:54:47,911 Iter 9, Epoch 1.9737, Loss 1.3563
2018-05-24 01:54:54,585 Iter 10, Epoch 2.1930, Loss 1.3876
2018-05-24 01:55:07,448 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 01:55:59,634 Iter 1, Epoch 0.2193, Loss 1.7946
2018-05-24 01:56:04,466 Iter 2, Epoch 0.4386, Loss 0.6929
2018-05-24 01:56:09,722 Iter 3, Epoch 0.6579, Loss 1.0935
2018-05-24 01:56:15,293 Iter 4, Epoch 0.8772, Loss 3.8516
2018-05-24 01:57:10,612 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2001
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 01:58:01,608 Iter 1, Epoch 0.2193, Loss 1.8164
2018-05-24 01:58:06,336 Iter 2, Epoch 0.4386, Loss 0.7070
2018-05-24 01:58:11,605 Iter 3, Epoch 0.6579, Loss 0.6299
2018-05-24 01:58:17,661 Iter 4, Epoch 0.8772, Loss 0.5611
2018-05-24 01:58:26,162 Iter 5, Epoch 1.0965, Loss 0.5251
2018-05-24 01:58:31,430 Iter 6, Epoch 1.3158, Loss 0.5753
2018-05-24 01:58:36,703 Iter 7, Epoch 1.5351, Loss 0.4907
2018-05-24 01:58:41,976 Iter 8, Epoch 1.7544, Loss 0.4685
2018-05-24 01:58:47,245 Iter 9, Epoch 1.9737, Loss 0.4160
2018-05-24 01:58:53,451 Iter 10, Epoch 2.1930, Loss 0.4815
2018-05-24 01:58:59,802 Iter 11, Epoch 2.4123, Loss 0.6784
2018-05-24 01:59:05,063 Iter 12, Epoch 2.6316, Loss 2.8429
2018-05-24 01:59:10,314 Iter 13, Epoch 2.8509, Loss 1.5737
2018-05-24 01:59:15,566 Iter 14, Epoch 3.0702, Loss 1.3555
2018-05-24 01:59:21,140 Iter 15, Epoch 3.2895, Loss 1.1892
2018-05-24 01:59:26,632 Iter 16, Epoch 3.5088, Loss 1.1652
2018-05-24 01:59:33,462 Iter 17, Epoch 3.7281, Loss 1.1333
2018-05-24 01:59:38,710 Iter 18, Epoch 3.9474, Loss 1.1403
2018-05-24 01:59:43,957 Iter 19, Epoch 4.1667, Loss 1.1568
2018-05-24 01:59:49,206 Iter 20, Epoch 4.3860, Loss 1.1115
2018-05-24 01:59:54,798 Iter 21, Epoch 4.6053, Loss 1.1285
2018-05-24 02:00:00,416 Iter 22, Epoch 4.8246, Loss 1.1130
2018-05-24 02:00:07,022 Iter 23, Epoch 5.0439, Loss 1.1130
2018-05-24 02:00:12,267 Iter 24, Epoch 5.2632, Loss 1.1464
2018-05-24 02:00:17,514 Iter 25, Epoch 5.4825, Loss 1.1194
2018-05-24 02:00:29,584 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:01:24,865 Iter 1, Epoch 0.2193, Loss 1.3670
2018-05-24 02:01:29,484 Iter 2, Epoch 0.4386, Loss 0.6604
2018-05-24 02:01:34,671 Iter 3, Epoch 0.6579, Loss 0.5609
2018-05-24 02:01:42,725 Iter 4, Epoch 0.8772, Loss 0.4458
2018-05-24 02:01:48,557 Iter 5, Epoch 1.0965, Loss 0.4559
2018-05-24 02:01:53,824 Iter 6, Epoch 1.3158, Loss 0.4890
2018-05-24 02:01:59,091 Iter 7, Epoch 1.5351, Loss 0.4462
2018-05-24 02:02:04,340 Iter 8, Epoch 1.7544, Loss 2.8355
2018-05-24 02:02:09,820 Iter 9, Epoch 1.9737, Loss 1.5679
2018-05-24 02:02:15,885 Iter 10, Epoch 2.1930, Loss 1.2984
2018-05-24 02:02:21,916 Iter 11, Epoch 2.4123, Loss 1.1994
2018-05-24 02:02:27,158 Iter 12, Epoch 2.6316, Loss 1.1880
2018-05-24 02:02:41,617 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.0005
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:03:31,882 Iter 1, Epoch 0.2193, Loss 1.5699
2018-05-24 02:03:36,730 Iter 2, Epoch 0.4386, Loss 0.6990
2018-05-24 02:03:42,040 Iter 3, Epoch 0.6579, Loss 0.6398
2018-05-24 02:03:47,351 Iter 4, Epoch 0.8772, Loss 0.5079
2018-05-24 02:03:55,078 Iter 5, Epoch 1.0965, Loss 0.4873
2018-05-24 02:04:01,028 Iter 6, Epoch 1.3158, Loss 0.4685
2018-05-24 02:04:06,340 Iter 7, Epoch 1.5351, Loss 0.4886
2018-05-24 02:04:11,647 Iter 8, Epoch 1.7544, Loss 0.5418
2018-05-24 02:04:16,958 Iter 9, Epoch 1.9737, Loss 0.8197
2018-05-24 02:04:22,909 Iter 10, Epoch 2.1930, Loss 3.3554
2018-05-24 02:04:30,220 Iter 11, Epoch 2.4123, Loss 1.9217
2018-05-24 02:04:35,508 Iter 12, Epoch 2.6316, Loss 1.6523
2018-05-24 02:04:40,797 Iter 13, Epoch 2.8509, Loss 1.4584
2018-05-24 02:04:46,087 Iter 14, Epoch 3.0702, Loss 1.3695
2018-05-24 02:05:37,026 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:06:28,846 Iter 1, Epoch 0.2193, Loss 3.1627
2018-05-24 02:06:33,465 Iter 2, Epoch 0.4386, Loss 2.7087
2018-05-24 02:06:38,671 Iter 3, Epoch 0.6579, Loss 1.4340
2018-05-24 02:06:44,454 Iter 4, Epoch 0.8772, Loss 1.3099
2018-05-24 02:06:52,480 Iter 5, Epoch 1.0965, Loss 1.4225
2018-05-24 02:06:57,728 Iter 6, Epoch 1.3158, Loss 1.0625
2018-05-24 02:07:02,975 Iter 7, Epoch 1.5351, Loss 2.1266
2018-05-24 02:07:08,219 Iter 8, Epoch 1.7544, Loss 1.9388
2018-05-24 02:07:13,463 Iter 9, Epoch 1.9737, Loss 1.7053
2018-05-24 02:07:19,738 Iter 10, Epoch 2.1930, Loss 1.5204
2018-05-24 02:07:25,519 Iter 11, Epoch 2.4123, Loss 1.4254
2018-05-24 02:07:31,056 Iter 12, Epoch 2.6316, Loss 1.3973
2018-05-24 02:07:36,299 Iter 13, Epoch 2.8509, Loss 1.2459
2018-05-24 02:07:41,917 Iter 14, Epoch 3.0702, Loss 1.2536
2018-05-24 02:07:47,417 Iter 15, Epoch 3.2895, Loss 1.2150
2018-05-24 02:07:52,919 Iter 16, Epoch 3.5088, Loss 1.1826
2018-05-24 02:07:59,459 Iter 17, Epoch 3.7281, Loss 1.1568
2018-05-24 02:08:04,703 Iter 18, Epoch 3.9474, Loss 1.1582
2018-05-24 02:08:09,948 Iter 19, Epoch 4.1667, Loss 1.1354
2018-05-24 02:08:15,193 Iter 20, Epoch 4.3860, Loss 1.1154
2018-05-24 02:08:21,101 Iter 21, Epoch 4.6053, Loss 1.1670
2018-05-24 02:08:26,814 Iter 22, Epoch 4.8246, Loss 1.1392
2018-05-24 02:08:32,375 Iter 23, Epoch 5.0439, Loss 1.1441
2018-05-24 02:08:37,920 Iter 24, Epoch 5.2632, Loss 1.1349
2018-05-24 02:08:43,168 Iter 25, Epoch 5.4825, Loss 1.1229
2018-05-24 02:08:48,644 Iter 26, Epoch 5.7018, Loss 1.0885
2018-05-24 02:08:54,227 Iter 27, Epoch 5.9211, Loss 1.1133
2018-05-24 02:08:59,888 Iter 28, Epoch 6.1404, Loss 1.0824
2018-05-24 02:09:05,468 Iter 29, Epoch 6.3596, Loss 1.1026
2018-05-24 02:09:11,264 Iter 30, Epoch 6.5789, Loss 1.1109
2018-05-24 02:09:17,060 Iter 31, Epoch 6.7982, Loss 1.1225
2018-05-24 02:09:22,305 Iter 32, Epoch 7.0175, Loss 1.1008
2018-05-24 02:09:27,820 Iter 33, Epoch 7.2368, Loss 1.1231
2018-05-24 02:09:33,867 Iter 34, Epoch 7.4561, Loss 1.1264
2018-05-24 02:09:39,644 Iter 35, Epoch 7.6754, Loss 1.1094
2018-05-24 02:09:45,243 Iter 36, Epoch 7.8947, Loss 1.0843
2018-05-24 02:09:50,489 Iter 37, Epoch 8.1140, Loss 1.1206
2018-05-24 02:09:56,069 Iter 38, Epoch 8.3333, Loss 1.1225
2018-05-24 02:10:01,520 Iter 39, Epoch 8.5526, Loss 1.1405
2018-05-24 02:10:07,377 Iter 40, Epoch 8.7719, Loss 1.1165
2018-05-24 02:10:14,247 Iter 41, Epoch 8.9912, Loss 1.0774
2018-05-24 02:10:19,494 Iter 42, Epoch 9.2105, Loss 1.0926
2018-05-24 02:10:24,742 Iter 43, Epoch 9.4298, Loss 1.1237
2018-05-24 02:10:30,263 Iter 44, Epoch 9.6491, Loss 1.1263
2018-05-24 02:10:36,381 Iter 45, Epoch 9.8684, Loss 1.1047
2018-05-24 02:10:42,105 Iter 46, Epoch 10.0877, Loss 1.1104
2018-05-24 02:11:01,913 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:11:55,114 Iter 1, Epoch 0.2193, Loss 1.3033
2018-05-24 02:12:00,002 Iter 2, Epoch 0.4386, Loss 0.7176
2018-05-24 02:12:05,265 Iter 3, Epoch 0.6579, Loss 0.6245
2018-05-24 02:12:11,299 Iter 4, Epoch 0.8772, Loss 0.4842
2018-05-24 02:12:19,401 Iter 5, Epoch 1.0965, Loss 0.4758
2018-05-24 02:12:24,667 Iter 6, Epoch 1.3158, Loss 0.5081
2018-05-24 02:12:29,934 Iter 7, Epoch 1.5351, Loss 0.4804
2018-05-24 02:12:35,178 Iter 8, Epoch 1.7544, Loss 2.8648
2018-05-24 02:12:40,821 Iter 9, Epoch 1.9737, Loss 1.6570
2018-05-24 02:12:47,401 Iter 10, Epoch 2.1930, Loss 1.3300
2018-05-24 02:12:52,646 Iter 11, Epoch 2.4123, Loss 1.2118
2018-05-24 02:12:58,320 Iter 12, Epoch 2.6316, Loss 1.2143
2018-05-24 02:13:03,562 Iter 13, Epoch 2.8509, Loss 1.1235
2018-05-24 02:13:08,802 Iter 14, Epoch 3.0702, Loss 1.1426
2018-05-24 02:13:14,693 Iter 15, Epoch 3.2895, Loss 1.1448
2018-05-24 02:13:21,078 Iter 16, Epoch 3.5088, Loss 1.1023
2018-05-24 02:13:26,320 Iter 17, Epoch 3.7281, Loss 1.0927
2018-05-24 02:13:31,781 Iter 18, Epoch 3.9474, Loss 1.1120
2018-05-24 02:13:37,023 Iter 19, Epoch 4.1667, Loss 1.0939
2018-05-24 02:13:42,270 Iter 20, Epoch 4.3860, Loss 1.0894
2018-05-24 02:13:48,288 Iter 21, Epoch 4.6053, Loss 1.1228
2018-05-24 02:13:54,628 Iter 22, Epoch 4.8246, Loss 1.1080
2018-05-24 02:13:59,869 Iter 23, Epoch 5.0439, Loss 1.1161
2018-05-24 02:14:13,655 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.0001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:15:05,422 Iter 1, Epoch 0.2193, Loss 2.9495
2018-05-24 02:15:10,213 Iter 2, Epoch 0.4386, Loss 1.0620
2018-05-24 02:15:15,473 Iter 3, Epoch 0.6579, Loss 0.8335
2018-05-24 02:15:21,294 Iter 4, Epoch 0.8772, Loss 0.6805
2018-05-24 02:15:29,694 Iter 5, Epoch 1.0965, Loss 0.6490
2018-05-24 02:15:34,955 Iter 6, Epoch 1.3158, Loss 0.6357
2018-05-24 02:15:40,221 Iter 7, Epoch 1.5351, Loss 0.5946
2018-05-24 02:15:45,468 Iter 8, Epoch 1.7544, Loss 5.3416
2018-05-24 02:15:50,708 Iter 9, Epoch 1.9737, Loss 4.9029
2018-05-24 02:15:57,116 Iter 10, Epoch 2.1930, Loss 4.2498
2018-05-24 02:16:02,661 Iter 11, Epoch 2.4123, Loss 3.5311
2018-05-24 02:16:08,388 Iter 12, Epoch 2.6316, Loss 2.9748
2018-05-24 02:16:13,626 Iter 13, Epoch 2.8509, Loss 2.7466
2018-05-24 02:16:18,865 Iter 14, Epoch 3.0702, Loss 2.3847
2018-05-24 02:16:24,097 Iter 15, Epoch 3.2895, Loss 2.1078
2018-05-24 02:16:31,166 Iter 16, Epoch 3.5088, Loss 1.9952
2018-05-24 02:16:36,766 Iter 17, Epoch 3.7281, Loss 1.7042
2018-05-24 02:16:41,997 Iter 18, Epoch 3.9474, Loss 1.6355
2018-05-24 02:16:47,234 Iter 19, Epoch 4.1667, Loss 1.5211
2018-05-24 02:16:52,473 Iter 20, Epoch 4.3860, Loss 1.4510
2018-05-24 02:16:58,275 Iter 21, Epoch 4.6053, Loss 1.4339
2018-05-24 02:17:04,626 Iter 22, Epoch 4.8246, Loss 1.3651
2018-05-24 02:17:10,195 Iter 23, Epoch 5.0439, Loss 1.3611
2018-05-24 02:17:15,430 Iter 24, Epoch 5.2632, Loss 1.3177
2018-05-24 02:17:20,667 Iter 25, Epoch 5.4825, Loss 1.3105
2018-05-24 02:17:25,904 Iter 26, Epoch 5.7018, Loss 1.2732
2018-05-24 02:17:31,910 Iter 27, Epoch 5.9211, Loss 1.2773
2018-05-24 02:17:38,253 Iter 28, Epoch 6.1404, Loss 1.2186
2018-05-24 02:17:43,487 Iter 29, Epoch 6.3596, Loss 1.2299
2018-05-24 02:17:48,998 Iter 30, Epoch 6.5789, Loss 1.2181
2018-05-24 02:17:54,232 Iter 31, Epoch 6.7982, Loss 1.2211
2018-05-24 02:17:59,466 Iter 32, Epoch 7.0175, Loss 1.1960
2018-05-24 02:18:05,852 Iter 33, Epoch 7.2368, Loss 1.2009
2018-05-24 02:18:11,800 Iter 34, Epoch 7.4561, Loss 1.2055
2018-05-24 02:18:17,031 Iter 35, Epoch 7.6754, Loss 1.1586
2018-05-24 02:18:22,635 Iter 36, Epoch 7.8947, Loss 1.1740
2018-05-24 02:18:27,868 Iter 37, Epoch 8.1140, Loss 1.1709
2018-05-24 02:18:33,620 Iter 38, Epoch 8.3333, Loss 1.2042
2018-05-24 02:18:39,095 Iter 39, Epoch 8.5526, Loss 1.2750
2018-05-24 02:18:45,565 Iter 40, Epoch 8.7719, Loss 1.2219
2018-05-24 02:18:50,793 Iter 41, Epoch 8.9912, Loss 1.1952
2018-05-24 02:18:56,256 Iter 42, Epoch 9.2105, Loss 1.1652
2018-05-24 02:19:01,493 Iter 43, Epoch 9.4298, Loss 1.1517
2018-05-24 02:19:07,229 Iter 44, Epoch 9.6491, Loss 1.1981
2018-05-24 02:19:12,657 Iter 45, Epoch 9.8684, Loss 1.1592
2018-05-24 02:19:18,753 Iter 46, Epoch 10.0877, Loss 1.1417
2018-05-24 02:20:36,407 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.0001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:21:23,328 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: TensorType((torch.float32, torch.float32, torch.int64, torch.int64,
  torch.int64))
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:22:14,983 Iter 1, Epoch 0.2193, Loss 1.5557
2018-05-24 02:22:19,677 Iter 2, Epoch 0.4386, Loss 0.7091
2018-05-24 02:22:24,936 Iter 3, Epoch 0.6579, Loss 0.6203
2018-05-24 02:22:30,482 Iter 4, Epoch 0.8772, Loss 0.4560
2018-05-24 02:22:36,669 Iter 5, Epoch 1.0965, Loss 0.4994
2018-05-24 02:22:43,601 Iter 6, Epoch 1.3158, Loss 0.5028
2018-05-24 02:22:48,874 Iter 7, Epoch 1.5351, Loss 0.4622
2018-05-24 02:22:54,125 Iter 8, Epoch 1.7544, Loss 0.4216
2018-05-24 02:22:59,788 Iter 9, Epoch 1.9737, Loss 0.4193
2018-05-24 02:23:05,061 Iter 10, Epoch 2.1930, Loss 0.4419
2018-05-24 02:23:11,413 Iter 11, Epoch 2.4123, Loss 0.4405
2018-05-24 02:23:16,935 Iter 12, Epoch 2.6316, Loss 0.3787
2018-05-24 02:23:22,208 Iter 13, Epoch 2.8509, Loss 0.3828
2018-05-24 02:23:27,781 Iter 14, Epoch 3.0702, Loss 0.4610
2018-05-24 02:23:33,041 Iter 15, Epoch 3.2895, Loss 0.4174
2018-05-24 02:23:38,315 Iter 16, Epoch 3.5088, Loss 0.4106
2018-05-24 02:23:52,280 -------------- New training session ----------------
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '2'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: {}
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 0
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 02:24:46,347 Iter 1, Epoch 0.2193, Loss 1.4243
2018-05-24 02:24:51,056 Iter 2, Epoch 0.4386, Loss 0.7287
2018-05-24 02:24:56,308 Iter 3, Epoch 0.6579, Loss 0.5445
2018-05-24 02:25:02,318 Iter 4, Epoch 0.8772, Loss 0.4855
2018-05-24 02:25:10,884 Iter 5, Epoch 1.0965, Loss 0.4723
2018-05-24 02:25:16,161 Iter 6, Epoch 1.3158, Loss 0.4992
2018-05-24 02:25:21,428 Iter 7, Epoch 1.5351, Loss 0.4472
2018-05-24 02:25:26,696 Iter 8, Epoch 1.7544, Loss 0.4821
2018-05-24 02:25:32,684 Iter 9, Epoch 1.9737, Loss 0.4195
2018-05-24 02:25:38,959 Iter 10, Epoch 2.1930, Loss 0.4182
2018-05-24 02:25:44,224 Iter 11, Epoch 2.4123, Loss 0.4422
2018-05-24 02:25:49,917 Iter 12, Epoch 2.6316, Loss 0.4253
2018-05-24 02:25:55,188 Iter 13, Epoch 2.8509, Loss 0.4328
2018-05-24 02:26:00,973 Iter 14, Epoch 3.0702, Loss 0.4625
2018-05-24 02:26:06,676 Iter 15, Epoch 3.2895, Loss 0.4232
2018-05-24 02:26:12,579 Iter 16, Epoch 3.5088, Loss 0.4158
2018-05-24 02:26:18,120 Iter 17, Epoch 3.7281, Loss 0.4103
2018-05-24 02:26:23,670 Iter 18, Epoch 3.9474, Loss 0.3909
2018-05-24 02:26:28,940 Iter 19, Epoch 4.1667, Loss 0.3646
2018-05-24 02:26:34,501 Iter 20, Epoch 4.3860, Loss 0.4066
2018-05-24 02:26:39,962 Iter 21, Epoch 4.6053, Loss 0.3875
2018-05-24 02:26:46,510 Iter 22, Epoch 4.8246, Loss 0.4097
2018-05-24 02:26:52,191 Iter 23, Epoch 5.0439, Loss 0.3768
2018-05-24 02:26:57,455 Iter 24, Epoch 5.2632, Loss 0.3734
2018-05-24 02:27:02,717 Iter 25, Epoch 5.4825, Loss 0.3909
2018-05-24 02:27:07,985 Iter 26, Epoch 5.7018, Loss 0.3527
2018-05-24 02:27:14,094 Iter 27, Epoch 5.9211, Loss 0.4202
2018-05-24 02:27:20,209 Iter 28, Epoch 6.1404, Loss 0.3875
2018-05-24 02:27:25,690 Iter 29, Epoch 6.3596, Loss 0.4045
2018-05-24 02:27:30,959 Iter 30, Epoch 6.5789, Loss 0.3441
2018-05-24 02:27:36,228 Iter 31, Epoch 6.7982, Loss 0.3776
2018-05-24 02:27:41,868 Iter 32, Epoch 7.0175, Loss 0.3970
2018-05-24 02:27:47,303 Iter 33, Epoch 7.2368, Loss 0.3683
2018-05-24 02:27:53,835 Iter 34, Epoch 7.4561, Loss 0.3788
2018-05-24 02:27:59,387 Iter 35, Epoch 7.6754, Loss 0.3548
2018-05-24 02:28:04,655 Iter 36, Epoch 7.8947, Loss 0.4000
2018-05-24 02:28:09,920 Iter 37, Epoch 8.1140, Loss 0.3583
2018-05-24 02:28:15,187 Iter 38, Epoch 8.3333, Loss 0.3498
2018-05-24 02:28:21,969 Iter 39, Epoch 8.5526, Loss 0.3698
2018-05-24 02:28:27,232 Iter 40, Epoch 8.7719, Loss 0.3993
2018-05-24 02:28:32,976 Iter 41, Epoch 8.9912, Loss 0.3804
2018-05-24 02:28:38,242 Iter 42, Epoch 9.2105, Loss 0.3892
2018-05-24 02:28:43,506 Iter 43, Epoch 9.4298, Loss 0.3880
2018-05-24 02:28:49,171 Iter 44, Epoch 9.6491, Loss 0.3305
2018-05-24 02:28:55,572 Iter 45, Epoch 9.8684, Loss 0.3922
2018-05-24 02:29:00,842 Iter 46, Epoch 10.0877, Loss 0.3496
2018-05-24 02:29:06,794 Iter 47, Epoch 10.3070, Loss 0.3987
2018-05-24 02:29:12,058 Iter 48, Epoch 10.5263, Loss 0.3753
2018-05-24 02:29:17,322 Iter 49, Epoch 10.7456, Loss 0.3841
2018-05-24 02:29:22,907 Iter 50, Epoch 10.9649, Loss 0.3453
2018-05-24 02:29:29,340 Iter 51, Epoch 11.1842, Loss 0.3799
2018-05-24 02:29:34,869 Iter 52, Epoch 11.4035, Loss 0.3077
2018-05-24 02:29:40,142 Iter 53, Epoch 11.6228, Loss 0.3781
2018-05-24 02:29:46,045 Iter 54, Epoch 11.8421, Loss 0.3617
2018-05-24 02:29:51,312 Iter 55, Epoch 12.0614, Loss 0.3655
2018-05-24 02:29:57,474 Iter 56, Epoch 12.2807, Loss 0.4129
2018-05-24 02:30:03,142 Iter 57, Epoch 12.5000, Loss 0.3134
2018-05-24 02:30:08,774 Iter 58, Epoch 12.7193, Loss 0.3360
2018-05-24 02:30:14,429 Iter 59, Epoch 12.9386, Loss 0.3382
2018-05-24 02:30:19,696 Iter 60, Epoch 13.1579, Loss 0.3520
2018-05-24 02:30:25,742 Iter 61, Epoch 13.3772, Loss 0.3649
2018-05-24 02:30:31,216 Iter 62, Epoch 13.5965, Loss 0.3551
2018-05-24 02:30:36,905 Iter 63, Epoch 13.8158, Loss 0.3444
2018-05-24 02:30:42,174 Iter 64, Epoch 14.0351, Loss 0.3605
2018-05-24 02:30:47,684 Iter 65, Epoch 14.2544, Loss 0.3667
2018-05-24 02:30:53,584 Iter 66, Epoch 14.4737, Loss 0.3753
2018-05-24 02:30:59,132 Iter 67, Epoch 14.6930, Loss 0.3404
2018-05-24 02:31:04,398 Iter 68, Epoch 14.9123, Loss 0.3468
2018-05-24 02:31:11,361 Iter 69, Epoch 15.1316, Loss 0.3368
2018-05-24 02:31:16,625 Iter 70, Epoch 15.3509, Loss 0.3648
2018-05-24 02:31:22,183 Iter 71, Epoch 15.5702, Loss 0.3525
2018-05-24 02:31:27,451 Iter 72, Epoch 15.7895, Loss 0.3711
2018-05-24 02:31:33,235 Iter 73, Epoch 16.0088, Loss 0.3743
2018-05-24 02:31:39,962 Iter 74, Epoch 16.2281, Loss 0.3009
2018-05-24 02:31:45,226 Iter 75, Epoch 16.4474, Loss 0.3422
2018-05-24 02:31:50,768 Iter 76, Epoch 16.6667, Loss 0.3702
2018-05-24 02:31:56,406 Iter 77, Epoch 16.8860, Loss 0.3558
2018-05-24 02:32:02,181 Iter 78, Epoch 17.1053, Loss 0.3455
2018-05-24 02:32:07,445 Iter 79, Epoch 17.3246, Loss 0.3486
2018-05-24 02:32:13,598 Iter 80, Epoch 17.5439, Loss 0.3556
2018-05-24 02:32:19,330 Iter 81, Epoch 17.7632, Loss 0.3664
2018-05-24 02:32:24,895 Iter 82, Epoch 17.9825, Loss 0.3268
2018-05-24 02:32:30,160 Iter 83, Epoch 18.2018, Loss 0.3323
2018-05-24 02:32:36,055 Iter 84, Epoch 18.4211, Loss 0.3590
2018-05-24 02:32:41,768 Iter 85, Epoch 18.6404, Loss 0.3445
2018-05-24 02:32:47,503 Iter 86, Epoch 18.8596, Loss 0.2992
2018-05-24 02:32:53,053 Iter 87, Epoch 19.0789, Loss 0.3730
2018-05-24 02:32:58,742 Iter 88, Epoch 19.2982, Loss 0.3322
2018-05-24 02:33:04,003 Iter 89, Epoch 19.5175, Loss 0.3278
2018-05-24 02:33:09,933 Iter 90, Epoch 19.7368, Loss 0.3417
2018-05-24 02:33:15,192 Iter 91, Epoch 19.9561, Loss 0.3344
2018-05-24 02:33:21,404 Iter 92, Epoch 20.1754, Loss 0.3456
2018-05-24 02:33:26,660 Iter 93, Epoch 20.3947, Loss 0.3297
2018-05-24 02:33:32,335 Iter 94, Epoch 20.6140, Loss 0.3273
2018-05-24 02:33:37,593 Iter 95, Epoch 20.8333, Loss 0.3436
2018-05-24 02:33:43,252 Iter 96, Epoch 21.0526, Loss 0.3419
2018-05-24 02:33:49,234 Iter 97, Epoch 21.2719, Loss 0.3674
2018-05-24 02:33:54,856 Iter 98, Epoch 21.4912, Loss 0.3204
2018-05-24 02:34:00,480 Iter 99, Epoch 21.7105, Loss 0.3521
2018-05-24 02:34:05,739 Iter 100, Epoch 21.9298, Loss 0.2908
2018-05-24 02:34:11,297 Iter 101, Epoch 22.1491, Loss 0.2871
2018-05-24 02:34:17,426 Iter 102, Epoch 22.3684, Loss 0.3176
2018-05-24 02:34:23,157 Iter 103, Epoch 22.5877, Loss 0.3234
2018-05-24 02:34:28,674 Iter 104, Epoch 22.8070, Loss 0.3359
2018-05-24 02:34:34,348 Iter 105, Epoch 23.0263, Loss 0.3448
2018-05-24 02:34:40,024 Iter 106, Epoch 23.2456, Loss 0.3138
2018-05-24 02:34:45,253 Iter 107, Epoch 23.4649, Loss 0.3179
2018-05-24 02:34:50,715 Iter 108, Epoch 23.6842, Loss 0.3100
2018-05-24 02:34:56,336 Iter 109, Epoch 23.9035, Loss 0.3337
2018-05-24 02:35:01,993 Iter 110, Epoch 24.1228, Loss 0.3038
2018-05-24 02:35:07,910 Iter 111, Epoch 24.3421, Loss 0.2943
2018-05-24 02:35:13,527 Iter 112, Epoch 24.5614, Loss 0.3581
2018-05-24 02:35:19,324 Iter 113, Epoch 24.7807, Loss 0.3054
2018-05-24 02:35:24,588 Iter 114, Epoch 25.0000, Loss 0.3570
2018-05-24 02:35:30,262 Iter 115, Epoch 25.2193, Loss 0.3080
2018-05-24 02:35:35,498 Iter 116, Epoch 25.4386, Loss 0.3455
2018-05-24 02:35:41,247 Iter 117, Epoch 25.6579, Loss 0.3438
2018-05-24 02:35:47,226 Iter 118, Epoch 25.8772, Loss 0.3214
2018-05-24 02:35:52,450 Iter 119, Epoch 26.0965, Loss 0.3034
2018-05-24 02:35:57,999 Iter 120, Epoch 26.3158, Loss 0.3589
2018-05-24 02:36:03,579 Iter 121, Epoch 26.5351, Loss 0.3077
2018-05-24 02:36:08,842 Iter 122, Epoch 26.7544, Loss 0.3179
2018-05-24 02:36:14,987 Iter 123, Epoch 26.9737, Loss 0.3068
2018-05-24 02:36:20,588 Iter 124, Epoch 27.1930, Loss 0.2841
2018-05-24 02:36:26,273 Iter 125, Epoch 27.4123, Loss 0.3166
2018-05-24 02:36:31,915 Iter 126, Epoch 27.6316, Loss 0.3578
2018-05-24 02:36:37,462 Iter 127, Epoch 27.8509, Loss 0.3608
2018-05-24 02:36:42,724 Iter 128, Epoch 28.0702, Loss 0.3208
2018-05-24 02:36:48,708 Iter 129, Epoch 28.2895, Loss 0.3228
2018-05-24 02:36:54,348 Iter 130, Epoch 28.5088, Loss 0.2939
2018-05-24 02:36:59,610 Iter 131, Epoch 28.7281, Loss 0.2970
2018-05-24 02:37:05,677 Iter 132, Epoch 28.9474, Loss 0.3291
2018-05-24 02:37:11,391 Iter 133, Epoch 29.1667, Loss 0.3270
2018-05-24 02:37:16,653 Iter 134, Epoch 29.3860, Loss 0.3285
2018-05-24 02:37:22,800 Iter 135, Epoch 29.6053, Loss 0.3094
2018-05-24 02:37:28,475 Iter 136, Epoch 29.8246, Loss 0.3411
2018-05-24 02:37:33,742 Iter 137, Epoch 30.0439, Loss 0.3033
2018-05-24 02:37:39,769 Iter 138, Epoch 30.2632, Loss 0.3363
2018-05-24 02:37:45,454 Iter 139, Epoch 30.4825, Loss 0.2888
2018-05-24 02:37:50,717 Iter 140, Epoch 30.7018, Loss 0.3207
2018-05-24 02:37:57,080 Iter 141, Epoch 30.9211, Loss 0.3111
2018-05-24 02:38:02,625 Iter 142, Epoch 31.1404, Loss 0.3023
2018-05-24 02:38:07,890 Iter 143, Epoch 31.3596, Loss 0.3217
2018-05-24 02:38:13,154 Iter 144, Epoch 31.5789, Loss 0.3349
2018-05-24 02:38:19,215 Iter 145, Epoch 31.7982, Loss 0.3295
2018-05-24 02:38:24,838 Iter 146, Epoch 32.0175, Loss 0.3183
2018-05-24 02:38:30,356 Iter 147, Epoch 32.2368, Loss 0.2894
2018-05-24 02:38:36,272 Iter 148, Epoch 32.4561, Loss 0.3033
2018-05-24 02:38:41,539 Iter 149, Epoch 32.6754, Loss 0.2499
2018-05-24 02:38:47,195 Iter 150, Epoch 32.8947, Loss 0.3142
2018-05-24 02:38:52,906 Iter 151, Epoch 33.1140, Loss 0.3882
2018-05-24 02:38:58,171 Iter 152, Epoch 33.3333, Loss 0.3143
2018-05-24 02:39:04,049 Iter 153, Epoch 33.5526, Loss 0.3147
2018-05-24 02:39:09,318 Iter 154, Epoch 33.7719, Loss 0.2985
2018-05-24 02:39:15,031 Iter 155, Epoch 33.9912, Loss 0.3076
2018-05-24 02:39:21,058 Iter 156, Epoch 34.2105, Loss 0.3061
2018-05-24 02:39:26,326 Iter 157, Epoch 34.4298, Loss 0.3252
2018-05-24 02:39:31,921 Iter 158, Epoch 34.6491, Loss 0.2877
2018-05-24 02:39:37,702 Iter 159, Epoch 34.8684, Loss 0.2912
2018-05-24 02:39:42,973 Iter 160, Epoch 35.0877, Loss 0.3136
2018-05-24 02:39:48,649 Iter 161, Epoch 35.3070, Loss 0.2949
2018-05-24 02:39:54,224 Iter 162, Epoch 35.5263, Loss 0.3166
2018-05-24 02:40:00,150 Iter 163, Epoch 35.7456, Loss 0.2904
2018-05-24 02:40:06,112 Iter 164, Epoch 35.9649, Loss 0.3263
2018-05-24 02:40:11,755 Iter 165, Epoch 36.1842, Loss 0.2869
2018-05-24 02:40:17,019 Iter 166, Epoch 36.4035, Loss 0.3166
2018-05-24 02:40:22,545 Iter 167, Epoch 36.6228, Loss 0.2905
2018-05-24 02:40:28,194 Iter 168, Epoch 36.8421, Loss 0.2881
2018-05-24 02:40:33,755 Iter 169, Epoch 37.0614, Loss 0.3347
2018-05-24 02:40:39,021 Iter 170, Epoch 37.2807, Loss 0.3112
2018-05-24 02:40:45,271 Iter 171, Epoch 37.5000, Loss 0.3067
2018-05-24 02:40:50,535 Iter 172, Epoch 37.7193, Loss 0.3274
2018-05-24 02:40:56,604 Iter 173, Epoch 37.9386, Loss 0.2843
2018-05-24 02:41:01,867 Iter 174, Epoch 38.1579, Loss 0.2873
2018-05-24 02:41:07,506 Iter 175, Epoch 38.3772, Loss 0.3252
2018-05-24 02:41:13,076 Iter 176, Epoch 38.5965, Loss 0.2980
2018-05-24 02:41:18,634 Iter 177, Epoch 38.8158, Loss 0.3195
2018-05-24 02:41:24,424 Iter 178, Epoch 39.0351, Loss 0.3132
2018-05-24 02:41:29,675 Iter 179, Epoch 39.2544, Loss 0.2859
2018-05-24 02:41:35,228 Iter 180, Epoch 39.4737, Loss 0.3178
2018-05-24 02:41:41,259 Iter 181, Epoch 39.6930, Loss 0.2836
2018-05-24 02:41:46,522 Iter 182, Epoch 39.9123, Loss 0.3325
2018-05-24 02:41:52,537 Iter 183, Epoch 40.1316, Loss 0.3044
2018-05-24 02:41:58,172 Iter 184, Epoch 40.3509, Loss 0.3274
2018-05-24 02:42:03,899 Iter 185, Epoch 40.5702, Loss 0.2808
2018-05-24 02:42:09,157 Iter 186, Epoch 40.7895, Loss 0.3175
2018-05-24 02:42:15,060 Iter 187, Epoch 41.0088, Loss 0.3251
2018-05-24 02:42:20,325 Iter 188, Epoch 41.2281, Loss 0.3042
2018-05-24 02:42:26,202 Iter 189, Epoch 41.4474, Loss 0.3044
2018-05-24 02:42:31,466 Iter 190, Epoch 41.6667, Loss 0.3046
2018-05-24 02:42:37,542 Iter 191, Epoch 41.8860, Loss 0.3078
2018-05-24 02:42:43,036 Iter 192, Epoch 42.1053, Loss 0.3072
2018-05-24 02:42:48,811 Iter 193, Epoch 42.3246, Loss 0.3269
2018-05-24 02:42:54,625 Iter 194, Epoch 42.5439, Loss 0.2774
2018-05-24 02:43:00,378 Iter 195, Epoch 42.7632, Loss 0.3262
2018-05-24 02:43:06,060 Iter 196, Epoch 42.9825, Loss 0.2820
2018-05-24 02:43:11,322 Iter 197, Epoch 43.2018, Loss 0.3091
2018-05-24 02:43:16,853 Iter 198, Epoch 43.4211, Loss 0.2857
2018-05-24 02:43:23,059 Iter 199, Epoch 43.6404, Loss 0.2976
2018-05-24 02:43:28,565 Iter 200, Epoch 43.8596, Loss 0.3097
2018-05-24 02:43:34,183 Iter 201, Epoch 44.0789, Loss 0.2777
2018-05-24 02:43:39,754 Iter 202, Epoch 44.2982, Loss 0.2854
2018-05-24 02:43:45,420 Iter 203, Epoch 44.5175, Loss 0.2982
2018-05-24 02:43:50,682 Iter 204, Epoch 44.7368, Loss 0.3263
2018-05-24 02:43:56,521 Iter 205, Epoch 44.9561, Loss 0.2923
2018-05-24 02:44:02,710 Iter 206, Epoch 45.1754, Loss 0.2966
2018-05-24 02:44:08,182 Iter 207, Epoch 45.3947, Loss 0.2642
2018-05-24 02:44:13,442 Iter 208, Epoch 45.6140, Loss 0.2745
2018-05-24 02:44:18,709 Iter 209, Epoch 45.8333, Loss 0.3035
2018-05-24 02:44:24,477 Iter 210, Epoch 46.0526, Loss 0.2935
2018-05-24 02:44:30,465 Iter 211, Epoch 46.2719, Loss 0.2799
2018-05-24 02:44:36,114 Iter 212, Epoch 46.4912, Loss 0.2734
2018-05-24 02:44:41,730 Iter 213, Epoch 46.7105, Loss 0.2959
2018-05-24 02:44:47,222 Iter 214, Epoch 46.9298, Loss 0.2935
2018-05-24 02:44:52,488 Iter 215, Epoch 47.1491, Loss 0.3143
2018-05-24 02:44:57,991 Iter 216, Epoch 47.3684, Loss 0.2957
2018-05-24 02:45:04,072 Iter 217, Epoch 47.5877, Loss 0.3025
2018-05-24 02:45:09,616 Iter 218, Epoch 47.8070, Loss 0.3210
2018-05-24 02:45:15,174 Iter 219, Epoch 48.0263, Loss 0.2719
2018-05-24 02:45:20,790 Iter 220, Epoch 48.2456, Loss 0.2858
2018-05-24 02:45:26,050 Iter 221, Epoch 48.4649, Loss 0.2989
2018-05-24 02:45:31,704 Iter 222, Epoch 48.6842, Loss 0.2653
2018-05-24 02:45:37,268 Iter 223, Epoch 48.9035, Loss 0.2843
2018-05-24 02:45:43,425 Iter 224, Epoch 49.1228, Loss 0.2965
2018-05-24 02:45:48,982 Iter 225, Epoch 49.3421, Loss 0.2663
2018-05-24 02:45:54,483 Iter 226, Epoch 49.5614, Loss 0.2798
2018-05-24 02:46:00,282 Iter 227, Epoch 49.7807, Loss 0.3043
2018-05-24 02:46:06,126 Iter 228, Epoch 50.0000, Loss 0.3022
2018-05-24 02:46:11,388 Iter 229, Epoch 50.2193, Loss 0.2725
2018-05-24 02:46:16,650 Iter 230, Epoch 50.4386, Loss 0.2902
2018-05-24 02:46:22,714 Iter 231, Epoch 50.6579, Loss 0.2745
2018-05-24 02:46:27,976 Iter 232, Epoch 50.8772, Loss 0.2858
2018-05-24 02:46:33,242 Iter 233, Epoch 51.0965, Loss 0.2978
2018-05-24 02:46:38,786 Iter 234, Epoch 51.3158, Loss 0.2628
2018-05-24 02:46:44,253 Iter 235, Epoch 51.5351, Loss 0.2954
2018-05-24 02:46:50,180 Iter 236, Epoch 51.7544, Loss 0.2897
2018-05-24 02:46:55,562 Iter 237, Epoch 51.9737, Loss 0.2764
2018-05-24 02:47:00,828 Iter 238, Epoch 52.1930, Loss 0.2895
2018-05-24 02:47:06,901 Iter 239, Epoch 52.4123, Loss 0.2905
2018-05-24 02:47:12,439 Iter 240, Epoch 52.6316, Loss 0.2780
2018-05-24 02:47:17,701 Iter 241, Epoch 52.8509, Loss 0.2777
2018-05-24 02:47:23,916 Iter 242, Epoch 53.0702, Loss 0.2586
2018-05-24 02:47:29,468 Iter 243, Epoch 53.2895, Loss 0.2669
2018-05-24 02:47:35,023 Iter 244, Epoch 53.5088, Loss 0.2909
2018-05-24 02:47:40,747 Iter 245, Epoch 53.7281, Loss 0.2738
2018-05-24 02:47:46,198 Iter 246, Epoch 53.9474, Loss 0.3014
2018-05-24 02:47:51,942 Iter 247, Epoch 54.1667, Loss 0.2896
2018-05-24 02:47:57,204 Iter 248, Epoch 54.3860, Loss 0.2650
2018-05-24 02:48:03,163 Iter 249, Epoch 54.6053, Loss 0.2850
2018-05-24 02:48:08,648 Iter 250, Epoch 54.8246, Loss 0.2864
2018-05-24 02:48:13,912 Iter 251, Epoch 55.0439, Loss 0.2704
2018-05-24 02:48:19,499 Iter 252, Epoch 55.2632, Loss 0.2633
2018-05-24 02:48:25,106 Iter 253, Epoch 55.4825, Loss 0.2960
2018-05-24 02:48:30,976 Iter 254, Epoch 55.7018, Loss 0.3051
2018-05-24 02:48:36,245 Iter 255, Epoch 55.9211, Loss 0.2748
2018-05-24 02:48:42,304 Iter 256, Epoch 56.1404, Loss 0.3131
2018-05-24 02:48:47,568 Iter 257, Epoch 56.3596, Loss 0.2644
2018-05-24 02:48:53,248 Iter 258, Epoch 56.5789, Loss 0.2492
2018-05-24 02:48:59,206 Iter 259, Epoch 56.7982, Loss 0.2806
2018-05-24 02:49:04,471 Iter 260, Epoch 57.0175, Loss 0.3295
2018-05-24 02:49:10,475 Iter 261, Epoch 57.2368, Loss 0.2727
2018-05-24 02:49:16,019 Iter 262, Epoch 57.4561, Loss 0.2931
2018-05-24 02:49:21,560 Iter 263, Epoch 57.6754, Loss 0.2949
2018-05-24 02:49:26,824 Iter 264, Epoch 57.8947, Loss 0.2956
2018-05-24 02:49:32,823 Iter 265, Epoch 58.1140, Loss 0.2790
2018-05-24 02:49:38,491 Iter 266, Epoch 58.3333, Loss 0.2577
2018-05-24 02:49:44,191 Iter 267, Epoch 58.5526, Loss 0.3213
2018-05-24 02:49:49,898 Iter 268, Epoch 58.7719, Loss 0.2641
2018-05-24 02:49:55,371 Iter 269, Epoch 58.9912, Loss 0.2649
2018-05-24 02:50:00,633 Iter 270, Epoch 59.2105, Loss 0.2924
2018-05-24 02:50:06,308 Iter 271, Epoch 59.4298, Loss 0.2978
2018-05-24 02:50:12,056 Iter 272, Epoch 59.6491, Loss 0.2894
2018-05-24 02:50:18,021 Iter 273, Epoch 59.8684, Loss 0.2714
2018-05-24 02:50:23,289 Iter 274, Epoch 60.0877, Loss 0.2713
2018-05-24 02:50:29,389 Iter 275, Epoch 60.3070, Loss 0.3218
2018-05-24 02:50:35,138 Iter 276, Epoch 60.5263, Loss 0.2560
2018-05-24 02:50:41,055 Iter 277, Epoch 60.7456, Loss 0.2748
2018-05-24 02:50:46,318 Iter 278, Epoch 60.9649, Loss 0.2391
2018-05-24 02:50:51,924 Iter 279, Epoch 61.1842, Loss 0.2522
2018-05-24 02:50:57,875 Iter 280, Epoch 61.4035, Loss 0.2869
2018-05-24 02:51:03,130 Iter 281, Epoch 61.6228, Loss 0.2844
2018-05-24 02:51:08,393 Iter 282, Epoch 61.8421, Loss 0.2531
2018-05-24 02:51:14,407 Iter 283, Epoch 62.0614, Loss 0.2731
2018-05-24 02:51:20,099 Iter 284, Epoch 62.2807, Loss 0.2754
2018-05-24 02:51:26,004 Iter 285, Epoch 62.5000, Loss 0.2763
2018-05-24 02:51:31,268 Iter 286, Epoch 62.7193, Loss 0.2535
2018-05-24 02:51:37,454 Iter 287, Epoch 62.9386, Loss 0.2853
2018-05-24 02:51:42,913 Iter 288, Epoch 63.1579, Loss 0.2628
2018-05-24 02:51:48,561 Iter 289, Epoch 63.3772, Loss 0.3008
2018-05-24 02:51:53,831 Iter 290, Epoch 63.5965, Loss 0.2427
2018-05-24 02:51:59,457 Iter 291, Epoch 63.8158, Loss 0.2914
2018-05-24 02:52:05,333 Iter 292, Epoch 64.0351, Loss 0.2968
2018-05-24 02:52:10,593 Iter 293, Epoch 64.2544, Loss 0.2700
2018-05-24 02:52:15,859 Iter 294, Epoch 64.4737, Loss 0.2906
2018-05-24 02:52:21,371 Iter 295, Epoch 64.6930, Loss 0.3041
2018-05-24 02:52:27,557 Iter 296, Epoch 64.9123, Loss 0.2617
2018-05-24 02:52:32,814 Iter 297, Epoch 65.1316, Loss 0.2561
2018-05-24 02:52:38,387 Iter 298, Epoch 65.3509, Loss 0.3002
2018-05-24 02:52:44,027 Iter 299, Epoch 65.5702, Loss 0.2920
2018-05-24 02:52:49,463 Iter 300, Epoch 65.7895, Loss 0.2642
2018-05-24 02:52:54,725 Iter 301, Epoch 66.0088, Loss 0.2389
2018-05-24 02:53:00,518 Iter 302, Epoch 66.2281, Loss 0.2928
2018-05-24 02:53:06,183 Iter 303, Epoch 66.4474, Loss 0.2879
2018-05-24 02:53:11,451 Iter 304, Epoch 66.6667, Loss 0.2829
2018-05-24 02:53:17,076 Iter 305, Epoch 66.8860, Loss 0.2521
2018-05-24 02:53:22,617 Iter 306, Epoch 67.1053, Loss 0.2534
2018-05-24 02:53:28,205 Iter 307, Epoch 67.3246, Loss 0.2780
2018-05-24 02:53:33,879 Iter 308, Epoch 67.5439, Loss 0.2710
2018-05-24 02:53:39,776 Iter 309, Epoch 67.7632, Loss 0.2519
2018-05-24 02:53:45,303 Iter 310, Epoch 67.9825, Loss 0.3070
2018-05-24 02:53:50,562 Iter 311, Epoch 68.2018, Loss 0.2788
2018-05-24 02:53:56,175 Iter 312, Epoch 68.4211, Loss 0.2974
2018-05-24 02:54:02,138 Iter 313, Epoch 68.6404, Loss 0.2641
2018-05-24 02:54:07,787 Iter 314, Epoch 68.8596, Loss 0.2602
2018-05-24 02:54:13,631 Iter 315, Epoch 69.0789, Loss 0.3147
2018-05-24 02:54:18,892 Iter 316, Epoch 69.2982, Loss 0.2672
2018-05-24 02:54:24,157 Iter 317, Epoch 69.5175, Loss 0.2270
2018-05-24 02:54:29,785 Iter 318, Epoch 69.7368, Loss 0.2854
2018-05-24 02:54:35,581 Iter 319, Epoch 69.9561, Loss 0.2414
2018-05-24 02:54:40,849 Iter 320, Epoch 70.1754, Loss 0.3136
2018-05-24 02:54:46,961 Iter 321, Epoch 70.3947, Loss 0.3013
2018-05-24 02:54:52,671 Iter 322, Epoch 70.6140, Loss 0.2683
2018-05-24 02:54:57,930 Iter 323, Epoch 70.8333, Loss 0.2701
2018-05-24 02:55:03,543 Iter 324, Epoch 71.0526, Loss 0.2791
2018-05-24 02:55:09,116 Iter 325, Epoch 71.2719, Loss 0.2930
2018-05-24 02:55:14,593 Iter 326, Epoch 71.4912, Loss 0.2948
2018-05-24 02:55:20,406 Iter 327, Epoch 71.7105, Loss 0.2839
2018-05-24 02:55:25,922 Iter 328, Epoch 71.9298, Loss 0.2828
2018-05-24 02:55:31,182 Iter 329, Epoch 72.1491, Loss 0.2602
2018-05-24 02:55:37,218 Iter 330, Epoch 72.3684, Loss 0.2748
2018-05-24 02:55:42,487 Iter 331, Epoch 72.5877, Loss 0.2801
2018-05-24 02:55:48,688 Iter 332, Epoch 72.8070, Loss 0.2845
2018-05-24 02:55:54,335 Iter 333, Epoch 73.0263, Loss 0.2896
2018-05-24 02:56:00,428 Iter 334, Epoch 73.2456, Loss 0.2634
2018-05-24 02:56:05,691 Iter 335, Epoch 73.4649, Loss 0.2639
2018-05-24 02:56:10,953 Iter 336, Epoch 73.6842, Loss 0.2637
2018-05-24 02:56:17,014 Iter 337, Epoch 73.9035, Loss 0.2757
2018-05-24 02:56:22,614 Iter 338, Epoch 74.1228, Loss 0.2799
2018-05-24 02:56:28,095 Iter 339, Epoch 74.3421, Loss 0.2487
2018-05-24 02:56:33,356 Iter 340, Epoch 74.5614, Loss 0.2448
2018-05-24 02:56:39,828 Iter 341, Epoch 74.7807, Loss 0.2854
2018-05-24 02:56:45,078 Iter 342, Epoch 75.0000, Loss 0.2868
2018-05-24 02:56:50,993 Iter 343, Epoch 75.2193, Loss 0.2852
2018-05-24 02:56:56,257 Iter 344, Epoch 75.4386, Loss 0.2903
2018-05-24 02:57:02,235 Iter 345, Epoch 75.6579, Loss 0.2822
2018-05-24 02:57:07,493 Iter 346, Epoch 75.8772, Loss 0.2702
2018-05-24 02:57:13,553 Iter 347, Epoch 76.0965, Loss 0.2718
2018-05-24 02:57:18,813 Iter 348, Epoch 76.3158, Loss 0.2478
2018-05-24 02:57:24,927 Iter 349, Epoch 76.5351, Loss 0.2582
2018-05-24 02:57:30,180 Iter 350, Epoch 76.7544, Loss 0.2388
2018-05-24 02:57:35,844 Iter 351, Epoch 76.9737, Loss 0.2612
2018-05-24 02:57:41,332 Iter 352, Epoch 77.1930, Loss 0.2913
2018-05-24 02:57:47,028 Iter 353, Epoch 77.4123, Loss 0.2813
2018-05-24 02:57:52,626 Iter 354, Epoch 77.6316, Loss 0.2672
2018-05-24 02:57:58,310 Iter 355, Epoch 77.8509, Loss 0.2733
2018-05-24 02:58:03,919 Iter 356, Epoch 78.0702, Loss 0.2500
2018-05-24 02:58:09,659 Iter 357, Epoch 78.2895, Loss 0.2853
2018-05-24 02:58:15,274 Iter 358, Epoch 78.5088, Loss 0.2780
2018-05-24 02:58:20,532 Iter 359, Epoch 78.7281, Loss 0.2895
2018-05-24 02:58:26,621 Iter 360, Epoch 78.9474, Loss 0.2428
2018-05-24 02:58:32,069 Iter 361, Epoch 79.1667, Loss 0.2586
2018-05-24 02:58:37,330 Iter 362, Epoch 79.3860, Loss 0.2730
2018-05-24 02:58:43,301 Iter 363, Epoch 79.6053, Loss 0.2550
2018-05-24 02:58:48,973 Iter 364, Epoch 79.8246, Loss 0.2749
2018-05-24 02:58:54,226 Iter 365, Epoch 80.0439, Loss 0.2549
2018-05-24 02:59:00,469 Iter 366, Epoch 80.2632, Loss 0.2520
2018-05-24 02:59:05,731 Iter 367, Epoch 80.4825, Loss 0.2687
2018-05-24 02:59:11,304 Iter 368, Epoch 80.7018, Loss 0.3010
2018-05-24 02:59:16,870 Iter 369, Epoch 80.9211, Loss 0.3186
2018-05-24 02:59:22,133 Iter 370, Epoch 81.1404, Loss 0.2354
2018-05-24 02:59:28,325 Iter 371, Epoch 81.3596, Loss 0.2417
2018-05-24 02:59:34,067 Iter 372, Epoch 81.5789, Loss 0.2442
2018-05-24 02:59:39,329 Iter 373, Epoch 81.7982, Loss 0.2770
2018-05-24 02:59:44,590 Iter 374, Epoch 82.0175, Loss 0.2932
2018-05-24 02:59:50,710 Iter 375, Epoch 82.2368, Loss 0.2585
2018-05-24 02:59:56,365 Iter 376, Epoch 82.4561, Loss 0.2518
2018-05-24 03:00:01,894 Iter 377, Epoch 82.6754, Loss 0.2898
2018-05-24 03:00:07,449 Iter 378, Epoch 82.8947, Loss 0.2615
2018-05-24 03:00:12,947 Iter 379, Epoch 83.1140, Loss 0.2533
2018-05-24 03:00:18,608 Iter 380, Epoch 83.3333, Loss 0.3071
2018-05-24 03:00:23,873 Iter 381, Epoch 83.5526, Loss 0.2852
2018-05-24 03:00:30,175 Iter 382, Epoch 83.7719, Loss 0.2325
2018-05-24 03:00:35,702 Iter 383, Epoch 83.9912, Loss 0.2394
2018-05-24 03:00:40,960 Iter 384, Epoch 84.2105, Loss 0.2741
2018-05-24 03:00:46,562 Iter 385, Epoch 84.4298, Loss 0.2516
2018-05-24 03:00:51,825 Iter 386, Epoch 84.6491, Loss 0.2653
2018-05-24 03:00:57,747 Iter 387, Epoch 84.8684, Loss 0.2436
2018-05-24 03:01:03,329 Iter 388, Epoch 85.0877, Loss 0.2993
2018-05-24 03:01:08,589 Iter 389, Epoch 85.3070, Loss 0.2556
2018-05-24 03:01:13,850 Iter 390, Epoch 85.5263, Loss 0.2498
2018-05-24 03:01:19,921 Iter 391, Epoch 85.7456, Loss 0.2240
2018-05-24 03:01:25,185 Iter 392, Epoch 85.9649, Loss 0.2719
2018-05-24 03:01:31,226 Iter 393, Epoch 86.1842, Loss 0.2646
2018-05-24 03:01:36,762 Iter 394, Epoch 86.4035, Loss 0.2887
2018-05-24 03:01:42,476 Iter 395, Epoch 86.6228, Loss 0.2513
2018-05-24 03:01:47,955 Iter 396, Epoch 86.8421, Loss 0.2677
2018-05-24 03:01:53,493 Iter 397, Epoch 87.0614, Loss 0.2639
2018-05-24 03:01:58,752 Iter 398, Epoch 87.2807, Loss 0.2579
2018-05-24 03:02:04,435 Iter 399, Epoch 87.5000, Loss 0.2544
2018-05-24 03:02:10,088 Iter 400, Epoch 87.7193, Loss 0.2286
2018-05-24 03:02:15,721 Iter 401, Epoch 87.9386, Loss 0.2931
2018-05-24 03:02:21,817 Iter 402, Epoch 88.1579, Loss 0.2844
2018-05-24 03:02:27,079 Iter 403, Epoch 88.3772, Loss 0.2643
2018-05-24 03:02:32,680 Iter 404, Epoch 88.5965, Loss 0.2531
2018-05-24 03:02:38,722 Iter 405, Epoch 88.8158, Loss 0.2551
2018-05-24 03:02:44,408 Iter 406, Epoch 89.0351, Loss 0.2603
2018-05-24 03:02:50,164 Iter 407, Epoch 89.2544, Loss 0.2652
2018-05-24 03:02:55,410 Iter 408, Epoch 89.4737, Loss 0.2455
2018-05-24 03:03:01,489 Iter 409, Epoch 89.6930, Loss 0.2386
2018-05-24 03:03:06,754 Iter 410, Epoch 89.9123, Loss 0.2781
2018-05-24 03:03:12,909 Iter 411, Epoch 90.1316, Loss 0.2764
2018-05-24 03:03:18,605 Iter 412, Epoch 90.3509, Loss 0.2677
2018-05-24 03:03:24,208 Iter 413, Epoch 90.5702, Loss 0.2640
2018-05-24 03:03:29,737 Iter 414, Epoch 90.7895, Loss 0.2371
2018-05-24 03:03:35,203 Iter 415, Epoch 91.0088, Loss 0.2882
2018-05-24 03:03:40,923 Iter 416, Epoch 91.2281, Loss 0.2498
2018-05-24 03:03:46,687 Iter 417, Epoch 91.4474, Loss 0.2599
2018-05-24 03:03:52,726 Iter 418, Epoch 91.6667, Loss 0.2538
2018-05-24 03:03:58,354 Iter 419, Epoch 91.8860, Loss 0.2962
2018-05-24 03:04:03,615 Iter 420, Epoch 92.1053, Loss 0.2472
2018-05-24 03:04:09,277 Iter 421, Epoch 92.3246, Loss 0.2738
2018-05-24 03:04:14,543 Iter 422, Epoch 92.5439, Loss 0.2334
2018-05-24 03:04:20,693 Iter 423, Epoch 92.7632, Loss 0.2660
2018-05-24 03:04:26,186 Iter 424, Epoch 92.9825, Loss 0.2502
2018-05-24 03:04:32,179 Iter 425, Epoch 93.2018, Loss 0.2370
2018-05-24 03:04:37,433 Iter 426, Epoch 93.4211, Loss 0.2480
2018-05-24 03:04:43,097 Iter 427, Epoch 93.6404, Loss 0.2573
2018-05-24 03:04:48,747 Iter 428, Epoch 93.8596, Loss 0.2646
2018-05-24 03:04:54,342 Iter 429, Epoch 94.0789, Loss 0.2583
2018-05-24 03:05:00,052 Iter 430, Epoch 94.2982, Loss 0.2361
2018-05-24 03:05:05,316 Iter 431, Epoch 94.5175, Loss 0.2578
2018-05-24 03:05:11,226 Iter 432, Epoch 94.7368, Loss 0.2859
2018-05-24 03:05:16,859 Iter 433, Epoch 94.9561, Loss 0.2749
2018-05-24 03:05:22,494 Iter 434, Epoch 95.1754, Loss 0.2394
2018-05-24 03:05:28,140 Iter 435, Epoch 95.3947, Loss 0.2473
2018-05-24 03:05:33,815 Iter 436, Epoch 95.6140, Loss 0.2481
2018-05-24 03:05:39,073 Iter 437, Epoch 95.8333, Loss 0.2612
2018-05-24 03:05:45,130 Iter 438, Epoch 96.0526, Loss 0.2287
2018-05-24 03:05:50,618 Iter 439, Epoch 96.2719, Loss 0.2853
2018-05-24 03:05:56,559 Iter 440, Epoch 96.4912, Loss 0.2384
2018-05-24 03:06:01,825 Iter 441, Epoch 96.7105, Loss 0.2493
2018-05-24 03:06:07,483 Iter 442, Epoch 96.9298, Loss 0.2400
2018-05-24 03:06:13,037 Iter 443, Epoch 97.1491, Loss 0.2753
2018-05-24 03:06:18,298 Iter 444, Epoch 97.3684, Loss 0.2300
2018-05-24 03:06:24,334 Iter 445, Epoch 97.5877, Loss 0.2556
2018-05-24 03:06:30,363 Iter 446, Epoch 97.8070, Loss 0.2708
2018-05-24 03:06:35,626 Iter 447, Epoch 98.0263, Loss 0.2700
2018-05-24 03:06:41,108 Iter 448, Epoch 98.2456, Loss 0.2393
2018-05-24 03:06:46,374 Iter 449, Epoch 98.4649, Loss 0.2914
2018-05-24 03:06:52,221 Iter 450, Epoch 98.6842, Loss 0.2431
2018-05-24 03:06:57,844 Iter 451, Epoch 98.9035, Loss 0.2829
2018-05-24 03:07:03,422 Iter 452, Epoch 99.1228, Loss 0.2863
2018-05-24 03:07:09,057 Iter 453, Epoch 99.3421, Loss 0.2517
2018-05-24 03:07:14,909 Iter 454, Epoch 99.5614, Loss 0.2439
2018-05-24 03:07:20,342 Iter 455, Epoch 99.7807, Loss 0.2542
2018-05-24 03:07:25,609 Iter 456, Epoch 100.0000, Loss 0.2452
2018-05-24 03:07:31,218 Iter 457, Epoch 100.2193, Loss 0.2057
2018-05-24 03:07:36,852 Iter 458, Epoch 100.4386, Loss 0.2391
2018-05-24 03:07:42,804 Iter 459, Epoch 100.6579, Loss 0.2605
2018-05-24 03:07:48,487 Iter 460, Epoch 100.8772, Loss 0.2654
2018-05-24 03:07:54,215 Iter 461, Epoch 101.0965, Loss 0.2618
2018-05-24 03:07:59,477 Iter 462, Epoch 101.3158, Loss 0.2369
2018-05-24 03:08:05,478 Iter 463, Epoch 101.5351, Loss 0.2554
2018-05-24 03:08:11,250 Iter 464, Epoch 101.7544, Loss 0.2567
2018-05-24 03:08:16,506 Iter 465, Epoch 101.9737, Loss 0.2966
2018-05-24 03:08:22,177 Iter 466, Epoch 102.1930, Loss 0.2406
2018-05-24 03:08:27,825 Iter 467, Epoch 102.4123, Loss 0.2554
2018-05-24 03:08:33,090 Iter 468, Epoch 102.6316, Loss 0.2754
2018-05-24 03:08:39,228 Iter 469, Epoch 102.8509, Loss 0.2675
2018-05-24 03:08:44,954 Iter 470, Epoch 103.0702, Loss 0.2393
2018-05-24 03:08:50,582 Iter 471, Epoch 103.2895, Loss 0.2532
2018-05-24 03:08:56,281 Iter 472, Epoch 103.5088, Loss 0.2623
2018-05-24 03:09:01,796 Iter 473, Epoch 103.7281, Loss 0.2365
2018-05-24 03:09:07,331 Iter 474, Epoch 103.9474, Loss 0.2439
2018-05-24 03:09:12,931 Iter 475, Epoch 104.1667, Loss 0.2363
2018-05-24 03:09:18,196 Iter 476, Epoch 104.3860, Loss 0.2486
2018-05-24 03:09:24,294 Iter 477, Epoch 104.6053, Loss 0.2706
2018-05-24 03:09:29,831 Iter 478, Epoch 104.8246, Loss 0.2904
2018-05-24 03:09:35,095 Iter 479, Epoch 105.0439, Loss 0.2579
2018-05-24 03:09:40,760 Iter 480, Epoch 105.2632, Loss 0.2466
2018-05-24 03:09:46,383 Iter 481, Epoch 105.4825, Loss 0.2348
2018-05-24 03:09:51,654 Iter 482, Epoch 105.7018, Loss 0.2571
2018-05-24 03:09:57,657 Iter 483, Epoch 105.9211, Loss 0.2655
2018-05-24 03:10:03,130 Iter 484, Epoch 106.1404, Loss 0.2444
2018-05-24 03:10:08,735 Iter 485, Epoch 106.3596, Loss 0.2536
2018-05-24 03:10:14,431 Iter 486, Epoch 106.5789, Loss 0.2564
2018-05-24 03:10:19,999 Iter 487, Epoch 106.7982, Loss 0.2494
2018-05-24 03:10:25,703 Iter 488, Epoch 107.0175, Loss 0.2453
2018-05-24 03:10:31,346 Iter 489, Epoch 107.2368, Loss 0.2517
2018-05-24 03:10:36,893 Iter 490, Epoch 107.4561, Loss 0.2472
2018-05-24 03:10:42,155 Iter 491, Epoch 107.6754, Loss 0.2651
2018-05-24 03:10:47,864 Iter 492, Epoch 107.8947, Loss 0.2404
2018-05-24 03:10:54,050 Iter 493, Epoch 108.1140, Loss 0.2443
2018-05-24 03:10:59,677 Iter 494, Epoch 108.3333, Loss 0.2629
2018-05-24 03:11:05,337 Iter 495, Epoch 108.5526, Loss 0.2871
2018-05-24 03:11:10,962 Iter 496, Epoch 108.7719, Loss 0.2688
2018-05-24 03:11:16,228 Iter 497, Epoch 108.9912, Loss 0.2519
2018-05-24 03:11:21,959 Iter 498, Epoch 109.2105, Loss 0.2439
2018-05-24 03:11:27,532 Iter 499, Epoch 109.4298, Loss 0.2432
2018-05-24 03:11:33,087 Iter 500, Epoch 109.6491, Loss 0.2361
2018-05-24 03:11:38,666 Iter 501, Epoch 109.8684, Loss 0.2699
2018-05-24 03:11:43,934 Iter 502, Epoch 110.0877, Loss 0.2417
2018-05-24 03:11:49,407 Iter 503, Epoch 110.3070, Loss 0.2367
2018-05-24 03:11:55,003 Iter 504, Epoch 110.5263, Loss 0.2763
2018-05-24 03:12:00,637 Iter 505, Epoch 110.7456, Loss 0.2476
2018-05-24 03:12:06,408 Iter 506, Epoch 110.9649, Loss 0.2712
2018-05-24 03:12:12,480 Iter 507, Epoch 111.1842, Loss 0.2626
2018-05-24 03:12:17,741 Iter 508, Epoch 111.4035, Loss 0.2581
2018-05-24 03:12:23,003 Iter 509, Epoch 111.6228, Loss 0.2673
2018-05-24 03:12:28,643 Iter 510, Epoch 111.8421, Loss 0.2590
2018-05-24 03:12:34,547 Iter 511, Epoch 112.0614, Loss 0.2496
2018-05-24 03:12:39,813 Iter 512, Epoch 112.2807, Loss 0.2457
2018-05-24 03:12:45,836 Iter 513, Epoch 112.5000, Loss 0.2496
2018-05-24 03:12:51,099 Iter 514, Epoch 112.7193, Loss 0.2864
2018-05-24 03:12:56,587 Iter 515, Epoch 112.9386, Loss 0.2492
2018-05-24 03:13:02,666 Iter 516, Epoch 113.1579, Loss 0.2385
2018-05-24 03:13:07,933 Iter 517, Epoch 113.3772, Loss 0.2417
2018-05-24 03:13:13,937 Iter 518, Epoch 113.5965, Loss 0.2817
2018-05-24 03:13:19,426 Iter 519, Epoch 113.8158, Loss 0.2445
2018-05-24 03:13:24,693 Iter 520, Epoch 114.0351, Loss 0.2183
2018-05-24 03:13:30,489 Iter 521, Epoch 114.2544, Loss 0.2312
2018-05-24 03:13:35,994 Iter 522, Epoch 114.4737, Loss 0.2493
2018-05-24 03:13:42,140 Iter 523, Epoch 114.6930, Loss 0.2811
2018-05-24 03:13:47,405 Iter 524, Epoch 114.9123, Loss 0.2527
2018-05-24 03:13:53,365 Iter 525, Epoch 115.1316, Loss 0.2531
2018-05-24 03:13:58,978 Iter 526, Epoch 115.3509, Loss 0.2436
2018-05-24 03:14:04,246 Iter 527, Epoch 115.5702, Loss 0.2643
2018-05-24 03:14:09,915 Iter 528, Epoch 115.7895, Loss 0.2561
2018-05-24 03:14:15,470 Iter 529, Epoch 116.0088, Loss 0.2326
2018-05-24 03:14:21,346 Iter 530, Epoch 116.2281, Loss 0.2354
2018-05-24 03:14:27,000 Iter 531, Epoch 116.4474, Loss 0.2530
2018-05-24 03:14:32,262 Iter 532, Epoch 116.6667, Loss 0.2452
2018-05-24 03:14:37,945 Iter 533, Epoch 116.8860, Loss 0.2401
2018-05-24 03:14:44,076 Iter 534, Epoch 117.1053, Loss 0.2659
2018-05-24 03:14:49,331 Iter 535, Epoch 117.3246, Loss 0.2531
2018-05-24 03:14:54,596 Iter 536, Epoch 117.5439, Loss 0.2247
2018-05-24 03:15:00,564 Iter 537, Epoch 117.7632, Loss 0.2465
2018-05-24 03:15:06,120 Iter 538, Epoch 117.9825, Loss 0.2193
2018-05-24 03:15:11,386 Iter 539, Epoch 118.2018, Loss 0.2212
2018-05-24 03:15:17,011 Iter 540, Epoch 118.4211, Loss 0.2231
2018-05-24 03:15:22,906 Iter 541, Epoch 118.6404, Loss 0.2544
2018-05-24 03:15:28,568 Iter 542, Epoch 118.8596, Loss 0.2412
2018-05-24 03:15:34,165 Iter 543, Epoch 119.0789, Loss 0.2738
2018-05-24 03:15:39,815 Iter 544, Epoch 119.2982, Loss 0.2272
2018-05-24 03:15:45,368 Iter 545, Epoch 119.5175, Loss 0.2418
2018-05-24 03:15:50,629 Iter 546, Epoch 119.7368, Loss 0.2704
2018-05-24 03:15:56,953 Iter 547, Epoch 119.9561, Loss 0.2135
2018-05-24 03:16:02,218 Iter 548, Epoch 120.1754, Loss 0.2291
2018-05-24 03:16:08,020 Iter 549, Epoch 120.3947, Loss 0.2249
2018-05-24 03:16:13,679 Iter 550, Epoch 120.6140, Loss 0.2557
2018-05-24 03:16:18,933 Iter 551, Epoch 120.8333, Loss 0.2844
2018-05-24 03:16:24,472 Iter 552, Epoch 121.0526, Loss 0.2654
2018-05-24 03:16:30,335 Iter 553, Epoch 121.2719, Loss 0.2481
2018-05-24 03:16:35,840 Iter 554, Epoch 121.4912, Loss 0.2254
2018-05-24 03:16:41,483 Iter 555, Epoch 121.7105, Loss 0.2621
2018-05-24 03:16:46,962 Iter 556, Epoch 121.9298, Loss 0.2330
2018-05-24 03:16:52,228 Iter 557, Epoch 122.1491, Loss 0.2440
2018-05-24 03:16:57,833 Iter 558, Epoch 122.3684, Loss 0.2324
2018-05-24 03:17:03,811 Iter 559, Epoch 122.5877, Loss 0.2423
2018-05-24 03:17:09,502 Iter 560, Epoch 122.8070, Loss 0.2612
2018-05-24 03:17:15,421 Iter 561, Epoch 123.0263, Loss 0.2418
2018-05-24 03:17:20,684 Iter 562, Epoch 123.2456, Loss 0.2872
2018-05-24 03:17:25,951 Iter 563, Epoch 123.4649, Loss 0.2288
2018-05-24 03:17:31,573 Iter 564, Epoch 123.6842, Loss 0.2545
2018-05-24 03:17:37,145 Iter 565, Epoch 123.9035, Loss 0.2300
2018-05-24 03:17:43,158 Iter 566, Epoch 124.1228, Loss 0.2801
2018-05-24 03:17:48,913 Iter 567, Epoch 124.3421, Loss 0.2384
2018-05-24 03:17:54,495 Iter 568, Epoch 124.5614, Loss 0.2565
2018-05-24 03:17:59,761 Iter 569, Epoch 124.7807, Loss 0.2430
2018-05-24 03:18:05,329 Iter 570, Epoch 125.0000, Loss 0.2376
2018-05-24 03:18:11,091 Iter 571, Epoch 125.2193, Loss 0.2352
2018-05-24 03:18:16,659 Iter 572, Epoch 125.4386, Loss 0.2381
2018-05-24 03:18:22,624 Iter 573, Epoch 125.6579, Loss 0.2560
2018-05-24 03:18:28,032 Iter 574, Epoch 125.8772, Loss 0.2536
2018-05-24 03:18:33,299 Iter 575, Epoch 126.0965, Loss 0.2668
2018-05-24 03:18:39,342 Iter 576, Epoch 126.3158, Loss 0.2388
2018-05-24 03:18:44,817 Iter 577, Epoch 126.5351, Loss 0.2102
2018-05-24 03:18:50,560 Iter 578, Epoch 126.7544, Loss 0.2852
2018-05-24 03:18:55,931 Iter 579, Epoch 126.9737, Loss 0.2642
2018-05-24 03:19:01,526 Iter 580, Epoch 127.1930, Loss 0.2542
2018-05-24 03:19:06,790 Iter 581, Epoch 127.4123, Loss 0.2768
2018-05-24 03:19:12,472 Iter 582, Epoch 127.6316, Loss 0.2439
2018-05-24 03:19:18,575 Iter 583, Epoch 127.8509, Loss 0.2349
2018-05-24 03:19:24,118 Iter 584, Epoch 128.0702, Loss 0.2254
2018-05-24 03:19:29,381 Iter 585, Epoch 128.2895, Loss 0.2433
2018-05-24 03:19:35,215 Iter 586, Epoch 128.5088, Loss 0.2561
2018-05-24 03:19:40,481 Iter 587, Epoch 128.7281, Loss 0.2425
2018-05-24 03:19:46,046 Iter 588, Epoch 128.9474, Loss 0.2425
2018-05-24 03:19:52,049 Iter 589, Epoch 129.1667, Loss 0.2536
2018-05-24 03:19:57,578 Iter 590, Epoch 129.3860, Loss 0.2328
2018-05-24 03:20:03,219 Iter 591, Epoch 129.6053, Loss 0.2122
2018-05-24 03:20:08,760 Iter 592, Epoch 129.8246, Loss 0.2547
2018-05-24 03:20:14,375 Iter 593, Epoch 130.0439, Loss 0.2650
2018-05-24 03:20:20,121 Iter 594, Epoch 130.2632, Loss 0.2457
2018-05-24 03:20:25,763 Iter 595, Epoch 130.4825, Loss 0.2461
2018-05-24 03:20:31,299 Iter 596, Epoch 130.7018, Loss 0.2659
2018-05-24 03:20:37,007 Iter 597, Epoch 130.9211, Loss 0.2153
2018-05-24 03:20:42,591 Iter 598, Epoch 131.1404, Loss 0.2349
2018-05-24 03:20:48,278 Iter 599, Epoch 131.3596, Loss 0.2339
2018-05-24 03:20:53,531 Iter 600, Epoch 131.5789, Loss 0.2591
2018-05-24 03:20:59,169 Iter 601, Epoch 131.7982, Loss 0.2402
2018-05-24 03:21:05,114 Iter 602, Epoch 132.0175, Loss 0.2539
2018-05-24 03:21:10,376 Iter 603, Epoch 132.2368, Loss 0.2284
2018-05-24 03:21:16,089 Iter 604, Epoch 132.4561, Loss 0.2485
2018-05-24 03:21:22,570 Iter 605, Epoch 132.6754, Loss 0.2693
2018-05-24 03:21:27,834 Iter 606, Epoch 132.8947, Loss 0.2522
2018-05-24 03:21:33,402 Iter 607, Epoch 133.1140, Loss 0.2442
2018-05-24 03:21:39,147 Iter 608, Epoch 133.3333, Loss 0.2537
2018-05-24 03:21:44,843 Iter 609, Epoch 133.5526, Loss 0.2409
2018-05-24 03:21:50,109 Iter 610, Epoch 133.7719, Loss 0.2326
2018-05-24 03:21:56,414 Iter 611, Epoch 133.9912, Loss 0.2372
2018-05-24 03:22:01,666 Iter 612, Epoch 134.2105, Loss 0.2649
2018-05-24 03:22:06,931 Iter 613, Epoch 134.4298, Loss 0.2458
2018-05-24 03:22:13,040 Iter 614, Epoch 134.6491, Loss 0.2435
2018-05-24 03:22:18,421 Iter 615, Epoch 134.8684, Loss 0.2286
2018-05-24 03:22:23,686 Iter 616, Epoch 135.0877, Loss 0.2621
2018-05-24 03:22:29,673 Iter 617, Epoch 135.3070, Loss 0.2337
2018-05-24 03:22:35,376 Iter 618, Epoch 135.5263, Loss 0.2298
2018-05-24 03:22:41,019 Iter 619, Epoch 135.7456, Loss 0.2600
2018-05-24 03:22:46,283 Iter 620, Epoch 135.9649, Loss 0.2460
2018-05-24 03:22:52,085 Iter 621, Epoch 136.1842, Loss 0.2393
2018-05-24 03:22:57,349 Iter 622, Epoch 136.4035, Loss 0.2450
2018-05-24 03:23:02,859 Iter 623, Epoch 136.6228, Loss 0.2630
2018-05-24 03:23:08,655 Iter 624, Epoch 136.8421, Loss 0.2397
2018-05-24 03:23:14,366 Iter 625, Epoch 137.0614, Loss 0.2323
2018-05-24 03:23:19,629 Iter 626, Epoch 137.2807, Loss 0.2446
2018-05-24 03:23:25,391 Iter 627, Epoch 137.5000, Loss 0.2381
2018-05-24 03:23:30,655 Iter 628, Epoch 137.7193, Loss 0.2395
2018-05-24 03:23:36,339 Iter 629, Epoch 137.9386, Loss 0.2377
2018-05-24 03:23:41,888 Iter 630, Epoch 138.1579, Loss 0.2423
2018-05-24 03:23:47,548 Iter 631, Epoch 138.3772, Loss 0.2286
2018-05-24 03:23:53,211 Iter 632, Epoch 138.5965, Loss 0.2389
2018-05-24 03:23:59,256 Iter 633, Epoch 138.8158, Loss 0.2203
2018-05-24 03:24:04,521 Iter 634, Epoch 139.0351, Loss 0.2503
2018-05-24 03:24:10,408 Iter 635, Epoch 139.2544, Loss 0.2575
2018-05-24 03:24:15,906 Iter 636, Epoch 139.4737, Loss 0.2559
2018-05-24 03:24:21,444 Iter 637, Epoch 139.6930, Loss 0.2489
2018-05-24 03:24:27,110 Iter 638, Epoch 139.9123, Loss 0.2319
2018-05-24 03:24:33,061 Iter 639, Epoch 140.1316, Loss 0.2470
2018-05-24 03:24:38,321 Iter 640, Epoch 140.3509, Loss 0.2269
2018-05-24 03:24:43,583 Iter 641, Epoch 140.5702, Loss 0.2154
2018-05-24 03:24:49,516 Iter 642, Epoch 140.7895, Loss 0.2351
2018-05-24 03:24:55,075 Iter 643, Epoch 141.0088, Loss 0.2483
2018-05-24 03:25:00,338 Iter 644, Epoch 141.2281, Loss 0.2369
2018-05-24 03:25:06,039 Iter 645, Epoch 141.4474, Loss 0.2183
2018-05-24 03:25:11,937 Iter 646, Epoch 141.6667, Loss 0.2259
2018-05-24 03:25:17,648 Iter 647, Epoch 141.8860, Loss 0.2500
2018-05-24 03:25:22,913 Iter 648, Epoch 142.1053, Loss 0.2561
2018-05-24 03:25:28,685 Iter 649, Epoch 142.3246, Loss 0.2405
2018-05-24 03:25:33,954 Iter 650, Epoch 142.5439, Loss 0.2391
2018-05-24 03:25:39,526 Iter 651, Epoch 142.7632, Loss 0.2381
2018-05-24 03:25:45,071 Iter 652, Epoch 142.9825, Loss 0.2348
2018-05-24 03:25:51,107 Iter 653, Epoch 143.2018, Loss 0.2327
2018-05-24 03:25:56,760 Iter 654, Epoch 143.4211, Loss 0.2221
2018-05-24 03:26:02,348 Iter 655, Epoch 143.6404, Loss 0.2559
2018-05-24 03:26:07,609 Iter 656, Epoch 143.8596, Loss 0.2426
2018-05-24 03:26:13,478 Iter 657, Epoch 144.0789, Loss 0.2531
2018-05-24 03:26:18,739 Iter 658, Epoch 144.2982, Loss 0.2341
2018-05-24 03:26:24,556 Iter 659, Epoch 144.5175, Loss 0.2327
2018-05-24 03:26:29,814 Iter 660, Epoch 144.7368, Loss 0.2455
2018-05-24 03:26:35,883 Iter 661, Epoch 144.9561, Loss 0.2430
2018-05-24 03:26:41,143 Iter 662, Epoch 145.1754, Loss 0.2455
2018-05-24 03:26:47,182 Iter 663, Epoch 145.3947, Loss 0.2385
2018-05-24 03:26:52,657 Iter 664, Epoch 145.6140, Loss 0.2184
2018-05-24 03:26:57,922 Iter 665, Epoch 145.8333, Loss 0.2603
2018-05-24 03:27:03,972 Iter 666, Epoch 146.0526, Loss 0.2565
2018-05-24 03:27:09,233 Iter 667, Epoch 146.2719, Loss 0.2136
2018-05-24 03:27:14,788 Iter 668, Epoch 146.4912, Loss 0.2109
2018-05-24 03:27:20,787 Iter 669, Epoch 146.7105, Loss 0.2189
2018-05-24 03:27:26,251 Iter 670, Epoch 146.9298, Loss 0.2386
2018-05-24 03:27:31,516 Iter 671, Epoch 147.1491, Loss 0.2446
2018-05-24 03:27:37,247 Iter 672, Epoch 147.3684, Loss 0.2407
2018-05-24 03:27:43,079 Iter 673, Epoch 147.5877, Loss 0.2489
2018-05-24 03:27:48,683 Iter 674, Epoch 147.8070, Loss 0.2334
2018-05-24 03:27:54,185 Iter 675, Epoch 148.0263, Loss 0.2237
2018-05-24 03:27:59,887 Iter 676, Epoch 148.2456, Loss 0.2309
2018-05-24 03:28:05,450 Iter 677, Epoch 148.4649, Loss 0.2360
2018-05-24 03:28:10,711 Iter 678, Epoch 148.6842, Loss 0.2443
2018-05-24 03:28:16,695 Iter 679, Epoch 148.9035, Loss 0.2446
2018-05-24 03:28:21,957 Iter 680, Epoch 149.1228, Loss 0.2310
2018-05-24 03:28:28,286 Iter 681, Epoch 149.3421, Loss 0.2180
2018-05-24 03:28:33,383 Iter 682, Epoch 149.5614, Loss 0.2549
2018-05-24 03:28:39,301 Iter 683, Epoch 149.7807, Loss 0.2641
2018-05-24 03:28:44,562 Iter 684, Epoch 150.0000, Loss 0.2470
2018-05-24 03:28:50,178 Iter 685, Epoch 150.2193, Loss 0.2588
2018-05-24 03:28:55,839 Iter 686, Epoch 150.4386, Loss 0.2380
2018-05-24 03:29:02,137 Iter 687, Epoch 150.6579, Loss 0.2034
2018-05-24 03:29:07,653 Iter 688, Epoch 150.8772, Loss 0.2340
2018-05-24 03:29:13,366 Iter 689, Epoch 151.0965, Loss 0.2320
2018-05-24 03:29:18,629 Iter 690, Epoch 151.3158, Loss 0.2618
2018-05-24 03:29:24,778 Iter 691, Epoch 151.5351, Loss 0.2653
2018-05-24 03:29:30,047 Iter 692, Epoch 151.7544, Loss 0.2515
2018-05-24 03:29:36,084 Iter 693, Epoch 151.9737, Loss 0.2194
2018-05-24 03:29:41,644 Iter 694, Epoch 152.1930, Loss 0.2222
2018-05-24 03:29:47,119 Iter 695, Epoch 152.4123, Loss 0.2173
2018-05-24 03:29:52,383 Iter 696, Epoch 152.6316, Loss 0.2511
2018-05-24 03:29:58,238 Iter 697, Epoch 152.8509, Loss 0.2409
2018-05-24 03:30:03,877 Iter 698, Epoch 153.0702, Loss 0.2320
2018-05-24 03:30:09,215 Iter 699, Epoch 153.2895, Loss 0.2477
2018-05-24 03:30:14,990 Iter 700, Epoch 153.5088, Loss 0.2500
2018-05-24 03:30:20,254 Iter 701, Epoch 153.7281, Loss 0.2400
2018-05-24 03:30:25,758 Iter 702, Epoch 153.9474, Loss 0.2330
2018-05-24 03:30:31,688 Iter 703, Epoch 154.1667, Loss 0.2442
2018-05-24 03:30:36,951 Iter 704, Epoch 154.3860, Loss 0.2179
2018-05-24 03:30:42,934 Iter 705, Epoch 154.6053, Loss 0.2312
2018-05-24 03:30:48,605 Iter 706, Epoch 154.8246, Loss 0.2248
2018-05-24 03:30:54,297 Iter 707, Epoch 155.0439, Loss 0.2215
2018-05-24 03:30:59,792 Iter 708, Epoch 155.2632, Loss 0.2118
2018-05-24 03:31:05,492 Iter 709, Epoch 155.4825, Loss 0.2316
2018-05-24 03:31:10,753 Iter 710, Epoch 155.7018, Loss 0.2287
2018-05-24 03:31:16,921 Iter 711, Epoch 155.9211, Loss 0.2472
2018-05-24 03:31:22,624 Iter 712, Epoch 156.1404, Loss 0.2223
2018-05-24 03:31:27,960 Iter 713, Epoch 156.3596, Loss 0.2409
2018-05-24 03:31:33,222 Iter 714, Epoch 156.5789, Loss 0.2456
2018-05-24 03:31:39,346 Iter 715, Epoch 156.7982, Loss 0.2222
2018-05-24 03:31:44,610 Iter 716, Epoch 157.0175, Loss 0.2844
2018-05-24 03:31:50,893 Iter 717, Epoch 157.2368, Loss 0.2356
2018-05-24 03:31:56,151 Iter 718, Epoch 157.4561, Loss 0.2269
2018-05-24 03:32:01,828 Iter 719, Epoch 157.6754, Loss 0.2432
2018-05-24 03:32:07,806 Iter 720, Epoch 157.8947, Loss 0.2393
2018-05-24 03:32:13,069 Iter 721, Epoch 158.1140, Loss 0.2176
2018-05-24 03:32:18,332 Iter 722, Epoch 158.3333, Loss 0.2502
2018-05-24 03:32:24,501 Iter 723, Epoch 158.5526, Loss 0.2627
2018-05-24 03:32:29,762 Iter 724, Epoch 158.7719, Loss 0.2374
2018-05-24 03:32:35,024 Iter 725, Epoch 158.9912, Loss 0.2244
2018-05-24 03:32:40,677 Iter 726, Epoch 159.2105, Loss 0.2352
2018-05-24 03:32:46,252 Iter 727, Epoch 159.4298, Loss 0.2184
2018-05-24 03:32:51,875 Iter 728, Epoch 159.6491, Loss 0.2320
2018-05-24 03:32:57,920 Iter 729, Epoch 159.8684, Loss 0.2465
2018-05-24 03:33:03,176 Iter 730, Epoch 160.0877, Loss 0.2087
2018-05-24 03:33:08,748 Iter 731, Epoch 160.3070, Loss 0.2439
2018-05-24 03:33:14,299 Iter 732, Epoch 160.5263, Loss 0.2475
2018-05-24 03:33:20,012 Iter 733, Epoch 160.7456, Loss 0.2421
2018-05-24 03:33:25,272 Iter 734, Epoch 160.9649, Loss 0.2282
2018-05-24 03:33:31,019 Iter 735, Epoch 161.1842, Loss 0.2228
2018-05-24 03:33:36,951 Iter 736, Epoch 161.4035, Loss 0.2065
2018-05-24 03:33:42,501 Iter 737, Epoch 161.6228, Loss 0.2349
2018-05-24 03:33:48,172 Iter 738, Epoch 161.8421, Loss 0.2550
2018-05-24 03:33:53,433 Iter 739, Epoch 162.0614, Loss 0.2519
2018-05-24 03:33:58,698 Iter 740, Epoch 162.2807, Loss 0.2358
2018-05-24 03:34:04,911 Iter 741, Epoch 162.5000, Loss 0.2264
2018-05-24 03:34:10,548 Iter 742, Epoch 162.7193, Loss 0.2593
2018-05-24 03:34:16,021 Iter 743, Epoch 162.9386, Loss 0.2146
2018-05-24 03:34:21,283 Iter 744, Epoch 163.1579, Loss 0.2105
2018-05-24 03:34:27,061 Iter 745, Epoch 163.3772, Loss 0.2392
2018-05-24 03:34:32,694 Iter 746, Epoch 163.5965, Loss 0.2475
2018-05-24 03:34:38,302 Iter 747, Epoch 163.8158, Loss 0.2127
2018-05-24 03:34:43,934 Iter 748, Epoch 164.0351, Loss 0.2612
2018-05-24 03:34:49,898 Iter 749, Epoch 164.2544, Loss 0.2296
2018-05-24 03:34:55,157 Iter 750, Epoch 164.4737, Loss 0.2169
2018-05-24 03:35:00,729 Iter 751, Epoch 164.6930, Loss 0.2759
2018-05-24 03:35:06,602 Iter 752, Epoch 164.9123, Loss 0.2412
2018-05-24 03:35:11,860 Iter 753, Epoch 165.1316, Loss 0.2347
2018-05-24 03:35:17,580 Iter 754, Epoch 165.3509, Loss 0.2426
2018-05-24 03:35:22,838 Iter 755, Epoch 165.5702, Loss 0.2378
2018-05-24 03:35:29,142 Iter 756, Epoch 165.7895, Loss 0.2526
2018-05-24 03:35:34,401 Iter 757, Epoch 166.0088, Loss 0.2355
2018-05-24 03:35:39,661 Iter 758, Epoch 166.2281, Loss 0.2404
2018-05-24 03:35:45,840 Iter 759, Epoch 166.4474, Loss 0.2085
2018-05-24 03:35:51,606 Iter 760, Epoch 166.6667, Loss 0.2582
2018-05-24 03:35:56,868 Iter 761, Epoch 166.8860, Loss 0.2462
2018-05-24 03:36:02,563 Iter 762, Epoch 167.1053, Loss 0.2338
2018-05-24 03:36:07,817 Iter 763, Epoch 167.3246, Loss 0.2296
2018-05-24 03:36:13,729 Iter 764, Epoch 167.5439, Loss 0.2426
2018-05-24 03:36:19,561 Iter 765, Epoch 167.7632, Loss 0.2279
2018-05-24 03:36:25,186 Iter 766, Epoch 167.9825, Loss 0.2470
2018-05-24 03:36:30,442 Iter 767, Epoch 168.2018, Loss 0.2154
2018-05-24 03:36:35,701 Iter 768, Epoch 168.4211, Loss 0.2102
2018-05-24 03:36:41,265 Iter 769, Epoch 168.6404, Loss 0.2675
2018-05-24 03:36:46,823 Iter 770, Epoch 168.8596, Loss 0.2568
2018-05-24 03:36:52,898 Iter 771, Epoch 169.0789, Loss 0.2408
2018-05-24 03:36:58,435 Iter 772, Epoch 169.2982, Loss 0.2377
2018-05-24 03:37:03,695 Iter 773, Epoch 169.5175, Loss 0.2042
2018-05-24 03:37:09,589 Iter 774, Epoch 169.7368, Loss 0.2329
2018-05-24 03:37:14,847 Iter 775, Epoch 169.9561, Loss 0.2295
2018-05-24 03:37:20,319 Iter 776, Epoch 170.1754, Loss 0.2417
2018-05-24 03:37:26,294 Iter 777, Epoch 170.3947, Loss 0.2148
2018-05-24 03:37:31,917 Iter 778, Epoch 170.6140, Loss 0.2125
2018-05-24 03:37:37,429 Iter 779, Epoch 170.8333, Loss 0.2503
2018-05-24 03:37:42,687 Iter 780, Epoch 171.0526, Loss 0.2288
2018-05-24 03:37:48,744 Iter 781, Epoch 171.2719, Loss 0.2182
2018-05-24 03:37:54,755 Iter 782, Epoch 171.4912, Loss 0.2221
2018-05-24 03:38:00,006 Iter 783, Epoch 171.7105, Loss 0.2599
2018-05-24 03:38:05,676 Iter 784, Epoch 171.9298, Loss 0.2483
2018-05-24 03:38:10,935 Iter 785, Epoch 172.1491, Loss 0.2153
2018-05-24 03:38:16,494 Iter 786, Epoch 172.3684, Loss 0.2221
2018-05-24 03:38:22,940 Iter 787, Epoch 172.5877, Loss 0.2281
2018-05-24 03:38:28,635 Iter 788, Epoch 172.8070, Loss 0.2339
2018-05-24 03:38:33,892 Iter 789, Epoch 173.0263, Loss 0.2427
2018-05-24 03:38:39,500 Iter 790, Epoch 173.2456, Loss 0.2094
2018-05-24 03:38:45,211 Iter 791, Epoch 173.4649, Loss 0.2557
2018-05-24 03:38:50,471 Iter 792, Epoch 173.6842, Loss 0.2340
2018-05-24 03:38:56,017 Iter 793, Epoch 173.9035, Loss 0.2436
2018-05-24 03:39:02,237 Iter 794, Epoch 174.1228, Loss 0.2389
2018-05-24 03:39:07,493 Iter 795, Epoch 174.3421, Loss 0.2413
2018-05-24 03:39:12,992 Iter 796, Epoch 174.5614, Loss 0.2417
2018-05-24 03:39:18,252 Iter 797, Epoch 174.7807, Loss 0.2082
2018-05-24 03:39:23,840 Iter 798, Epoch 175.0000, Loss 0.2359
2018-05-24 03:39:29,384 Iter 799, Epoch 175.2193, Loss 0.2347
2018-05-24 03:39:35,569 Iter 800, Epoch 175.4386, Loss 0.2267
2018-05-24 03:39:40,824 Iter 801, Epoch 175.6579, Loss 0.2548
2018-05-24 03:39:46,290 Iter 802, Epoch 175.8772, Loss 0.2145
2018-05-24 03:39:51,907 Iter 803, Epoch 176.0965, Loss 0.2092
2018-05-24 03:39:57,687 Iter 804, Epoch 176.3158, Loss 0.2216
2018-05-24 03:40:03,367 Iter 805, Epoch 176.5351, Loss 0.2157
2018-05-24 03:40:09,556 Iter 806, Epoch 176.7544, Loss 0.2373
2018-05-24 03:40:14,653 Iter 807, Epoch 176.9737, Loss 0.2380
2018-05-24 03:40:20,272 Iter 808, Epoch 177.1930, Loss 0.2103
2018-05-24 03:40:25,947 Iter 809, Epoch 177.4123, Loss 0.2402
2018-05-24 03:40:31,411 Iter 810, Epoch 177.6316, Loss 0.2504
2018-05-24 03:40:37,052 Iter 811, Epoch 177.8509, Loss 0.2264
2018-05-24 03:40:42,313 Iter 812, Epoch 178.0702, Loss 0.2010
2018-05-24 03:40:48,334 Iter 813, Epoch 178.2895, Loss 0.2320
2018-05-24 03:40:54,034 Iter 814, Epoch 178.5088, Loss 0.2290
2018-05-24 03:40:59,653 Iter 815, Epoch 178.7281, Loss 0.2379
2018-05-24 03:41:05,294 Iter 816, Epoch 178.9474, Loss 0.2439
2018-05-24 03:41:10,556 Iter 817, Epoch 179.1667, Loss 0.2185
2018-05-24 03:41:16,632 Iter 818, Epoch 179.3860, Loss 0.2365
2018-05-24 03:41:22,379 Iter 819, Epoch 179.6053, Loss 0.2523
2018-05-24 03:41:28,017 Iter 820, Epoch 179.8246, Loss 0.2240
2018-05-24 03:41:33,713 Iter 821, Epoch 180.0439, Loss 0.2089
2018-05-24 03:41:39,443 Iter 822, Epoch 180.2632, Loss 0.2232
2018-05-24 03:41:44,705 Iter 823, Epoch 180.4825, Loss 0.2261
2018-05-24 03:41:50,686 Iter 824, Epoch 180.7018, Loss 0.2356
2018-05-24 03:41:56,384 Iter 825, Epoch 180.9211, Loss 0.2445
2018-05-24 03:42:01,979 Iter 826, Epoch 181.1404, Loss 0.2162
2018-05-24 03:42:07,582 Iter 827, Epoch 181.3596, Loss 0.2354
2018-05-24 03:42:13,062 Iter 828, Epoch 181.5789, Loss 0.2483
2018-05-24 03:42:19,033 Iter 829, Epoch 181.7982, Loss 0.2299
2018-05-24 03:42:24,293 Iter 830, Epoch 182.0175, Loss 0.2462
2018-05-24 03:42:29,937 Iter 831, Epoch 182.2368, Loss 0.2107
2018-05-24 03:42:35,809 Iter 832, Epoch 182.4561, Loss 0.2466
2018-05-24 03:42:41,506 Iter 833, Epoch 182.6754, Loss 0.2679
2018-05-24 03:42:46,763 Iter 834, Epoch 182.8947, Loss 0.2285
2018-05-24 03:42:52,024 Iter 835, Epoch 183.1140, Loss 0.2320
2018-05-24 03:42:58,137 Iter 836, Epoch 183.3333, Loss 0.2204
2018-05-24 03:43:03,391 Iter 837, Epoch 183.5526, Loss 0.2015
2018-05-24 03:43:09,338 Iter 838, Epoch 183.7719, Loss 0.2253
2018-05-24 03:43:14,598 Iter 839, Epoch 183.9912, Loss 0.2661
2018-05-24 03:43:19,859 Iter 840, Epoch 184.2105, Loss 0.2440
2018-05-24 03:43:25,408 Iter 841, Epoch 184.4298, Loss 0.2325
2018-05-24 03:43:30,969 Iter 842, Epoch 184.6491, Loss 0.2358
2018-05-24 03:43:36,829 Iter 843, Epoch 184.8684, Loss 0.1841
2018-05-24 03:43:42,339 Iter 844, Epoch 185.0877, Loss 0.2272
2018-05-24 03:43:47,766 Iter 845, Epoch 185.3070, Loss 0.2219
2018-05-24 03:43:53,334 Iter 846, Epoch 185.5263, Loss 0.2279
2018-05-24 03:43:58,964 Iter 847, Epoch 185.7456, Loss 0.2276
2018-05-24 03:44:04,226 Iter 848, Epoch 185.9649, Loss 0.2418
2018-05-24 03:44:10,545 Iter 849, Epoch 186.1842, Loss 0.2299
2018-05-24 03:44:15,801 Iter 850, Epoch 186.4035, Loss 0.2289
2018-05-24 03:44:21,060 Iter 851, Epoch 186.6228, Loss 0.2410
2018-05-24 03:44:27,093 Iter 852, Epoch 186.8421, Loss 0.1959
2018-05-24 03:44:32,686 Iter 853, Epoch 187.0614, Loss 0.2081
2018-05-24 03:44:38,547 Iter 854, Epoch 187.2807, Loss 0.2407
2018-05-24 03:44:43,802 Iter 855, Epoch 187.5000, Loss 0.2344
2018-05-24 03:44:49,306 Iter 856, Epoch 187.7193, Loss 0.2031
2018-05-24 03:44:54,844 Iter 857, Epoch 187.9386, Loss 0.2235
2018-05-24 03:45:00,368 Iter 858, Epoch 188.1579, Loss 0.2147
2018-05-24 03:45:05,626 Iter 859, Epoch 188.3772, Loss 0.2355
2018-05-24 03:45:11,383 Iter 860, Epoch 188.5965, Loss 0.2160
2018-05-24 03:45:17,365 Iter 861, Epoch 188.8158, Loss 0.2351
2018-05-24 03:45:23,001 Iter 862, Epoch 189.0351, Loss 0.2282
2018-05-24 03:45:28,260 Iter 863, Epoch 189.2544, Loss 0.2304
2018-05-24 03:45:34,142 Iter 864, Epoch 189.4737, Loss 0.2204
2018-05-24 03:45:39,813 Iter 865, Epoch 189.6930, Loss 0.2478
2018-05-24 03:45:45,076 Iter 866, Epoch 189.9123, Loss 0.2214
2018-05-24 03:45:50,937 Iter 867, Epoch 190.1316, Loss 0.2180
2018-05-24 03:45:56,523 Iter 868, Epoch 190.3509, Loss 0.2270
2018-05-24 03:46:02,089 Iter 869, Epoch 190.5702, Loss 0.2123
2018-05-24 03:46:07,793 Iter 870, Epoch 190.7895, Loss 0.2331
2018-05-24 03:46:13,485 Iter 871, Epoch 191.0088, Loss 0.2370
2018-05-24 03:46:19,188 Iter 872, Epoch 191.2281, Loss 0.2411
2018-05-24 03:46:24,874 Iter 873, Epoch 191.4474, Loss 0.2185
2018-05-24 03:46:30,351 Iter 874, Epoch 191.6667, Loss 0.2244
2018-05-24 03:46:35,611 Iter 875, Epoch 191.8860, Loss 0.2003
2018-05-24 03:46:41,142 Iter 876, Epoch 192.1053, Loss 0.2196
2018-05-24 03:46:46,798 Iter 877, Epoch 192.3246, Loss 0.2402
2018-05-24 03:46:52,506 Iter 878, Epoch 192.5439, Loss 0.2180
2018-05-24 03:46:58,545 Iter 879, Epoch 192.7632, Loss 0.2136
2018-05-24 03:47:04,200 Iter 880, Epoch 192.9825, Loss 0.2246
2018-05-24 03:47:09,902 Iter 881, Epoch 193.2018, Loss 0.2218
2018-05-24 03:47:15,589 Iter 882, Epoch 193.4211, Loss 0.2234
2018-05-24 03:47:21,227 Iter 883, Epoch 193.6404, Loss 0.2574
2018-05-24 03:47:27,005 Iter 884, Epoch 193.8596, Loss 0.2335
2018-05-24 03:47:32,680 Iter 885, Epoch 194.0789, Loss 0.2480
2018-05-24 03:47:38,289 Iter 886, Epoch 194.2982, Loss 0.2416
2018-05-24 03:47:43,542 Iter 887, Epoch 194.5175, Loss 0.2270
2018-05-24 03:47:49,094 Iter 888, Epoch 194.7368, Loss 0.2325
2018-05-24 03:47:55,369 Iter 889, Epoch 194.9561, Loss 0.2335
2018-05-24 03:48:00,630 Iter 890, Epoch 195.1754, Loss 0.2153
2018-05-24 03:48:06,260 Iter 891, Epoch 195.3947, Loss 0.2459
2018-05-24 03:48:11,874 Iter 892, Epoch 195.6140, Loss 0.2578
2018-05-24 03:48:17,729 Iter 893, Epoch 195.8333, Loss 0.2228
2018-05-24 03:48:22,985 Iter 894, Epoch 196.0526, Loss 0.2162
2018-05-24 03:48:28,240 Iter 895, Epoch 196.2719, Loss 0.2324
2018-05-24 03:48:34,630 Iter 896, Epoch 196.4912, Loss 0.2121
2018-05-24 03:48:39,889 Iter 897, Epoch 196.7105, Loss 0.2200
2018-05-24 03:48:45,414 Iter 898, Epoch 196.9298, Loss 0.2632
2018-05-24 03:48:51,113 Iter 899, Epoch 197.1491, Loss 0.2501
2018-05-24 03:48:56,727 Iter 900, Epoch 197.3684, Loss 0.2230
2018-05-24 03:49:02,234 Iter 901, Epoch 197.5877, Loss 0.2327
2018-05-24 03:49:07,925 Iter 902, Epoch 197.8070, Loss 0.2109
2018-05-24 03:49:13,507 Iter 903, Epoch 198.0263, Loss 0.2284
2018-05-24 03:49:19,005 Iter 904, Epoch 198.2456, Loss 0.2059
2018-05-24 03:49:24,263 Iter 905, Epoch 198.4649, Loss 0.2444
2018-05-24 03:49:30,199 Iter 906, Epoch 198.6842, Loss 0.2157
2018-05-24 03:49:35,456 Iter 907, Epoch 198.9035, Loss 0.2286
2018-05-24 03:49:41,823 Iter 908, Epoch 199.1228, Loss 0.2069
2018-05-24 03:49:47,262 Iter 909, Epoch 199.3421, Loss 0.2284
2018-05-24 03:49:52,879 Iter 910, Epoch 199.5614, Loss 0.2759
2018-05-24 03:49:58,465 Iter 911, Epoch 199.7807, Loss 0.2283
2018-05-24 03:50:03,717 Iter 912, Epoch 200.0000, Loss 0.2048
2018-05-24 03:50:09,713 Iter 913, Epoch 200.2193, Loss 0.2358
2018-05-24 03:50:14,969 Iter 914, Epoch 200.4386, Loss 0.2190
2018-05-24 03:50:21,230 Iter 915, Epoch 200.6579, Loss 0.2192
2018-05-24 03:50:26,485 Iter 916, Epoch 200.8772, Loss 0.2620
2018-05-24 03:50:32,006 Iter 917, Epoch 201.0965, Loss 0.2107
2018-05-24 03:50:37,820 Iter 918, Epoch 201.3158, Loss 0.2410
2018-05-24 03:50:43,076 Iter 919, Epoch 201.5351, Loss 0.2224
2018-05-24 03:50:48,769 Iter 920, Epoch 201.7544, Loss 0.2395
2018-05-24 03:50:54,312 Iter 921, Epoch 201.9737, Loss 0.2490
2018-05-24 03:50:59,843 Iter 922, Epoch 202.1930, Loss 0.2381
2018-05-24 03:51:05,379 Iter 923, Epoch 202.4123, Loss 0.2284
2018-05-24 03:51:11,197 Iter 924, Epoch 202.6316, Loss 0.2172
2018-05-24 03:51:16,455 Iter 925, Epoch 202.8509, Loss 0.2322
2018-05-24 03:51:22,209 Iter 926, Epoch 203.0702, Loss 0.2182
2018-05-24 03:51:27,881 Iter 927, Epoch 203.2895, Loss 0.2132
2018-05-24 03:51:33,457 Iter 928, Epoch 203.5088, Loss 0.2031
2018-05-24 03:51:39,033 Iter 929, Epoch 203.7281, Loss 0.2267
2018-05-24 03:51:44,459 Iter 930, Epoch 203.9474, Loss 0.2224
2018-05-24 03:51:49,714 Iter 931, Epoch 204.1667, Loss 0.2236
2018-05-24 03:51:55,235 Iter 932, Epoch 204.3860, Loss 0.2159
2018-05-24 03:52:01,296 Iter 933, Epoch 204.6053, Loss 0.2199
2018-05-24 03:52:06,555 Iter 934, Epoch 204.8246, Loss 0.2350
2018-05-24 03:52:12,108 Iter 935, Epoch 205.0439, Loss 0.2588
2018-05-24 03:52:18,088 Iter 936, Epoch 205.2632, Loss 0.2158
2018-05-24 03:52:23,669 Iter 937, Epoch 205.4825, Loss 0.2443
2018-05-24 03:52:29,211 Iter 938, Epoch 205.7018, Loss 0.2347
2018-05-24 03:52:34,966 Iter 939, Epoch 205.9211, Loss 0.2340
2018-05-24 03:52:40,963 Iter 940, Epoch 206.1404, Loss 0.2288
2018-05-24 03:52:46,220 Iter 941, Epoch 206.3596, Loss 0.2005
2018-05-24 03:52:51,481 Iter 942, Epoch 206.5789, Loss 0.2184
2018-05-24 03:52:57,499 Iter 943, Epoch 206.7982, Loss 0.2400
2018-05-24 03:53:03,115 Iter 944, Epoch 207.0175, Loss 0.2306
2018-05-24 03:53:08,691 Iter 945, Epoch 207.2368, Loss 0.2210
2018-05-24 03:53:14,332 Iter 946, Epoch 207.4561, Loss 0.2205
2018-05-24 03:53:19,872 Iter 947, Epoch 207.6754, Loss 0.2261
2018-05-24 03:53:25,516 Iter 948, Epoch 207.8947, Loss 0.2202
2018-05-24 03:53:31,192 Iter 949, Epoch 208.1140, Loss 0.2263
2018-05-24 03:53:36,963 Iter 950, Epoch 208.3333, Loss 0.2230
2018-05-24 03:53:42,796 Iter 951, Epoch 208.5526, Loss 0.2352
2018-05-24 03:53:48,053 Iter 952, Epoch 208.7719, Loss 0.2262
2018-05-24 03:53:53,312 Iter 953, Epoch 208.9912, Loss 0.2308
2018-05-24 03:53:58,962 Iter 954, Epoch 209.2105, Loss 0.2248
2018-05-24 03:54:04,605 Iter 955, Epoch 209.4298, Loss 0.2496
2018-05-24 03:54:10,312 Iter 956, Epoch 209.6491, Loss 0.2148
2018-05-24 03:54:16,398 Iter 957, Epoch 209.8684, Loss 0.2215
2018-05-24 03:54:21,943 Iter 958, Epoch 210.0877, Loss 0.2196
2018-05-24 03:54:27,513 Iter 959, Epoch 210.3070, Loss 0.2230
2018-05-24 03:54:32,764 Iter 960, Epoch 210.5263, Loss 0.2499
2018-05-24 03:54:38,744 Iter 961, Epoch 210.7456, Loss 0.2279
2018-05-24 03:54:44,003 Iter 962, Epoch 210.9649, Loss 0.2372
2018-05-24 03:54:50,000 Iter 963, Epoch 211.1842, Loss 0.2147
2018-05-24 03:54:55,667 Iter 964, Epoch 211.4035, Loss 0.2071
2018-05-24 03:55:01,155 Iter 965, Epoch 211.6228, Loss 0.2425
2018-05-24 03:55:07,226 Iter 966, Epoch 211.8421, Loss 0.2571
2018-05-24 03:55:12,483 Iter 967, Epoch 212.0614, Loss 0.2199
2018-05-24 03:55:17,740 Iter 968, Epoch 212.2807, Loss 0.2374
2018-05-24 03:55:23,706 Iter 969, Epoch 212.5000, Loss 0.2082
2018-05-24 03:55:29,307 Iter 970, Epoch 212.7193, Loss 0.2337
2018-05-24 03:55:34,859 Iter 971, Epoch 212.9386, Loss 0.2346
2018-05-24 03:55:40,626 Iter 972, Epoch 213.1579, Loss 0.2415
2018-05-24 03:55:45,876 Iter 973, Epoch 213.3772, Loss 0.2074
2018-05-24 03:55:51,533 Iter 974, Epoch 213.5965, Loss 0.2334
2018-05-24 03:55:57,502 Iter 975, Epoch 213.8158, Loss 0.2311
2018-05-24 03:56:03,191 Iter 976, Epoch 214.0351, Loss 0.2253
2018-05-24 03:56:08,662 Iter 977, Epoch 214.2544, Loss 0.2226
2018-05-24 03:56:13,923 Iter 978, Epoch 214.4737, Loss 0.2301
2018-05-24 03:56:19,903 Iter 979, Epoch 214.6930, Loss 0.2131
2018-05-24 03:56:25,500 Iter 980, Epoch 214.9123, Loss 0.2134
2018-05-24 03:56:31,271 Iter 981, Epoch 215.1316, Loss 0.2140
2018-05-24 03:56:36,528 Iter 982, Epoch 215.3509, Loss 0.2332
2018-05-24 03:56:41,788 Iter 983, Epoch 215.5702, Loss 0.2233
2018-05-24 03:56:47,658 Iter 984, Epoch 215.7895, Loss 0.2360
2018-05-24 03:56:53,361 Iter 985, Epoch 216.0088, Loss 0.2182
2018-05-24 03:56:58,619 Iter 986, Epoch 216.2281, Loss 0.2136
2018-05-24 03:57:04,765 Iter 987, Epoch 216.4474, Loss 0.2094
2018-05-24 03:57:10,017 Iter 988, Epoch 216.6667, Loss 0.2170
2018-05-24 03:57:15,727 Iter 989, Epoch 216.8860, Loss 0.2259
2018-05-24 03:57:21,434 Iter 990, Epoch 217.1053, Loss 0.2315
2018-05-24 03:57:27,096 Iter 991, Epoch 217.3246, Loss 0.2292
2018-05-24 03:57:32,823 Iter 992, Epoch 217.5439, Loss 0.2154
2018-05-24 03:57:38,542 Iter 993, Epoch 217.7632, Loss 0.2276
2018-05-24 03:57:44,068 Iter 994, Epoch 217.9825, Loss 0.2264
2018-05-24 03:57:50,053 Iter 995, Epoch 218.2018, Loss 0.2497
2018-05-24 03:57:55,307 Iter 996, Epoch 218.4211, Loss 0.2338
2018-05-24 03:58:00,565 Iter 997, Epoch 218.6404, Loss 0.2213
2018-05-24 03:58:06,131 Iter 998, Epoch 218.8596, Loss 0.2391
2018-05-24 03:58:12,255 Iter 999, Epoch 219.0789, Loss 0.2219
2018-05-24 03:58:17,951 Iter 1000, Epoch 219.2982, Loss 0.2125
2018-05-24 03:58:23,211 Iter 1001, Epoch 219.5175, Loss 0.2120
2018-05-24 03:58:29,030 Iter 1002, Epoch 219.7368, Loss 0.2305
2018-05-24 03:58:34,751 Iter 1003, Epoch 219.9561, Loss 0.2320
2018-05-24 03:58:40,815 Iter 1004, Epoch 220.1754, Loss 0.2258
2018-05-24 03:58:46,068 Iter 1005, Epoch 220.3947, Loss 0.1969
2018-05-24 03:58:51,524 Iter 1006, Epoch 220.6140, Loss 0.2379
2018-05-24 03:58:56,781 Iter 1007, Epoch 220.8333, Loss 0.2368
2018-05-24 03:59:02,538 Iter 1008, Epoch 221.0526, Loss 0.2168
2018-05-24 03:59:07,794 Iter 1009, Epoch 221.2719, Loss 0.2189
2018-05-24 03:59:13,753 Iter 1010, Epoch 221.4912, Loss 0.2439
2018-05-24 03:59:19,404 Iter 1011, Epoch 221.7105, Loss 0.2179
2018-05-24 03:59:24,956 Iter 1012, Epoch 221.9298, Loss 0.2491
2018-05-24 03:59:30,336 Iter 1013, Epoch 222.1491, Loss 0.2526
2018-05-24 03:59:36,040 Iter 1014, Epoch 222.3684, Loss 0.2356
2018-05-24 03:59:41,301 Iter 1015, Epoch 222.5877, Loss 0.2057
2018-05-24 03:59:47,376 Iter 1016, Epoch 222.8070, Loss 0.2230
2018-05-24 03:59:53,065 Iter 1017, Epoch 223.0263, Loss 0.2206
2018-05-24 03:59:58,536 Iter 1018, Epoch 223.2456, Loss 0.2193
2018-05-24 04:00:04,210 Iter 1019, Epoch 223.4649, Loss 0.2305
2018-05-24 04:00:09,785 Iter 1020, Epoch 223.6842, Loss 0.2323
2018-05-24 04:00:15,515 Iter 1021, Epoch 223.9035, Loss 0.2117
2018-05-24 04:00:20,774 Iter 1022, Epoch 224.1228, Loss 0.2173
2018-05-24 04:00:27,003 Iter 1023, Epoch 224.3421, Loss 0.2284
2018-05-24 04:00:32,260 Iter 1024, Epoch 224.5614, Loss 0.2205
2018-05-24 04:00:37,517 Iter 1025, Epoch 224.7807, Loss 0.2269
2018-05-24 04:00:43,371 Iter 1026, Epoch 225.0000, Loss 0.2196
2018-05-24 04:00:48,629 Iter 1027, Epoch 225.2193, Loss 0.2431
2018-05-24 04:00:54,170 Iter 1028, Epoch 225.4386, Loss 0.2373
2018-05-24 04:01:00,133 Iter 1029, Epoch 225.6579, Loss 0.2284
2018-05-24 04:01:05,677 Iter 1030, Epoch 225.8772, Loss 0.2016
2018-05-24 04:01:11,256 Iter 1031, Epoch 226.0965, Loss 0.2225
2018-05-24 04:01:16,814 Iter 1032, Epoch 226.3158, Loss 0.2219
2018-05-24 04:01:22,590 Iter 1033, Epoch 226.5351, Loss 0.2317
2018-05-24 04:01:27,850 Iter 1034, Epoch 226.7544, Loss 0.2357
2018-05-24 04:01:34,230 Iter 1035, Epoch 226.9737, Loss 0.2339
2018-05-24 04:01:39,485 Iter 1036, Epoch 227.1930, Loss 0.2551
2018-05-24 04:01:45,055 Iter 1037, Epoch 227.4123, Loss 0.2376
2018-05-24 04:01:50,581 Iter 1038, Epoch 227.6316, Loss 0.2218
2018-05-24 04:01:56,146 Iter 1039, Epoch 227.8509, Loss 0.2165
2018-05-24 04:02:01,992 Iter 1040, Epoch 228.0702, Loss 0.2273
2018-05-24 04:02:07,248 Iter 1041, Epoch 228.2895, Loss 0.2100
2018-05-24 04:02:12,902 Iter 1042, Epoch 228.5088, Loss 0.2268
2018-05-24 04:02:18,553 Iter 1043, Epoch 228.7281, Loss 0.2395
2018-05-24 04:02:23,809 Iter 1044, Epoch 228.9474, Loss 0.2274
2018-05-24 04:02:29,934 Iter 1045, Epoch 229.1667, Loss 0.2100
2018-05-24 04:02:35,868 Iter 1046, Epoch 229.3860, Loss 0.2280
2018-05-24 04:02:41,122 Iter 1047, Epoch 229.6053, Loss 0.2341
2018-05-24 04:02:46,746 Iter 1048, Epoch 229.8246, Loss 0.2414
2018-05-24 04:02:52,006 Iter 1049, Epoch 230.0439, Loss 0.2262
2018-05-24 04:02:58,103 Iter 1050, Epoch 230.2632, Loss 0.2110
2018-05-24 04:03:03,361 Iter 1051, Epoch 230.4825, Loss 0.2185
2018-05-24 04:03:09,306 Iter 1052, Epoch 230.7018, Loss 0.2229
2018-05-24 04:03:15,447 Iter 1053, Epoch 230.9211, Loss 0.2275
2018-05-24 04:03:20,702 Iter 1054, Epoch 231.1404, Loss 0.2293
2018-05-24 04:03:26,231 Iter 1055, Epoch 231.3596, Loss 0.2130
2018-05-24 04:03:31,834 Iter 1056, Epoch 231.5789, Loss 0.2041
2018-05-24 04:03:37,095 Iter 1057, Epoch 231.7982, Loss 0.2350
2018-05-24 04:03:43,038 Iter 1058, Epoch 232.0175, Loss 0.2428
2018-05-24 04:03:49,011 Iter 1059, Epoch 232.2368, Loss 0.2047
2018-05-24 04:03:54,692 Iter 1060, Epoch 232.4561, Loss 0.2483
2018-05-24 04:03:59,955 Iter 1061, Epoch 232.6754, Loss 0.2389
2018-05-24 04:04:05,626 Iter 1062, Epoch 232.8947, Loss 0.1951
2018-05-24 04:04:10,887 Iter 1063, Epoch 233.1140, Loss 0.2171
2018-05-24 04:04:16,500 Iter 1064, Epoch 233.3333, Loss 0.2522
2018-05-24 04:04:22,634 Iter 1065, Epoch 233.5526, Loss 0.2020
2018-05-24 04:04:28,177 Iter 1066, Epoch 233.7719, Loss 0.2026
2018-05-24 04:04:33,438 Iter 1067, Epoch 233.9912, Loss 0.2291
2018-05-24 04:04:39,486 Iter 1068, Epoch 234.2105, Loss 0.2334
2018-05-24 04:04:45,195 Iter 1069, Epoch 234.4298, Loss 0.2213
2018-05-24 04:04:50,455 Iter 1070, Epoch 234.6491, Loss 0.2065
2018-05-24 04:04:56,359 Iter 1071, Epoch 234.8684, Loss 0.2492
2018-05-24 04:05:02,015 Iter 1072, Epoch 235.0877, Loss 0.2313
2018-05-24 04:05:07,505 Iter 1073, Epoch 235.3070, Loss 0.2058
2018-05-24 04:05:12,942 Iter 1074, Epoch 235.5263, Loss 0.2133
2018-05-24 04:05:18,201 Iter 1075, Epoch 235.7456, Loss 0.2164
2018-05-24 04:05:24,318 Iter 1076, Epoch 235.9649, Loss 0.2238
2018-05-24 04:05:29,817 Iter 1077, Epoch 236.1842, Loss 0.2223
2018-05-24 04:05:35,580 Iter 1078, Epoch 236.4035, Loss 0.2032
2018-05-24 04:05:41,154 Iter 1079, Epoch 236.6228, Loss 0.2269
2018-05-24 04:05:46,408 Iter 1080, Epoch 236.8421, Loss 0.2323
2018-05-24 04:05:51,966 Iter 1081, Epoch 237.0614, Loss 0.2202
2018-05-24 04:05:58,090 Iter 1082, Epoch 237.2807, Loss 0.2451
2018-05-24 04:06:03,862 Iter 1083, Epoch 237.5000, Loss 0.2026
2018-05-24 04:06:09,123 Iter 1084, Epoch 237.7193, Loss 0.2247
2018-05-24 04:06:14,382 Iter 1085, Epoch 237.9386, Loss 0.2263
2018-05-24 04:06:20,219 Iter 1086, Epoch 238.1579, Loss 0.1961
2018-05-24 04:06:25,751 Iter 1087, Epoch 238.3772, Loss 0.2210
2018-05-24 04:06:31,010 Iter 1088, Epoch 238.5965, Loss 0.2229
2018-05-24 04:06:37,015 Iter 1089, Epoch 238.8158, Loss 0.2279
2018-05-24 04:06:42,601 Iter 1090, Epoch 239.0351, Loss 0.2227
2018-05-24 04:06:47,862 Iter 1091, Epoch 239.2544, Loss 0.2088
2018-05-24 04:06:53,567 Iter 1092, Epoch 239.4737, Loss 0.2156
2018-05-24 04:06:59,590 Iter 1093, Epoch 239.6930, Loss 0.2382
2018-05-24 04:07:04,839 Iter 1094, Epoch 239.9123, Loss 0.2348
2018-05-24 04:07:10,832 Iter 1095, Epoch 240.1316, Loss 0.2205
2018-05-24 04:07:16,343 Iter 1096, Epoch 240.3509, Loss 0.2284
2018-05-24 04:07:21,599 Iter 1097, Epoch 240.5702, Loss 0.2293
2018-05-24 04:07:27,560 Iter 1098, Epoch 240.7895, Loss 0.2170
2018-05-24 04:07:33,155 Iter 1099, Epoch 241.0088, Loss 0.2134
2018-05-24 04:07:38,407 Iter 1100, Epoch 241.2281, Loss 0.2302
2018-05-24 04:07:44,484 Iter 1101, Epoch 241.4474, Loss 0.2240
2018-05-24 04:07:50,223 Iter 1102, Epoch 241.6667, Loss 0.2057
2018-05-24 04:07:55,479 Iter 1103, Epoch 241.8860, Loss 0.2338
2018-05-24 04:08:01,080 Iter 1104, Epoch 242.1053, Loss 0.2187
2018-05-24 04:08:06,591 Iter 1105, Epoch 242.3246, Loss 0.1989
2018-05-24 04:08:12,335 Iter 1106, Epoch 242.5439, Loss 0.2059
2018-05-24 04:08:17,892 Iter 1107, Epoch 242.7632, Loss 0.2465
2018-05-24 04:08:23,839 Iter 1108, Epoch 242.9825, Loss 0.2349
2018-05-24 04:08:29,098 Iter 1109, Epoch 243.2018, Loss 0.2097
2018-05-24 04:08:34,690 Iter 1110, Epoch 243.4211, Loss 0.2340
2018-05-24 04:08:39,947 Iter 1111, Epoch 243.6404, Loss 0.2237
2018-05-24 04:08:45,876 Iter 1112, Epoch 243.8596, Loss 0.2208
2018-05-24 04:08:51,732 Iter 1113, Epoch 244.0789, Loss 0.2427
2018-05-24 04:08:56,983 Iter 1114, Epoch 244.2982, Loss 0.2238
2018-05-24 04:09:02,242 Iter 1115, Epoch 244.5175, Loss 0.2279
2018-05-24 04:09:07,869 Iter 1116, Epoch 244.7368, Loss 0.2201
2018-05-24 04:09:13,472 Iter 1117, Epoch 244.9561, Loss 0.2323
2018-05-24 04:09:19,182 Iter 1118, Epoch 245.1754, Loss 0.2508
2018-05-24 04:09:25,248 Iter 1119, Epoch 245.3947, Loss 0.2105
2018-05-24 04:09:30,886 Iter 1120, Epoch 245.6140, Loss 0.2091
2018-05-24 04:09:36,484 Iter 1121, Epoch 245.8333, Loss 0.1987
2018-05-24 04:09:42,616 Iter 1122, Epoch 246.0526, Loss 0.2489
2018-05-24 04:09:47,866 Iter 1123, Epoch 246.2719, Loss 0.2158
2018-05-24 04:09:53,947 Iter 1124, Epoch 246.4912, Loss 0.1873
2018-05-24 04:09:59,203 Iter 1125, Epoch 246.7105, Loss 0.2152
2018-05-24 04:10:04,850 Iter 1126, Epoch 246.9298, Loss 0.2152
2018-05-24 04:10:10,108 Iter 1127, Epoch 247.1491, Loss 0.2091
2018-05-24 04:10:15,971 Iter 1128, Epoch 247.3684, Loss 0.2209
2018-05-24 04:10:21,619 Iter 1129, Epoch 247.5877, Loss 0.2199
2018-05-24 04:10:27,240 Iter 1130, Epoch 247.8070, Loss 0.2144
2018-05-24 04:10:33,053 Iter 1131, Epoch 248.0263, Loss 0.2479
2018-05-24 04:10:38,545 Iter 1132, Epoch 248.2456, Loss 0.2221
2018-05-24 04:10:43,802 Iter 1133, Epoch 248.4649, Loss 0.2405
2018-05-24 04:10:49,063 Iter 1134, Epoch 248.6842, Loss 0.2115
2018-05-24 04:10:55,344 Iter 1135, Epoch 248.9035, Loss 0.2145
2018-05-24 04:11:00,863 Iter 1136, Epoch 249.1228, Loss 0.2368
2018-05-24 04:11:06,593 Iter 1137, Epoch 249.3421, Loss 0.2197
2018-05-24 04:11:12,130 Iter 1138, Epoch 249.5614, Loss 0.2395
2018-05-24 04:11:17,753 Iter 1139, Epoch 249.7807, Loss 0.2319
2018-05-24 04:11:23,013 Iter 1140, Epoch 250.0000, Loss 0.2240
2018-05-24 04:11:28,898 Iter 1141, Epoch 250.2193, Loss 0.2197
2018-05-24 04:11:34,156 Iter 1142, Epoch 250.4386, Loss 0.2215
2018-05-24 04:11:40,438 Iter 1143, Epoch 250.6579, Loss 0.2197
2018-05-24 04:11:45,690 Iter 1144, Epoch 250.8772, Loss 0.2376
2018-05-24 04:11:51,402 Iter 1145, Epoch 251.0965, Loss 0.1878
2018-05-24 04:11:56,662 Iter 1146, Epoch 251.3158, Loss 0.2199
2018-05-24 04:12:02,656 Iter 1147, Epoch 251.5351, Loss 0.2315
2018-05-24 04:12:07,915 Iter 1148, Epoch 251.7544, Loss 0.2212
2018-05-24 04:12:14,383 Iter 1149, Epoch 251.9737, Loss 0.2020
2018-05-24 04:12:19,639 Iter 1150, Epoch 252.1930, Loss 0.2230
2018-05-24 04:12:25,513 Iter 1151, Epoch 252.4123, Loss 0.2325
2018-05-24 04:12:30,771 Iter 1152, Epoch 252.6316, Loss 0.2197
2018-05-24 04:12:36,484 Iter 1153, Epoch 252.8509, Loss 0.2241
2018-05-24 04:12:41,734 Iter 1154, Epoch 253.0702, Loss 0.1949
2018-05-24 04:12:47,782 Iter 1155, Epoch 253.2895, Loss 0.2163
2018-05-24 04:12:53,248 Iter 1156, Epoch 253.5088, Loss 0.2183
2018-05-24 04:12:58,508 Iter 1157, Epoch 253.7281, Loss 0.2137
2018-05-24 04:13:04,194 Iter 1158, Epoch 253.9474, Loss 0.2350
2018-05-24 04:13:09,970 Iter 1159, Epoch 254.1667, Loss 0.2082
2018-05-24 04:13:15,784 Iter 1160, Epoch 254.3860, Loss 0.2304
2018-05-24 04:13:21,643 Iter 1161, Epoch 254.6053, Loss 0.2451
2018-05-24 04:13:26,899 Iter 1162, Epoch 254.8246, Loss 0.2039
2018-05-24 04:13:32,161 Iter 1163, Epoch 255.0439, Loss 0.2243
2018-05-24 04:13:38,289 Iter 1164, Epoch 255.2632, Loss 0.1982
2018-05-24 04:13:44,058 Iter 1165, Epoch 255.4825, Loss 0.2126
2018-05-24 04:13:49,793 Iter 1166, Epoch 255.7018, Loss 0.2295
2018-05-24 04:13:55,340 Iter 1167, Epoch 255.9211, Loss 0.2219
2018-05-24 04:14:01,110 Iter 1168, Epoch 256.1404, Loss 0.2259
2018-05-24 04:14:06,652 Iter 1169, Epoch 256.3596, Loss 0.2178
2018-05-24 04:14:12,417 Iter 1170, Epoch 256.5789, Loss 0.2147
2018-05-24 04:14:18,099 Iter 1171, Epoch 256.7982, Loss 0.2085
2018-05-24 04:14:23,664 Iter 1172, Epoch 257.0175, Loss 0.2411
2018-05-24 04:14:29,231 Iter 1173, Epoch 257.2368, Loss 0.2307
2018-05-24 04:14:34,817 Iter 1174, Epoch 257.4561, Loss 0.2019
2018-05-24 04:14:40,071 Iter 1175, Epoch 257.6754, Loss 0.2345
2018-05-24 04:14:46,241 Iter 1176, Epoch 257.8947, Loss 0.2253
2018-05-24 04:14:51,499 Iter 1177, Epoch 258.1140, Loss 0.2711
2018-05-24 04:14:57,240 Iter 1178, Epoch 258.3333, Loss 0.2538
2018-05-24 04:15:03,141 Iter 1179, Epoch 258.5526, Loss 0.2007
2018-05-24 04:15:08,636 Iter 1180, Epoch 258.7719, Loss 0.2071
2018-05-24 04:15:14,692 Iter 1181, Epoch 258.9912, Loss 0.2067
2018-05-24 04:15:19,943 Iter 1182, Epoch 259.2105, Loss 0.2233
2018-05-24 04:15:25,519 Iter 1183, Epoch 259.4298, Loss 0.2048
2018-05-24 04:15:30,775 Iter 1184, Epoch 259.6491, Loss 0.2124
2018-05-24 04:15:36,926 Iter 1185, Epoch 259.8684, Loss 0.2201
2018-05-24 04:15:42,182 Iter 1186, Epoch 260.0877, Loss 0.2412
2018-05-24 04:15:47,440 Iter 1187, Epoch 260.3070, Loss 0.2300
2018-05-24 04:15:53,294 Iter 1188, Epoch 260.5263, Loss 0.2175
2018-05-24 04:15:58,554 Iter 1189, Epoch 260.7456, Loss 0.1995
2018-05-24 04:16:04,099 Iter 1190, Epoch 260.9649, Loss 0.2254
2018-05-24 04:16:10,332 Iter 1191, Epoch 261.1842, Loss 0.2135
2018-05-24 04:16:16,180 Iter 1192, Epoch 261.4035, Loss 0.2134
2018-05-24 04:16:21,437 Iter 1193, Epoch 261.6228, Loss 0.2354
2018-05-24 04:16:26,929 Iter 1194, Epoch 261.8421, Loss 0.2271
2018-05-24 04:16:32,515 Iter 1195, Epoch 262.0614, Loss 0.1928
2018-05-24 04:16:38,143 Iter 1196, Epoch 262.2807, Loss 0.2151
2018-05-24 04:16:44,078 Iter 1197, Epoch 262.5000, Loss 0.2473
2018-05-24 04:16:49,334 Iter 1198, Epoch 262.7193, Loss 0.2092
2018-05-24 04:16:54,953 Iter 1199, Epoch 262.9386, Loss 0.2032
2018-05-24 04:17:00,463 Iter 1200, Epoch 263.1579, Loss 0.2179
2018-05-24 04:17:06,138 Iter 1201, Epoch 263.3772, Loss 0.2184
2018-05-24 04:17:11,778 Iter 1202, Epoch 263.5965, Loss 0.2434
2018-05-24 04:17:17,213 Iter 1203, Epoch 263.8158, Loss 0.2306
2018-05-24 04:17:22,889 Iter 1204, Epoch 264.0351, Loss 0.2412
2018-05-24 04:17:28,148 Iter 1205, Epoch 264.2544, Loss 0.2451
2018-05-24 04:17:34,199 Iter 1206, Epoch 264.4737, Loss 0.2138
2018-05-24 04:17:39,814 Iter 1207, Epoch 264.6930, Loss 0.2237
2018-05-24 04:17:45,357 Iter 1208, Epoch 264.9123, Loss 0.2220
2018-05-24 04:17:51,039 Iter 1209, Epoch 265.1316, Loss 0.2033
2018-05-24 04:17:56,496 Iter 1210, Epoch 265.3509, Loss 0.2287
2018-05-24 04:18:02,105 Iter 1211, Epoch 265.5702, Loss 0.2084
2018-05-24 04:18:07,365 Iter 1212, Epoch 265.7895, Loss 0.2289
2018-05-24 04:18:13,327 Iter 1213, Epoch 266.0088, Loss 0.2216
2018-05-24 04:18:18,929 Iter 1214, Epoch 266.2281, Loss 0.2207
2018-05-24 04:18:24,704 Iter 1215, Epoch 266.4474, Loss 0.2278
2018-05-24 04:18:30,309 Iter 1216, Epoch 266.6667, Loss 0.2165
2018-05-24 04:18:36,023 Iter 1217, Epoch 266.8860, Loss 0.2226
2018-05-24 04:18:41,283 Iter 1218, Epoch 267.1053, Loss 0.2101
2018-05-24 04:18:47,295 Iter 1219, Epoch 267.3246, Loss 0.2315
2018-05-24 04:18:53,373 Iter 1220, Epoch 267.5439, Loss 0.2293
2018-05-24 04:18:58,820 Iter 1221, Epoch 267.7632, Loss 0.2414
2018-05-24 04:19:04,074 Iter 1222, Epoch 267.9825, Loss 0.2117
2018-05-24 04:19:09,331 Iter 1223, Epoch 268.2018, Loss 0.2341
2018-05-24 04:19:15,670 Iter 1224, Epoch 268.4211, Loss 0.2180
2018-05-24 04:19:20,925 Iter 1225, Epoch 268.6404, Loss 0.2070
2018-05-24 04:19:26,525 Iter 1226, Epoch 268.8596, Loss 0.2252
2018-05-24 04:19:32,095 Iter 1227, Epoch 269.0789, Loss 0.2067
2018-05-24 04:19:37,610 Iter 1228, Epoch 269.2982, Loss 0.2451
2018-05-24 04:19:43,078 Iter 1229, Epoch 269.5175, Loss 0.2197
2018-05-24 04:19:48,342 Iter 1230, Epoch 269.7368, Loss 0.2367
2018-05-24 04:19:54,465 Iter 1231, Epoch 269.9561, Loss 0.2196
2018-05-24 04:20:00,233 Iter 1232, Epoch 270.1754, Loss 0.2062
2018-05-24 04:20:05,499 Iter 1233, Epoch 270.3947, Loss 0.2051
2018-05-24 04:20:11,352 Iter 1234, Epoch 270.6140, Loss 0.2112
2018-05-24 04:20:16,611 Iter 1235, Epoch 270.8333, Loss 0.2260
2018-05-24 04:20:22,750 Iter 1236, Epoch 271.0526, Loss 0.2257
2018-05-24 04:20:28,322 Iter 1237, Epoch 271.2719, Loss 0.2239
2018-05-24 04:20:34,057 Iter 1238, Epoch 271.4912, Loss 0.2232
2018-05-24 04:20:39,316 Iter 1239, Epoch 271.7105, Loss 0.2048
2018-05-24 04:20:44,777 Iter 1240, Epoch 271.9298, Loss 0.2281
2018-05-24 04:20:50,406 Iter 1241, Epoch 272.1491, Loss 0.1975
2018-05-24 04:20:56,273 Iter 1242, Epoch 272.3684, Loss 0.2208
2018-05-24 04:21:01,533 Iter 1243, Epoch 272.5877, Loss 0.2299
2018-05-24 04:21:07,577 Iter 1244, Epoch 272.8070, Loss 0.2156
2018-05-24 04:21:12,837 Iter 1245, Epoch 273.0263, Loss 0.2386
2018-05-24 04:21:18,317 Iter 1246, Epoch 273.2456, Loss 0.2377
2018-05-24 04:21:23,799 Iter 1247, Epoch 273.4649, Loss 0.2283
2018-05-24 04:21:29,228 Iter 1248, Epoch 273.6842, Loss 0.2029
2018-05-24 04:21:35,129 Iter 1249, Epoch 273.9035, Loss 0.2486
2018-05-24 04:21:40,773 Iter 1250, Epoch 274.1228, Loss 0.2033
2018-05-24 04:21:46,033 Iter 1251, Epoch 274.3421, Loss 0.2387
2018-05-24 04:21:51,717 Iter 1252, Epoch 274.5614, Loss 0.2162
2018-05-24 04:21:57,713 Iter 1253, Epoch 274.7807, Loss 0.2099
2018-05-24 04:22:02,972 Iter 1254, Epoch 275.0000, Loss 0.2241
2018-05-24 04:22:08,233 Iter 1255, Epoch 275.2193, Loss 0.2242
2018-05-24 04:22:13,782 Iter 1256, Epoch 275.4386, Loss 0.2141
2018-05-24 04:22:19,765 Iter 1257, Epoch 275.6579, Loss 0.2094
2018-05-24 04:22:25,292 Iter 1258, Epoch 275.8772, Loss 0.1901
2018-05-24 04:22:30,550 Iter 1259, Epoch 276.0965, Loss 0.2032
2018-05-24 04:22:36,206 Iter 1260, Epoch 276.3158, Loss 0.2235
2018-05-24 04:22:41,778 Iter 1261, Epoch 276.5351, Loss 0.2235
2018-05-24 04:22:47,439 Iter 1262, Epoch 276.7544, Loss 0.2152
2018-05-24 04:22:53,462 Iter 1263, Epoch 276.9737, Loss 0.2244
2018-05-24 04:22:59,150 Iter 1264, Epoch 277.1930, Loss 0.2115
2018-05-24 04:23:04,857 Iter 1265, Epoch 277.4123, Loss 0.2063
2018-05-24 04:23:10,455 Iter 1266, Epoch 277.6316, Loss 0.2334
2018-05-24 04:23:15,715 Iter 1267, Epoch 277.8509, Loss 0.2419
2018-05-24 04:23:21,217 Iter 1268, Epoch 278.0702, Loss 0.2144
2018-05-24 04:23:26,831 Iter 1269, Epoch 278.2895, Loss 0.2253
2018-05-24 04:23:32,835 Iter 1270, Epoch 278.5088, Loss 0.2163
2018-05-24 04:23:38,498 Iter 1271, Epoch 278.7281, Loss 0.2226
2018-05-24 04:23:43,759 Iter 1272, Epoch 278.9474, Loss 0.2319
2018-05-24 04:23:49,487 Iter 1273, Epoch 279.1667, Loss 0.2132
2018-05-24 04:23:55,487 Iter 1274, Epoch 279.3860, Loss 0.2341
2018-05-24 04:24:00,747 Iter 1275, Epoch 279.6053, Loss 0.2122
2018-05-24 04:24:06,740 Iter 1276, Epoch 279.8246, Loss 0.2107
2018-05-24 04:24:11,998 Iter 1277, Epoch 280.0439, Loss 0.2354
2018-05-24 04:24:17,966 Iter 1278, Epoch 280.2632, Loss 0.2078
2018-05-24 04:24:23,223 Iter 1279, Epoch 280.4825, Loss 0.2319
2018-05-24 04:24:28,786 Iter 1280, Epoch 280.7018, Loss 0.2158
2018-05-24 04:24:34,643 Iter 1281, Epoch 280.9211, Loss 0.2163
2018-05-24 04:24:40,242 Iter 1282, Epoch 281.1404, Loss 0.2068
2018-05-24 04:24:45,884 Iter 1283, Epoch 281.3596, Loss 0.2184
2018-05-24 04:24:51,143 Iter 1284, Epoch 281.5789, Loss 0.2145
2018-05-24 04:24:56,917 Iter 1285, Epoch 281.7982, Loss 0.2242
2018-05-24 04:25:02,941 Iter 1286, Epoch 282.0175, Loss 0.2075
2018-05-24 04:25:08,199 Iter 1287, Epoch 282.2368, Loss 0.1919
2018-05-24 04:25:13,780 Iter 1288, Epoch 282.4561, Loss 0.2143
2018-05-24 04:25:19,044 Iter 1289, Epoch 282.6754, Loss 0.2395
2018-05-24 04:25:25,104 Iter 1290, Epoch 282.8947, Loss 0.2076
2018-05-24 04:25:30,807 Iter 1291, Epoch 283.1140, Loss 0.2062
2018-05-24 04:25:36,405 Iter 1292, Epoch 283.3333, Loss 0.2140
2018-05-24 04:25:41,858 Iter 1293, Epoch 283.5526, Loss 0.1994
2018-05-24 04:25:47,565 Iter 1294, Epoch 283.7719, Loss 0.2358
2018-05-24 04:25:53,062 Iter 1295, Epoch 283.9912, Loss 0.2111
2018-05-24 04:25:58,806 Iter 1296, Epoch 284.2105, Loss 0.1829
2018-05-24 04:26:04,062 Iter 1297, Epoch 284.4298, Loss 0.2425
2018-05-24 04:26:10,263 Iter 1298, Epoch 284.6491, Loss 0.2509
2018-05-24 04:26:15,719 Iter 1299, Epoch 284.8684, Loss 0.2223
2018-05-24 04:26:21,416 Iter 1300, Epoch 285.0877, Loss 0.2025
2018-05-24 04:26:26,675 Iter 1301, Epoch 285.3070, Loss 0.2322
2018-05-24 04:26:32,126 Iter 1302, Epoch 285.5263, Loss 0.2102
2018-05-24 04:26:37,762 Iter 1303, Epoch 285.7456, Loss 0.2352
2018-05-24 04:26:43,300 Iter 1304, Epoch 285.9649, Loss 0.2011
2018-05-24 04:26:49,087 Iter 1305, Epoch 286.1842, Loss 0.2347
2018-05-24 04:26:54,657 Iter 1306, Epoch 286.4035, Loss 0.2110
2018-05-24 04:26:59,917 Iter 1307, Epoch 286.6228, Loss 0.2100
2018-05-24 04:27:05,404 Iter 1308, Epoch 286.8421, Loss 0.2447
2018-05-24 04:27:11,483 Iter 1309, Epoch 287.0614, Loss 0.1933
2018-05-24 04:27:16,744 Iter 1310, Epoch 287.2807, Loss 0.2201
2018-05-24 04:27:22,852 Iter 1311, Epoch 287.5000, Loss 0.2065
2018-05-24 04:27:28,244 Iter 1312, Epoch 287.7193, Loss 0.2457
2018-05-24 04:27:33,734 Iter 1313, Epoch 287.9386, Loss 0.2209
2018-05-24 04:27:39,371 Iter 1314, Epoch 288.1579, Loss 0.2263
2018-05-24 04:27:45,020 Iter 1315, Epoch 288.3772, Loss 0.2264
2018-05-24 04:27:50,701 Iter 1316, Epoch 288.5965, Loss 0.2077
2018-05-24 04:27:56,170 Iter 1317, Epoch 288.8158, Loss 0.2434
2018-05-24 04:28:01,765 Iter 1318, Epoch 289.0351, Loss 0.2093
2018-05-24 04:28:07,022 Iter 1319, Epoch 289.2544, Loss 0.2046
2018-05-24 04:28:12,681 Iter 1320, Epoch 289.4737, Loss 0.2169
2018-05-24 04:28:18,855 Iter 1321, Epoch 289.6930, Loss 0.2200
2018-05-24 04:28:24,314 Iter 1322, Epoch 289.9123, Loss 0.2320
2018-05-24 04:28:30,095 Iter 1323, Epoch 290.1316, Loss 0.2149
2018-05-24 04:28:35,768 Iter 1324, Epoch 290.3509, Loss 0.2264
2018-05-24 04:28:41,030 Iter 1325, Epoch 290.5702, Loss 0.2134
2018-05-24 04:28:46,666 Iter 1326, Epoch 290.7895, Loss 0.2250
2018-05-24 04:28:52,448 Iter 1327, Epoch 291.0088, Loss 0.2217
2018-05-24 04:28:58,107 Iter 1328, Epoch 291.2281, Loss 0.2144
2018-05-24 04:29:03,528 Iter 1329, Epoch 291.4474, Loss 0.2236
2018-05-24 04:29:09,077 Iter 1330, Epoch 291.6667, Loss 0.2423
2018-05-24 04:29:14,650 Iter 1331, Epoch 291.8860, Loss 0.2222
2018-05-24 04:29:20,667 Iter 1332, Epoch 292.1053, Loss 0.2182
2018-05-24 04:29:25,926 Iter 1333, Epoch 292.3246, Loss 0.1921
2018-05-24 04:29:31,570 Iter 1334, Epoch 292.5439, Loss 0.2259
2018-05-24 04:29:37,274 Iter 1335, Epoch 292.7632, Loss 0.2122
2018-05-24 04:29:42,778 Iter 1336, Epoch 292.9825, Loss 0.2168
2018-05-24 04:29:48,034 Iter 1337, Epoch 293.2018, Loss 0.2059
2018-05-24 04:29:53,294 Iter 1338, Epoch 293.4211, Loss 0.2033
2018-05-24 04:29:59,024 Iter 1339, Epoch 293.6404, Loss 0.2273
2018-05-24 04:30:05,345 Iter 1340, Epoch 293.8596, Loss 0.2289
2018-05-24 04:30:10,601 Iter 1341, Epoch 294.0789, Loss 0.2348
2018-05-24 04:30:16,761 Iter 1342, Epoch 294.2982, Loss 0.2057
2018-05-24 04:30:22,018 Iter 1343, Epoch 294.5175, Loss 0.2266
2018-05-24 04:30:27,274 Iter 1344, Epoch 294.7368, Loss 0.2140
2018-05-24 04:30:32,856 Iter 1345, Epoch 294.9561, Loss 0.2341
2018-05-24 04:30:38,897 Iter 1346, Epoch 295.1754, Loss 0.2172
2018-05-24 04:30:44,526 Iter 1347, Epoch 295.3947, Loss 0.2064
2018-05-24 04:30:50,021 Iter 1348, Epoch 295.6140, Loss 0.2488
2018-05-24 04:30:55,698 Iter 1349, Epoch 295.8333, Loss 0.1970
2018-05-24 04:31:00,957 Iter 1350, Epoch 296.0526, Loss 0.2300
2018-05-24 04:31:06,891 Iter 1351, Epoch 296.2719, Loss 0.2296
2018-05-24 04:31:12,499 Iter 1352, Epoch 296.4912, Loss 0.2139
2018-05-24 04:31:18,179 Iter 1353, Epoch 296.7105, Loss 0.2126
2018-05-24 04:31:23,920 Iter 1354, Epoch 296.9298, Loss 0.2075
2018-05-24 04:31:29,470 Iter 1355, Epoch 297.1491, Loss 0.1999
2018-05-24 04:31:35,059 Iter 1356, Epoch 297.3684, Loss 0.2382
2018-05-24 04:31:40,780 Iter 1357, Epoch 297.5877, Loss 0.2154
2018-05-24 04:31:46,032 Iter 1358, Epoch 297.8070, Loss 0.2205
2018-05-24 04:31:52,202 Iter 1359, Epoch 298.0263, Loss 0.2257
2018-05-24 04:31:57,728 Iter 1360, Epoch 298.2456, Loss 0.2131
2018-05-24 04:32:03,263 Iter 1361, Epoch 298.4649, Loss 0.2278
2018-05-24 04:32:08,955 Iter 1362, Epoch 298.6842, Loss 0.2276
2018-05-24 04:32:14,403 Iter 1363, Epoch 298.9035, Loss 0.2150
2018-05-24 04:32:19,662 Iter 1364, Epoch 299.1228, Loss 0.2391
2018-05-24 04:32:25,642 Iter 1365, Epoch 299.3421, Loss 0.2235
2018-05-24 04:32:31,274 Iter 1366, Epoch 299.5614, Loss 0.2254
2018-05-24 04:32:36,901 Iter 1367, Epoch 299.7807, Loss 0.2596
2018-05-24 04:32:42,665 Iter 1368, Epoch 300.0000, Loss 0.2098
2018-05-24 04:32:48,290 Iter 1369, Epoch 300.2193, Loss 0.2165
2018-05-24 04:32:53,901 Iter 1370, Epoch 300.4386, Loss 0.2245
2018-05-24 04:32:59,453 Iter 1371, Epoch 300.6579, Loss 0.2110
2018-05-24 04:33:04,935 Iter 1372, Epoch 300.8772, Loss 0.2247
2018-05-24 04:33:10,440 Iter 1373, Epoch 301.0965, Loss 0.2052
2018-05-24 04:33:15,703 Iter 1374, Epoch 301.3158, Loss 0.2160
2018-05-24 04:33:21,431 Iter 1375, Epoch 301.5351, Loss 0.2140
2018-05-24 04:33:27,416 Iter 1376, Epoch 301.7544, Loss 0.2254
2018-05-24 04:33:33,079 Iter 1377, Epoch 301.9737, Loss 0.1987
2018-05-24 04:33:38,753 Iter 1378, Epoch 302.1930, Loss 0.1975
2018-05-24 04:33:44,291 Iter 1379, Epoch 302.4123, Loss 0.2128
2018-05-24 04:33:49,983 Iter 1380, Epoch 302.6316, Loss 0.2060
2018-05-24 04:33:55,612 Iter 1381, Epoch 302.8509, Loss 0.2218
2018-05-24 04:34:01,370 Iter 1382, Epoch 303.0702, Loss 0.2028
2018-05-24 04:34:07,005 Iter 1383, Epoch 303.2895, Loss 0.2070
2018-05-24 04:34:12,262 Iter 1384, Epoch 303.5088, Loss 0.1931
2018-05-24 04:34:17,813 Iter 1385, Epoch 303.7281, Loss 0.1916
2018-05-24 04:34:23,071 Iter 1386, Epoch 303.9474, Loss 0.2098
2018-05-24 04:34:28,642 Iter 1387, Epoch 304.1667, Loss 0.2083
2018-05-24 04:34:34,229 Iter 1388, Epoch 304.3860, Loss 0.1986
2018-05-24 04:34:40,414 Iter 1389, Epoch 304.6053, Loss 0.2118
2018-05-24 04:34:45,668 Iter 1390, Epoch 304.8246, Loss 0.1958
2018-05-24 04:34:50,924 Iter 1391, Epoch 305.0439, Loss 0.2043
2018-05-24 04:34:56,475 Iter 1392, Epoch 305.2632, Loss 0.2102
2018-05-24 04:35:02,131 Iter 1393, Epoch 305.4825, Loss 0.1889
2018-05-24 04:35:08,269 Iter 1394, Epoch 305.7018, Loss 0.2144
2018-05-24 04:35:13,994 Iter 1395, Epoch 305.9211, Loss 0.1975
2018-05-24 04:35:19,250 Iter 1396, Epoch 306.1404, Loss 0.1912
2018-05-24 04:35:24,509 Iter 1397, Epoch 306.3596, Loss 0.1898
2018-05-24 04:35:30,074 Iter 1398, Epoch 306.5789, Loss 0.1986
2018-05-24 04:35:35,640 Iter 1399, Epoch 306.7982, Loss 0.1865
2018-05-24 04:35:41,945 Iter 1400, Epoch 307.0175, Loss 0.2196
2018-05-24 04:35:47,196 Iter 1401, Epoch 307.2368, Loss 0.2213
2018-05-24 04:35:52,692 Iter 1402, Epoch 307.4561, Loss 0.1825
2018-05-24 04:35:58,163 Iter 1403, Epoch 307.6754, Loss 0.1960
2018-05-24 04:36:03,794 Iter 1404, Epoch 307.8947, Loss 0.1894
2018-05-24 04:36:09,537 Iter 1405, Epoch 308.1140, Loss 0.1962
2018-05-24 04:36:15,771 Iter 1406, Epoch 308.3333, Loss 0.1968
2018-05-24 04:36:21,029 Iter 1407, Epoch 308.5526, Loss 0.1957
2018-05-24 04:36:26,528 Iter 1408, Epoch 308.7719, Loss 0.2127
2018-05-24 04:36:32,014 Iter 1409, Epoch 308.9912, Loss 0.1821
2018-05-24 04:36:37,405 Iter 1410, Epoch 309.2105, Loss 0.1788
2018-05-24 04:36:42,914 Iter 1411, Epoch 309.4298, Loss 0.2080
2018-05-24 04:36:48,177 Iter 1412, Epoch 309.6491, Loss 0.2026
2018-05-24 04:36:54,257 Iter 1413, Epoch 309.8684, Loss 0.1817
2018-05-24 04:36:59,511 Iter 1414, Epoch 310.0877, Loss 0.1982
2018-05-24 04:37:05,168 Iter 1415, Epoch 310.3070, Loss 0.1983
2018-05-24 04:37:10,421 Iter 1416, Epoch 310.5263, Loss 0.2075
2018-05-24 04:37:16,608 Iter 1417, Epoch 310.7456, Loss 0.1981
2018-05-24 04:37:21,866 Iter 1418, Epoch 310.9649, Loss 0.1933
2018-05-24 04:37:27,903 Iter 1419, Epoch 311.1842, Loss 0.1758
2018-05-24 04:37:33,454 Iter 1420, Epoch 311.4035, Loss 0.2029
2018-05-24 04:37:38,902 Iter 1421, Epoch 311.6228, Loss 0.1862
2018-05-24 04:37:44,541 Iter 1422, Epoch 311.8421, Loss 0.1967
2018-05-24 04:37:49,799 Iter 1423, Epoch 312.0614, Loss 0.2060
2018-05-24 04:37:55,696 Iter 1424, Epoch 312.2807, Loss 0.1799
2018-05-24 04:38:01,382 Iter 1425, Epoch 312.5000, Loss 0.1948
2018-05-24 04:38:06,879 Iter 1426, Epoch 312.7193, Loss 0.2280
2018-05-24 04:38:12,866 Iter 1427, Epoch 312.9386, Loss 0.2069
2018-05-24 04:38:18,117 Iter 1428, Epoch 313.1579, Loss 0.2042
2018-05-24 04:38:23,765 Iter 1429, Epoch 313.3772, Loss 0.1991
2018-05-24 04:38:29,024 Iter 1430, Epoch 313.5965, Loss 0.1905
2018-05-24 04:38:35,353 Iter 1431, Epoch 313.8158, Loss 0.1970
2018-05-24 04:38:40,605 Iter 1432, Epoch 314.0351, Loss 0.2015
2018-05-24 04:38:45,861 Iter 1433, Epoch 314.2544, Loss 0.2034
2018-05-24 04:38:51,590 Iter 1434, Epoch 314.4737, Loss 0.1910
2018-05-24 04:38:57,242 Iter 1435, Epoch 314.6930, Loss 0.2023
2018-05-24 04:39:02,494 Iter 1436, Epoch 314.9123, Loss 0.2132
2018-05-24 04:39:08,578 Iter 1437, Epoch 315.1316, Loss 0.1757
2018-05-24 04:39:14,147 Iter 1438, Epoch 315.3509, Loss 0.1958
2018-05-24 04:39:19,707 Iter 1439, Epoch 315.5702, Loss 0.1968
2018-05-24 04:39:25,334 Iter 1440, Epoch 315.7895, Loss 0.1899
2018-05-24 04:39:30,953 Iter 1441, Epoch 316.0088, Loss 0.2117
2018-05-24 04:39:36,212 Iter 1442, Epoch 316.2281, Loss 0.1934
2018-05-24 04:39:42,346 Iter 1443, Epoch 316.4474, Loss 0.2042
2018-05-24 04:39:47,923 Iter 1444, Epoch 316.6667, Loss 0.2005
2018-05-24 04:39:53,179 Iter 1445, Epoch 316.8860, Loss 0.2127
2018-05-24 04:39:58,437 Iter 1446, Epoch 317.1053, Loss 0.1866
2018-05-24 04:40:04,500 Iter 1447, Epoch 317.3246, Loss 0.1908
2018-05-24 04:40:10,299 Iter 1448, Epoch 317.5439, Loss 0.1953
2018-05-24 04:40:15,874 Iter 1449, Epoch 317.7632, Loss 0.1902
2018-05-24 04:40:21,414 Iter 1450, Epoch 317.9825, Loss 0.2119
2018-05-24 04:40:27,053 Iter 1451, Epoch 318.2018, Loss 0.1940
2018-05-24 04:40:32,310 Iter 1452, Epoch 318.4211, Loss 0.2070
2018-05-24 04:40:37,904 Iter 1453, Epoch 318.6404, Loss 0.2156
2018-05-24 04:40:44,048 Iter 1454, Epoch 318.8596, Loss 0.1937
2018-05-24 04:40:49,693 Iter 1455, Epoch 319.0789, Loss 0.1795
2018-05-24 04:40:55,482 Iter 1456, Epoch 319.2982, Loss 0.2023
2018-05-24 04:41:00,739 Iter 1457, Epoch 319.5175, Loss 0.1988
2018-05-24 04:41:06,308 Iter 1458, Epoch 319.7368, Loss 0.1789
2018-05-24 04:41:12,514 Iter 1459, Epoch 319.9561, Loss 0.2007
2018-05-24 04:41:18,064 Iter 1460, Epoch 320.1754, Loss 0.1955
2018-05-24 04:41:23,615 Iter 1461, Epoch 320.3947, Loss 0.1808
2018-05-24 04:41:29,221 Iter 1462, Epoch 320.6140, Loss 0.2124
2018-05-24 04:41:34,479 Iter 1463, Epoch 320.8333, Loss 0.1912
2018-05-24 04:41:40,020 Iter 1464, Epoch 321.0526, Loss 0.2007
2018-05-24 04:41:46,052 Iter 1465, Epoch 321.2719, Loss 0.1953
2018-05-24 04:41:51,864 Iter 1466, Epoch 321.4912, Loss 0.2009
2018-05-24 04:41:57,482 Iter 1467, Epoch 321.7105, Loss 0.1845
2018-05-24 04:42:02,741 Iter 1468, Epoch 321.9298, Loss 0.1816
2018-05-24 04:42:08,296 Iter 1469, Epoch 322.1491, Loss 0.1833
2018-05-24 04:42:14,030 Iter 1470, Epoch 322.3684, Loss 0.2034
2018-05-24 04:42:19,559 Iter 1471, Epoch 322.5877, Loss 0.2220
2018-05-24 04:42:25,158 Iter 1472, Epoch 322.8070, Loss 0.1951
2018-05-24 04:42:30,970 Iter 1473, Epoch 323.0263, Loss 0.1811
2018-05-24 04:42:36,579 Iter 1474, Epoch 323.2456, Loss 0.2076
2018-05-24 04:42:41,839 Iter 1475, Epoch 323.4649, Loss 0.1969
2018-05-24 04:42:47,099 Iter 1476, Epoch 323.6842, Loss 0.1859
2018-05-24 04:42:52,755 Iter 1477, Epoch 323.9035, Loss 0.2027
2018-05-24 04:42:58,676 Iter 1478, Epoch 324.1228, Loss 0.1870
2018-05-24 04:43:04,350 Iter 1479, Epoch 324.3421, Loss 0.1897
2018-05-24 04:43:09,806 Iter 1480, Epoch 324.5614, Loss 0.2010
2018-05-24 04:43:15,748 Iter 1481, Epoch 324.7807, Loss 0.1762
2018-05-24 04:43:21,001 Iter 1482, Epoch 325.0000, Loss 0.2094
2018-05-24 04:43:26,698 Iter 1483, Epoch 325.2193, Loss 0.2071
2018-05-24 04:43:32,207 Iter 1484, Epoch 325.4386, Loss 0.1881
2018-05-24 04:43:37,931 Iter 1485, Epoch 325.6579, Loss 0.2028
2018-05-24 04:43:43,494 Iter 1486, Epoch 325.8772, Loss 0.2003
2018-05-24 04:43:49,519 Iter 1487, Epoch 326.0965, Loss 0.1835
2018-05-24 04:43:54,771 Iter 1488, Epoch 326.3158, Loss 0.1716
2018-05-24 04:44:00,029 Iter 1489, Epoch 326.5351, Loss 0.1967
2018-05-24 04:44:05,952 Iter 1490, Epoch 326.7544, Loss 0.1833
2018-05-24 04:44:11,757 Iter 1491, Epoch 326.9737, Loss 0.2094
2018-05-24 04:44:17,017 Iter 1492, Epoch 327.1930, Loss 0.2033
2018-05-24 04:44:22,278 Iter 1493, Epoch 327.4123, Loss 0.1718
2018-05-24 04:44:27,930 Iter 1494, Epoch 327.6316, Loss 0.2066
2018-05-24 04:44:33,982 Iter 1495, Epoch 327.8509, Loss 0.1825
2018-05-24 04:44:39,536 Iter 1496, Epoch 328.0702, Loss 0.1982
2018-05-24 04:44:45,396 Iter 1497, Epoch 328.2895, Loss 0.1815
2018-05-24 04:44:50,653 Iter 1498, Epoch 328.5088, Loss 0.1852
2018-05-24 04:44:56,348 Iter 1499, Epoch 328.7281, Loss 0.2123
2018-05-24 04:45:01,949 Iter 1500, Epoch 328.9474, Loss 0.2242
2018-05-24 04:45:07,208 Iter 1501, Epoch 329.1667, Loss 0.1948
2018-05-24 04:45:13,391 Iter 1502, Epoch 329.3860, Loss 0.1898
2018-05-24 04:45:19,138 Iter 1503, Epoch 329.6053, Loss 0.1849
2018-05-24 04:45:24,395 Iter 1504, Epoch 329.8246, Loss 0.1890
2018-05-24 04:45:29,659 Iter 1505, Epoch 330.0439, Loss 0.1865
2018-05-24 04:45:35,189 Iter 1506, Epoch 330.2632, Loss 0.1915
2018-05-24 04:45:40,847 Iter 1507, Epoch 330.4825, Loss 0.2197
2018-05-24 04:45:46,451 Iter 1508, Epoch 330.7018, Loss 0.2050
2018-05-24 04:45:52,854 Iter 1509, Epoch 330.9211, Loss 0.1821
2018-05-24 04:45:58,116 Iter 1510, Epoch 331.1404, Loss 0.1881
2018-05-24 04:46:03,718 Iter 1511, Epoch 331.3596, Loss 0.1891
2018-05-24 04:46:09,471 Iter 1512, Epoch 331.5789, Loss 0.1970
2018-05-24 04:46:15,104 Iter 1513, Epoch 331.7982, Loss 0.1937
2018-05-24 04:46:20,668 Iter 1514, Epoch 332.0175, Loss 0.2017
2018-05-24 04:46:26,308 Iter 1515, Epoch 332.2368, Loss 0.1981
2018-05-24 04:46:31,779 Iter 1516, Epoch 332.4561, Loss 0.1932
2018-05-24 04:46:37,384 Iter 1517, Epoch 332.6754, Loss 0.1878
2018-05-24 04:46:42,642 Iter 1518, Epoch 332.8947, Loss 0.2043
2018-05-24 04:46:48,385 Iter 1519, Epoch 333.1140, Loss 0.1822
2018-05-24 04:46:54,343 Iter 1520, Epoch 333.3333, Loss 0.1799
2018-05-24 04:47:00,034 Iter 1521, Epoch 333.5526, Loss 0.1936
2018-05-24 04:47:05,614 Iter 1522, Epoch 333.7719, Loss 0.1925
2018-05-24 04:47:11,164 Iter 1523, Epoch 333.9912, Loss 0.1905
2018-05-24 04:47:16,729 Iter 1524, Epoch 334.2105, Loss 0.2000
2018-05-24 04:47:21,988 Iter 1525, Epoch 334.4298, Loss 0.1962
2018-05-24 04:47:28,017 Iter 1526, Epoch 334.6491, Loss 0.1795
2018-05-24 04:47:33,560 Iter 1527, Epoch 334.8684, Loss 0.1756
2018-05-24 04:47:39,142 Iter 1528, Epoch 335.0877, Loss 0.2116
2018-05-24 04:47:44,401 Iter 1529, Epoch 335.3070, Loss 0.1907
2018-05-24 04:47:49,899 Iter 1530, Epoch 335.5263, Loss 0.1753
2018-05-24 04:47:55,543 Iter 1531, Epoch 335.7456, Loss 0.2114
2018-05-24 04:48:01,647 Iter 1532, Epoch 335.9649, Loss 0.1844
2018-05-24 04:48:07,368 Iter 1533, Epoch 336.1842, Loss 0.1889
2018-05-24 04:48:12,811 Iter 1534, Epoch 336.4035, Loss 0.1863
2018-05-24 04:48:18,421 Iter 1535, Epoch 336.6228, Loss 0.2086
2018-05-24 04:48:24,114 Iter 1536, Epoch 336.8421, Loss 0.1994
2018-05-24 04:48:29,884 Iter 1537, Epoch 337.0614, Loss 0.1758
2018-05-24 04:48:35,141 Iter 1538, Epoch 337.2807, Loss 0.2037
2018-05-24 04:48:41,215 Iter 1539, Epoch 337.5000, Loss 0.1840
2018-05-24 04:48:46,845 Iter 1540, Epoch 337.7193, Loss 0.2077
2018-05-24 04:48:52,578 Iter 1541, Epoch 337.9386, Loss 0.1818
2018-05-24 04:48:58,129 Iter 1542, Epoch 338.1579, Loss 0.1890
2018-05-24 04:49:03,690 Iter 1543, Epoch 338.3772, Loss 0.1903
2018-05-24 04:49:08,945 Iter 1544, Epoch 338.5965, Loss 0.2035
2018-05-24 04:49:15,002 Iter 1545, Epoch 338.8158, Loss 0.2053
2018-05-24 04:49:20,537 Iter 1546, Epoch 339.0351, Loss 0.1816
2018-05-24 04:49:26,224 Iter 1547, Epoch 339.2544, Loss 0.2117
2018-05-24 04:49:31,478 Iter 1548, Epoch 339.4737, Loss 0.2063
2018-05-24 04:49:37,429 Iter 1549, Epoch 339.6930, Loss 0.1874
2018-05-24 04:49:43,212 Iter 1550, Epoch 339.9123, Loss 0.1814
2018-05-24 04:49:48,880 Iter 1551, Epoch 340.1316, Loss 0.1814
2018-05-24 04:49:54,137 Iter 1552, Epoch 340.3509, Loss 0.1822
2018-05-24 04:49:59,398 Iter 1553, Epoch 340.5702, Loss 0.2026
2018-05-24 04:50:05,298 Iter 1554, Epoch 340.7895, Loss 0.1819
2018-05-24 04:50:10,844 Iter 1555, Epoch 341.0088, Loss 0.1918
2018-05-24 04:50:16,103 Iter 1556, Epoch 341.2281, Loss 0.1986
2018-05-24 04:50:22,246 Iter 1557, Epoch 341.4474, Loss 0.1766
2018-05-24 04:50:27,504 Iter 1558, Epoch 341.6667, Loss 0.1862
2018-05-24 04:50:33,505 Iter 1559, Epoch 341.8860, Loss 0.1998
2018-05-24 04:50:38,753 Iter 1560, Epoch 342.1053, Loss 0.1813
2018-05-24 04:50:44,387 Iter 1561, Epoch 342.3246, Loss 0.1965
2018-05-24 04:50:49,641 Iter 1562, Epoch 342.5439, Loss 0.1977
2018-05-24 04:50:55,683 Iter 1563, Epoch 342.7632, Loss 0.1631
2018-05-24 04:51:01,283 Iter 1564, Epoch 342.9825, Loss 0.2077
2018-05-24 04:51:06,903 Iter 1565, Epoch 343.2018, Loss 0.1980
2018-05-24 04:51:12,631 Iter 1566, Epoch 343.4211, Loss 0.1902
2018-05-24 04:51:18,231 Iter 1567, Epoch 343.6404, Loss 0.1938
2018-05-24 04:51:23,491 Iter 1568, Epoch 343.8596, Loss 0.1735
2018-05-24 04:51:29,500 Iter 1569, Epoch 344.0789, Loss 0.2079
2018-05-24 04:51:35,061 Iter 1570, Epoch 344.2982, Loss 0.1995
2018-05-24 04:51:40,321 Iter 1571, Epoch 344.5175, Loss 0.2008
2018-05-24 04:51:45,985 Iter 1572, Epoch 344.7368, Loss 0.1989
2018-05-24 04:51:52,070 Iter 1573, Epoch 344.9561, Loss 0.1842
2018-05-24 04:51:57,650 Iter 1574, Epoch 345.1754, Loss 0.1932
2018-05-24 04:52:02,908 Iter 1575, Epoch 345.3947, Loss 0.1849
2018-05-24 04:52:08,508 Iter 1576, Epoch 345.6140, Loss 0.1956
2018-05-24 04:52:13,944 Iter 1577, Epoch 345.8333, Loss 0.1946
2018-05-24 04:52:19,699 Iter 1578, Epoch 346.0526, Loss 0.1959
2018-05-24 04:52:25,172 Iter 1579, Epoch 346.2719, Loss 0.1823
2018-05-24 04:52:30,752 Iter 1580, Epoch 346.4912, Loss 0.1925
2018-05-24 04:52:36,360 Iter 1581, Epoch 346.7105, Loss 0.2072
2018-05-24 04:52:41,831 Iter 1582, Epoch 346.9298, Loss 0.2033
2018-05-24 04:52:47,750 Iter 1583, Epoch 347.1491, Loss 0.1939
2018-05-24 04:52:53,011 Iter 1584, Epoch 347.3684, Loss 0.2068
2018-05-24 04:52:58,697 Iter 1585, Epoch 347.5877, Loss 0.2075
2018-05-24 04:53:04,312 Iter 1586, Epoch 347.8070, Loss 0.1947
2018-05-24 04:53:09,872 Iter 1587, Epoch 348.0263, Loss 0.1979
2018-05-24 04:53:15,456 Iter 1588, Epoch 348.2456, Loss 0.1972
2018-05-24 04:53:21,174 Iter 1589, Epoch 348.4649, Loss 0.1841
2018-05-24 04:53:26,713 Iter 1590, Epoch 348.6842, Loss 0.1846
2018-05-24 04:53:32,360 Iter 1591, Epoch 348.9035, Loss 0.1950
2018-05-24 04:53:37,615 Iter 1592, Epoch 349.1228, Loss 0.1934
2018-05-24 04:53:43,692 Iter 1593, Epoch 349.3421, Loss 0.1635
2018-05-24 04:53:49,341 Iter 1594, Epoch 349.5614, Loss 0.2023
2018-05-24 04:53:55,011 Iter 1595, Epoch 349.7807, Loss 0.1901
2018-05-24 04:54:00,264 Iter 1596, Epoch 350.0000, Loss 0.2067
2018-05-24 04:54:06,237 Iter 1597, Epoch 350.2193, Loss 0.1755
2018-05-24 04:54:11,494 Iter 1598, Epoch 350.4386, Loss 0.2120
2018-05-24 04:54:17,535 Iter 1599, Epoch 350.6579, Loss 0.1905
2018-05-24 04:54:23,047 Iter 1600, Epoch 350.8772, Loss 0.1878
2018-05-24 04:54:28,625 Iter 1601, Epoch 351.0965, Loss 0.2068
2018-05-24 04:54:34,406 Iter 1602, Epoch 351.3158, Loss 0.1898
2018-05-24 04:54:39,667 Iter 1603, Epoch 351.5351, Loss 0.2050
2018-05-24 04:54:45,635 Iter 1604, Epoch 351.7544, Loss 0.1841
2018-05-24 04:54:51,060 Iter 1605, Epoch 351.9737, Loss 0.1974
2018-05-24 04:54:56,681 Iter 1606, Epoch 352.1930, Loss 0.1947
2018-05-24 04:55:02,256 Iter 1607, Epoch 352.4123, Loss 0.1987
2018-05-24 04:55:07,843 Iter 1608, Epoch 352.6316, Loss 0.1888
2018-05-24 04:55:13,102 Iter 1609, Epoch 352.8509, Loss 0.1801
2018-05-24 04:55:18,722 Iter 1610, Epoch 353.0702, Loss 0.1976
2018-05-24 04:55:24,689 Iter 1611, Epoch 353.2895, Loss 0.1849
2018-05-24 04:55:30,356 Iter 1612, Epoch 353.5088, Loss 0.2049
2018-05-24 04:55:35,938 Iter 1613, Epoch 353.7281, Loss 0.1908
2018-05-24 04:55:41,605 Iter 1614, Epoch 353.9474, Loss 0.1885
2018-05-24 04:55:47,276 Iter 1615, Epoch 354.1667, Loss 0.1985
2018-05-24 04:55:52,535 Iter 1616, Epoch 354.3860, Loss 0.2132
2018-05-24 04:55:58,542 Iter 1617, Epoch 354.6053, Loss 0.1821
2018-05-24 04:56:04,100 Iter 1618, Epoch 354.8246, Loss 0.2140
2018-05-24 04:56:09,362 Iter 1619, Epoch 355.0439, Loss 0.1633
2018-05-24 04:56:15,276 Iter 1620, Epoch 355.2632, Loss 0.1915
2018-05-24 04:56:20,858 Iter 1621, Epoch 355.4825, Loss 0.2132
2018-05-24 04:56:26,116 Iter 1622, Epoch 355.7018, Loss 0.1846
2018-05-24 04:56:32,027 Iter 1623, Epoch 355.9211, Loss 0.1810
2018-05-24 04:56:37,638 Iter 1624, Epoch 356.1404, Loss 0.1729
2018-05-24 04:56:42,898 Iter 1625, Epoch 356.3596, Loss 0.1894
2018-05-24 04:56:48,518 Iter 1626, Epoch 356.5789, Loss 0.1890
2018-05-24 04:56:54,542 Iter 1627, Epoch 356.7982, Loss 0.1920
2018-05-24 04:57:00,479 Iter 1628, Epoch 357.0175, Loss 0.1829
2018-05-24 04:57:05,734 Iter 1629, Epoch 357.2368, Loss 0.1851
2018-05-24 04:57:11,726 Iter 1630, Epoch 357.4561, Loss 0.1941
2018-05-24 04:57:16,993 Iter 1631, Epoch 357.6754, Loss 0.1780
2018-05-24 04:57:22,579 Iter 1632, Epoch 357.8947, Loss 0.1787
2018-05-24 04:57:27,839 Iter 1633, Epoch 358.1140, Loss 0.2062
2018-05-24 04:57:33,413 Iter 1634, Epoch 358.3333, Loss 0.2016
2018-05-24 04:57:39,572 Iter 1635, Epoch 358.5526, Loss 0.1927
2018-05-24 04:57:45,275 Iter 1636, Epoch 358.7719, Loss 0.1929
2018-05-24 04:57:50,536 Iter 1637, Epoch 358.9912, Loss 0.1736
2018-05-24 04:57:55,795 Iter 1638, Epoch 359.2105, Loss 0.1786
2018-05-24 04:58:01,823 Iter 1639, Epoch 359.4298, Loss 0.1872
2018-05-24 04:58:07,082 Iter 1640, Epoch 359.6491, Loss 0.1912
2018-05-24 04:58:13,279 Iter 1641, Epoch 359.8684, Loss 0.2055
2018-05-24 04:58:18,955 Iter 1642, Epoch 360.0877, Loss 0.1742
2018-05-24 04:58:24,218 Iter 1643, Epoch 360.3070, Loss 0.1978
2018-05-24 04:58:29,841 Iter 1644, Epoch 360.5263, Loss 0.2010
2018-05-24 04:58:35,808 Iter 1645, Epoch 360.7456, Loss 0.1863
2018-05-24 04:58:41,529 Iter 1646, Epoch 360.9649, Loss 0.1688
2018-05-24 04:58:47,206 Iter 1647, Epoch 361.1842, Loss 0.2072
2018-05-24 04:58:52,753 Iter 1648, Epoch 361.4035, Loss 0.1741
2018-05-24 04:58:58,441 Iter 1649, Epoch 361.6228, Loss 0.1888
2018-05-24 04:59:03,698 Iter 1650, Epoch 361.8421, Loss 0.1973
2018-05-24 04:59:09,599 Iter 1651, Epoch 362.0614, Loss 0.1886
2018-05-24 04:59:15,187 Iter 1652, Epoch 362.2807, Loss 0.1879
2018-05-24 04:59:20,947 Iter 1653, Epoch 362.5000, Loss 0.1705
2018-05-24 04:59:26,400 Iter 1654, Epoch 362.7193, Loss 0.1826
2018-05-24 04:59:31,962 Iter 1655, Epoch 362.9386, Loss 0.1914
2018-05-24 04:59:37,655 Iter 1656, Epoch 363.1579, Loss 0.1826
2018-05-24 04:59:43,332 Iter 1657, Epoch 363.3772, Loss 0.1904
2018-05-24 04:59:49,028 Iter 1658, Epoch 363.5965, Loss 0.1734
2018-05-24 04:59:54,704 Iter 1659, Epoch 363.8158, Loss 0.1832
2018-05-24 05:00:00,416 Iter 1660, Epoch 364.0351, Loss 0.1807
2018-05-24 05:00:06,019 Iter 1661, Epoch 364.2544, Loss 0.1933
2018-05-24 05:00:11,803 Iter 1662, Epoch 364.4737, Loss 0.1870
2018-05-24 05:00:17,166 Iter 1663, Epoch 364.6930, Loss 0.1791
2018-05-24 05:00:23,008 Iter 1664, Epoch 364.9123, Loss 0.2000
2018-05-24 05:00:28,636 Iter 1665, Epoch 365.1316, Loss 0.2010
2018-05-24 05:00:34,126 Iter 1666, Epoch 365.3509, Loss 0.1780
2018-05-24 05:00:39,645 Iter 1667, Epoch 365.5702, Loss 0.1751
2018-05-24 05:00:44,904 Iter 1668, Epoch 365.7895, Loss 0.1900
2018-05-24 05:00:50,641 Iter 1669, Epoch 366.0088, Loss 0.1866
2018-05-24 05:00:56,351 Iter 1670, Epoch 366.2281, Loss 0.1722
2018-05-24 05:01:02,355 Iter 1671, Epoch 366.4474, Loss 0.1867
2018-05-24 05:01:07,612 Iter 1672, Epoch 366.6667, Loss 0.2065
2018-05-24 05:01:12,871 Iter 1673, Epoch 366.8860, Loss 0.1992
2018-05-24 05:01:18,684 Iter 1674, Epoch 367.1053, Loss 0.2024
2018-05-24 05:01:23,942 Iter 1675, Epoch 367.3246, Loss 0.1839
2018-05-24 05:01:29,723 Iter 1676, Epoch 367.5439, Loss 0.1836
2018-05-24 05:01:35,844 Iter 1677, Epoch 367.7632, Loss 0.1893
2018-05-24 05:01:41,098 Iter 1678, Epoch 367.9825, Loss 0.1707
2018-05-24 05:01:46,358 Iter 1679, Epoch 368.2018, Loss 0.1789
2018-05-24 05:01:52,029 Iter 1680, Epoch 368.4211, Loss 0.1770
2018-05-24 05:01:57,634 Iter 1681, Epoch 368.6404, Loss 0.2057
2018-05-24 05:02:03,314 Iter 1682, Epoch 368.8596, Loss 0.1815
2018-05-24 05:02:09,400 Iter 1683, Epoch 369.0789, Loss 0.1910
2018-05-24 05:02:14,934 Iter 1684, Epoch 369.2982, Loss 0.1879
2018-05-24 05:02:20,631 Iter 1685, Epoch 369.5175, Loss 0.2036
2018-05-24 05:02:25,886 Iter 1686, Epoch 369.7368, Loss 0.1928
2018-05-24 05:02:31,828 Iter 1687, Epoch 369.9561, Loss 0.1669
2018-05-24 05:02:37,437 Iter 1688, Epoch 370.1754, Loss 0.1992
2018-05-24 05:02:43,359 Iter 1689, Epoch 370.3947, Loss 0.1923
2018-05-24 05:02:48,611 Iter 1690, Epoch 370.6140, Loss 0.1913
2018-05-24 05:02:54,275 Iter 1691, Epoch 370.8333, Loss 0.1810
2018-05-24 05:03:00,070 Iter 1692, Epoch 371.0526, Loss 0.1770
2018-05-24 05:03:05,331 Iter 1693, Epoch 371.2719, Loss 0.1744
2018-05-24 05:03:10,965 Iter 1694, Epoch 371.4912, Loss 0.1913
2018-05-24 05:03:16,593 Iter 1695, Epoch 371.7105, Loss 0.1951
2018-05-24 05:03:22,092 Iter 1696, Epoch 371.9298, Loss 0.1985
2018-05-24 05:03:27,347 Iter 1697, Epoch 372.1491, Loss 0.2075
2018-05-24 05:03:33,283 Iter 1698, Epoch 372.3684, Loss 0.2029
2018-05-24 05:03:38,994 Iter 1699, Epoch 372.5877, Loss 0.1840
2018-05-24 05:03:44,514 Iter 1700, Epoch 372.8070, Loss 0.1943
2018-05-24 05:03:50,387 Iter 1701, Epoch 373.0263, Loss 0.1864
2018-05-24 05:03:55,645 Iter 1702, Epoch 373.2456, Loss 0.1735
2018-05-24 05:04:01,364 Iter 1703, Epoch 373.4649, Loss 0.1802
2018-05-24 05:04:06,794 Iter 1704, Epoch 373.6842, Loss 0.1855
2018-05-24 05:04:12,587 Iter 1705, Epoch 373.9035, Loss 0.1683
2018-05-24 05:04:17,845 Iter 1706, Epoch 374.1228, Loss 0.1836
2018-05-24 05:04:23,796 Iter 1707, Epoch 374.3421, Loss 0.1668
2018-05-24 05:04:29,464 Iter 1708, Epoch 374.5614, Loss 0.1970
2018-05-24 05:04:34,834 Iter 1709, Epoch 374.7807, Loss 0.1930
2018-05-24 05:04:40,093 Iter 1710, Epoch 375.0000, Loss 0.2044
2018-05-24 05:04:46,156 Iter 1711, Epoch 375.2193, Loss 0.1818
2018-05-24 05:04:51,418 Iter 1712, Epoch 375.4386, Loss 0.1965
2018-05-24 05:04:57,491 Iter 1713, Epoch 375.6579, Loss 0.1807
2018-05-24 05:05:03,110 Iter 1714, Epoch 375.8772, Loss 0.1805
2018-05-24 05:05:08,791 Iter 1715, Epoch 376.0965, Loss 0.1915
2018-05-24 05:05:14,416 Iter 1716, Epoch 376.3158, Loss 0.1952
2018-05-24 05:05:19,907 Iter 1717, Epoch 376.5351, Loss 0.1997
2018-05-24 05:05:25,634 Iter 1718, Epoch 376.7544, Loss 0.1739
2018-05-24 05:05:31,247 Iter 1719, Epoch 376.9737, Loss 0.1803
2018-05-24 05:05:36,826 Iter 1720, Epoch 377.1930, Loss 0.1773
2018-05-24 05:05:42,327 Iter 1721, Epoch 377.4123, Loss 0.1891
2018-05-24 05:05:48,540 Iter 1722, Epoch 377.6316, Loss 0.2035
2018-05-24 05:05:53,798 Iter 1723, Epoch 377.8509, Loss 0.1820
2018-05-24 05:05:59,060 Iter 1724, Epoch 378.0702, Loss 0.1871
2018-05-24 05:06:05,161 Iter 1725, Epoch 378.2895, Loss 0.1899
2018-05-24 05:06:10,418 Iter 1726, Epoch 378.5088, Loss 0.1979
2018-05-24 05:06:15,678 Iter 1727, Epoch 378.7281, Loss 0.1761
2018-05-24 05:06:21,641 Iter 1728, Epoch 378.9474, Loss 0.1724
2018-05-24 05:06:27,295 Iter 1729, Epoch 379.1667, Loss 0.1972
2018-05-24 05:06:32,553 Iter 1730, Epoch 379.3860, Loss 0.1830
2018-05-24 05:06:38,630 Iter 1731, Epoch 379.6053, Loss 0.1868
2018-05-24 05:06:44,240 Iter 1732, Epoch 379.8246, Loss 0.1831
2018-05-24 05:06:49,501 Iter 1733, Epoch 380.0439, Loss 0.1819
2018-05-24 05:06:55,097 Iter 1734, Epoch 380.2632, Loss 0.1783
2018-05-24 05:07:01,258 Iter 1735, Epoch 380.4825, Loss 0.1918
2018-05-24 05:07:06,521 Iter 1736, Epoch 380.7018, Loss 0.2061
2018-05-24 05:07:12,630 Iter 1737, Epoch 380.9211, Loss 0.1892
2018-05-24 05:07:18,166 Iter 1738, Epoch 381.1404, Loss 0.1806
2018-05-24 05:07:23,723 Iter 1739, Epoch 381.3596, Loss 0.1922
2018-05-24 05:07:29,141 Iter 1740, Epoch 381.5789, Loss 0.1712
2018-05-24 05:07:34,647 Iter 1741, Epoch 381.7982, Loss 0.1718
2018-05-24 05:07:40,828 Iter 1742, Epoch 382.0175, Loss 0.1997
2018-05-24 05:07:46,089 Iter 1743, Epoch 382.2368, Loss 0.1910
2018-05-24 05:07:51,631 Iter 1744, Epoch 382.4561, Loss 0.1866
2018-05-24 05:07:56,897 Iter 1745, Epoch 382.6754, Loss 0.1799
2018-05-24 05:08:02,460 Iter 1746, Epoch 382.8947, Loss 0.1840
2018-05-24 05:08:08,726 Iter 1747, Epoch 383.1140, Loss 0.1910
2018-05-24 05:08:14,326 Iter 1748, Epoch 383.3333, Loss 0.1820
2018-05-24 05:08:19,852 Iter 1749, Epoch 383.5526, Loss 0.1928
2018-05-24 05:08:25,373 Iter 1750, Epoch 383.7719, Loss 0.1970
2018-05-24 05:08:30,637 Iter 1751, Epoch 383.9912, Loss 0.1771
2018-05-24 05:08:37,130 Iter 1752, Epoch 384.2105, Loss 0.1836
2018-05-24 05:08:42,392 Iter 1753, Epoch 384.4298, Loss 0.1910
2018-05-24 05:08:47,656 Iter 1754, Epoch 384.6491, Loss 0.1696
2018-05-24 05:08:53,942 Iter 1755, Epoch 384.8684, Loss 0.1922
2018-05-24 05:08:59,388 Iter 1756, Epoch 385.0877, Loss 0.1848
2018-05-24 05:09:05,017 Iter 1757, Epoch 385.3070, Loss 0.1727
2018-05-24 05:09:10,993 Iter 1758, Epoch 385.5263, Loss 0.1883
2018-05-24 05:09:16,251 Iter 1759, Epoch 385.7456, Loss 0.1878
2018-05-24 05:09:22,031 Iter 1760, Epoch 385.9649, Loss 0.1983
2018-05-24 05:09:27,817 Iter 1761, Epoch 386.1842, Loss 0.1967
2018-05-24 05:09:33,071 Iter 1762, Epoch 386.4035, Loss 0.1954
2018-05-24 05:09:38,689 Iter 1763, Epoch 386.6228, Loss 0.1872
2018-05-24 05:09:43,945 Iter 1764, Epoch 386.8421, Loss 0.1850
2018-05-24 05:09:49,951 Iter 1765, Epoch 387.0614, Loss 0.1827
2018-05-24 05:09:55,486 Iter 1766, Epoch 387.2807, Loss 0.1791
2018-05-24 05:10:00,748 Iter 1767, Epoch 387.5000, Loss 0.1936
2018-05-24 05:10:06,612 Iter 1768, Epoch 387.7193, Loss 0.1769
2018-05-24 05:10:11,873 Iter 1769, Epoch 387.9386, Loss 0.1897
2018-05-24 05:10:17,400 Iter 1770, Epoch 388.1579, Loss 0.2000
2018-05-24 05:10:23,038 Iter 1771, Epoch 388.3772, Loss 0.1950
2018-05-24 05:10:28,733 Iter 1772, Epoch 388.5965, Loss 0.1761
2018-05-24 05:10:34,838 Iter 1773, Epoch 388.8158, Loss 0.2052
2018-05-24 05:10:40,454 Iter 1774, Epoch 389.0351, Loss 0.1967
2018-05-24 05:10:46,081 Iter 1775, Epoch 389.2544, Loss 0.1857
2018-05-24 05:10:51,579 Iter 1776, Epoch 389.4737, Loss 0.1668
2018-05-24 05:10:57,150 Iter 1777, Epoch 389.6930, Loss 0.1857
2018-05-24 05:11:02,409 Iter 1778, Epoch 389.9123, Loss 0.2027
2018-05-24 05:11:08,592 Iter 1779, Epoch 390.1316, Loss 0.1784
2018-05-24 05:11:14,316 Iter 1780, Epoch 390.3509, Loss 0.1827
2018-05-24 05:11:19,578 Iter 1781, Epoch 390.5702, Loss 0.1778
2018-05-24 05:11:25,472 Iter 1782, Epoch 390.7895, Loss 0.1971
2018-05-24 05:11:31,231 Iter 1783, Epoch 391.0088, Loss 0.1989
2018-05-24 05:11:36,892 Iter 1784, Epoch 391.2281, Loss 0.1888
2018-05-24 05:11:42,332 Iter 1785, Epoch 391.4474, Loss 0.1893
2018-05-24 05:11:47,947 Iter 1786, Epoch 391.6667, Loss 0.1840
2018-05-24 05:11:53,420 Iter 1787, Epoch 391.8860, Loss 0.2008
2018-05-24 05:11:58,898 Iter 1788, Epoch 392.1053, Loss 0.1878
2018-05-24 05:12:04,546 Iter 1789, Epoch 392.3246, Loss 0.1904
2018-05-24 05:12:09,808 Iter 1790, Epoch 392.5439, Loss 0.1964
2018-05-24 05:12:15,835 Iter 1791, Epoch 392.7632, Loss 0.1872
2018-05-24 05:12:21,366 Iter 1792, Epoch 392.9825, Loss 0.1781
2018-05-24 05:12:26,620 Iter 1793, Epoch 393.2018, Loss 0.1886
2018-05-24 05:12:33,026 Iter 1794, Epoch 393.4211, Loss 0.1907
2018-05-24 05:12:38,277 Iter 1795, Epoch 393.6404, Loss 0.2095
2018-05-24 05:12:43,539 Iter 1796, Epoch 393.8596, Loss 0.1865
2018-05-24 05:12:49,480 Iter 1797, Epoch 394.0789, Loss 0.2077
2018-05-24 05:12:55,143 Iter 1798, Epoch 394.2982, Loss 0.1975
2018-05-24 05:13:01,162 Iter 1799, Epoch 394.5175, Loss 0.1765
2018-05-24 05:13:06,418 Iter 1800, Epoch 394.7368, Loss 0.1828
2018-05-24 05:13:12,111 Iter 1801, Epoch 394.9561, Loss 0.1853
2018-05-24 05:13:17,735 Iter 1802, Epoch 395.1754, Loss 0.1785
2018-05-24 05:13:23,375 Iter 1803, Epoch 395.3947, Loss 0.1876
2018-05-24 05:13:28,849 Iter 1804, Epoch 395.6140, Loss 0.1909
2018-05-24 05:13:34,344 Iter 1805, Epoch 395.8333, Loss 0.2072
2018-05-24 05:13:39,794 Iter 1806, Epoch 396.0526, Loss 0.1860
2018-05-24 05:13:45,054 Iter 1807, Epoch 396.2719, Loss 0.1745
2018-05-24 05:13:51,229 Iter 1808, Epoch 396.4912, Loss 0.1776
2018-05-24 05:13:56,642 Iter 1809, Epoch 396.7105, Loss 0.1807
2018-05-24 05:14:02,183 Iter 1810, Epoch 396.9298, Loss 0.1894
2018-05-24 05:14:07,907 Iter 1811, Epoch 397.1491, Loss 0.1884
2018-05-24 05:14:13,004 Iter 1812, Epoch 397.3684, Loss 0.1994
2018-05-24 05:14:18,624 Iter 1813, Epoch 397.5877, Loss 0.1955
2018-05-24 05:14:24,348 Iter 1814, Epoch 397.8070, Loss 0.1894
2018-05-24 05:14:30,205 Iter 1815, Epoch 398.0263, Loss 0.1731
2018-05-24 05:14:36,013 Iter 1816, Epoch 398.2456, Loss 0.1923
2018-05-24 05:14:41,357 Iter 1817, Epoch 398.4649, Loss 0.1854
2018-05-24 05:14:46,623 Iter 1818, Epoch 398.6842, Loss 0.1806
2018-05-24 05:14:52,627 Iter 1819, Epoch 398.9035, Loss 0.1776
2018-05-24 05:14:58,241 Iter 1820, Epoch 399.1228, Loss 0.1849
2018-05-24 05:15:03,921 Iter 1821, Epoch 399.3421, Loss 0.1825
2018-05-24 05:15:09,522 Iter 1822, Epoch 399.5614, Loss 0.2003
2018-05-24 05:15:14,781 Iter 1823, Epoch 399.7807, Loss 0.1867
2018-05-24 05:15:20,326 Iter 1824, Epoch 400.0000, Loss 0.1874
2018-05-24 05:15:26,475 Iter 1825, Epoch 400.2193, Loss 0.1820
2018-05-24 05:15:32,120 Iter 1826, Epoch 400.4386, Loss 0.2027
2018-05-24 05:15:37,521 Iter 1827, Epoch 400.6579, Loss 0.1935
2018-05-24 05:15:43,169 Iter 1828, Epoch 400.8772, Loss 0.1870
2018-05-24 05:15:48,431 Iter 1829, Epoch 401.0965, Loss 0.1827
2018-05-24 05:15:54,099 Iter 1830, Epoch 401.3158, Loss 0.1707
2018-05-24 05:16:00,085 Iter 1831, Epoch 401.5351, Loss 0.1617
2018-05-24 05:16:06,046 Iter 1832, Epoch 401.7544, Loss 0.1882
2018-05-24 05:16:11,304 Iter 1833, Epoch 401.9737, Loss 0.1746
2018-05-24 05:16:16,903 Iter 1834, Epoch 402.1930, Loss 0.1798
2018-05-24 05:16:22,163 Iter 1835, Epoch 402.4123, Loss 0.1925
2018-05-24 05:16:28,267 Iter 1836, Epoch 402.6316, Loss 0.1597
2018-05-24 05:16:33,739 Iter 1837, Epoch 402.8509, Loss 0.1869
2018-05-24 05:16:39,561 Iter 1838, Epoch 403.0702, Loss 0.1944
2018-05-24 05:16:45,137 Iter 1839, Epoch 403.2895, Loss 0.1951
2018-05-24 05:16:50,708 Iter 1840, Epoch 403.5088, Loss 0.2005
2018-05-24 05:16:55,973 Iter 1841, Epoch 403.7281, Loss 0.1739
2018-05-24 05:17:01,442 Iter 1842, Epoch 403.9474, Loss 0.1684
2018-05-24 05:17:07,246 Iter 1843, Epoch 404.1667, Loss 0.1852
2018-05-24 05:17:12,512 Iter 1844, Epoch 404.3860, Loss 0.1747
2018-05-24 05:17:18,435 Iter 1845, Epoch 404.6053, Loss 0.1940
2018-05-24 05:17:23,979 Iter 1846, Epoch 404.8246, Loss 0.1756
2018-05-24 05:17:29,242 Iter 1847, Epoch 405.0439, Loss 0.1922
2018-05-24 05:17:35,134 Iter 1848, Epoch 405.2632, Loss 0.1844
2018-05-24 05:17:40,393 Iter 1849, Epoch 405.4825, Loss 0.1782
2018-05-24 05:17:46,108 Iter 1850, Epoch 405.7018, Loss 0.1807
2018-05-24 05:17:52,220 Iter 1851, Epoch 405.9211, Loss 0.1857
2018-05-24 05:17:57,841 Iter 1852, Epoch 406.1404, Loss 0.1808
2018-05-24 05:18:03,104 Iter 1853, Epoch 406.3596, Loss 0.2005
2018-05-24 05:18:08,366 Iter 1854, Epoch 406.5789, Loss 0.1835
2018-05-24 05:18:14,041 Iter 1855, Epoch 406.7982, Loss 0.1671
2018-05-24 05:18:19,593 Iter 1856, Epoch 407.0175, Loss 0.1766
2018-05-24 05:18:25,907 Iter 1857, Epoch 407.2368, Loss 0.1753
2018-05-24 05:18:31,165 Iter 1858, Epoch 407.4561, Loss 0.1737
2018-05-24 05:18:36,588 Iter 1859, Epoch 407.6754, Loss 0.1800
2018-05-24 05:18:41,846 Iter 1860, Epoch 407.8947, Loss 0.1980
2018-05-24 05:18:47,767 Iter 1861, Epoch 408.1140, Loss 0.1859
2018-05-24 05:18:53,022 Iter 1862, Epoch 408.3333, Loss 0.1833
2018-05-24 05:18:59,179 Iter 1863, Epoch 408.5526, Loss 0.1828
2018-05-24 05:19:04,656 Iter 1864, Epoch 408.7719, Loss 0.1869
2018-05-24 05:19:10,215 Iter 1865, Epoch 408.9912, Loss 0.1859
2018-05-24 05:19:15,811 Iter 1866, Epoch 409.2105, Loss 0.1793
2018-05-24 05:19:21,076 Iter 1867, Epoch 409.4298, Loss 0.1835
2018-05-24 05:19:27,242 Iter 1868, Epoch 409.6491, Loss 0.1810
2018-05-24 05:19:32,866 Iter 1869, Epoch 409.8684, Loss 0.1861
2018-05-24 05:19:38,375 Iter 1870, Epoch 410.0877, Loss 0.1903
2018-05-24 05:19:43,870 Iter 1871, Epoch 410.3070, Loss 0.1862
2018-05-24 05:19:49,606 Iter 1872, Epoch 410.5263, Loss 0.1819
2018-05-24 05:19:55,209 Iter 1873, Epoch 410.7456, Loss 0.1954
2018-05-24 05:20:00,473 Iter 1874, Epoch 410.9649, Loss 0.1879
2018-05-24 05:20:06,592 Iter 1875, Epoch 411.1842, Loss 0.1761
2018-05-24 05:20:12,117 Iter 1876, Epoch 411.4035, Loss 0.1865
2018-05-24 05:20:17,379 Iter 1877, Epoch 411.6228, Loss 0.1903
2018-05-24 05:20:23,320 Iter 1878, Epoch 411.8421, Loss 0.1974
2018-05-24 05:20:28,931 Iter 1879, Epoch 412.0614, Loss 0.1836
2018-05-24 05:20:34,498 Iter 1880, Epoch 412.2807, Loss 0.1853
2018-05-24 05:20:40,113 Iter 1881, Epoch 412.5000, Loss 0.1951
2018-05-24 05:20:46,137 Iter 1882, Epoch 412.7193, Loss 0.1780
2018-05-24 05:20:51,397 Iter 1883, Epoch 412.9386, Loss 0.1886
2018-05-24 05:20:56,658 Iter 1884, Epoch 413.1579, Loss 0.1786
2018-05-24 05:21:02,873 Iter 1885, Epoch 413.3772, Loss 0.2047
2018-05-24 05:21:08,127 Iter 1886, Epoch 413.5965, Loss 0.1894
2018-05-24 05:21:14,250 Iter 1887, Epoch 413.8158, Loss 0.1726
2018-05-24 05:21:19,915 Iter 1888, Epoch 414.0351, Loss 0.1810
2018-05-24 05:21:25,399 Iter 1889, Epoch 414.2544, Loss 0.1863
2018-05-24 05:21:30,662 Iter 1890, Epoch 414.4737, Loss 0.1714
2018-05-24 05:21:36,598 Iter 1891, Epoch 414.6930, Loss 0.1796
2018-05-24 05:21:42,260 Iter 1892, Epoch 414.9123, Loss 0.1843
2018-05-24 05:21:47,877 Iter 1893, Epoch 415.1316, Loss 0.1746
2018-05-24 05:21:53,567 Iter 1894, Epoch 415.3509, Loss 0.1740
2018-05-24 05:21:59,167 Iter 1895, Epoch 415.5702, Loss 0.1739
2018-05-24 05:22:04,430 Iter 1896, Epoch 415.7895, Loss 0.1927
2018-05-24 05:22:10,380 Iter 1897, Epoch 416.0088, Loss 0.1849
2018-05-24 05:22:15,905 Iter 1898, Epoch 416.2281, Loss 0.1985
2018-05-24 05:22:21,477 Iter 1899, Epoch 416.4474, Loss 0.1955
2018-05-24 05:22:27,143 Iter 1900, Epoch 416.6667, Loss 0.1545
2018-05-24 05:22:32,784 Iter 1901, Epoch 416.8860, Loss 0.1944
2018-05-24 05:22:38,408 Iter 1902, Epoch 417.1053, Loss 0.1771
2018-05-24 05:22:44,114 Iter 1903, Epoch 417.3246, Loss 0.1906
2018-05-24 05:22:49,375 Iter 1904, Epoch 417.5439, Loss 0.1824
2018-05-24 05:22:55,439 Iter 1905, Epoch 417.7632, Loss 0.1742
2018-05-24 05:23:01,034 Iter 1906, Epoch 417.9825, Loss 0.1900
2018-05-24 05:23:06,614 Iter 1907, Epoch 418.2018, Loss 0.1904
2018-05-24 05:23:11,873 Iter 1908, Epoch 418.4211, Loss 0.1800
2018-05-24 05:23:17,975 Iter 1909, Epoch 418.6404, Loss 0.1857
2018-05-24 05:23:23,234 Iter 1910, Epoch 418.8596, Loss 0.1869
2018-05-24 05:23:29,423 Iter 1911, Epoch 419.0789, Loss 0.1841
2018-05-24 05:23:34,961 Iter 1912, Epoch 419.2982, Loss 0.1706
2018-05-24 05:23:40,484 Iter 1913, Epoch 419.5175, Loss 0.1885
2018-05-24 05:23:46,217 Iter 1914, Epoch 419.7368, Loss 0.1916
2018-05-24 05:23:51,767 Iter 1915, Epoch 419.9561, Loss 0.1965
2018-05-24 05:23:57,022 Iter 1916, Epoch 420.1754, Loss 0.1924
2018-05-24 05:24:02,958 Iter 1917, Epoch 420.3947, Loss 0.1768
2018-05-24 05:24:08,537 Iter 1918, Epoch 420.6140, Loss 0.1821
2018-05-24 05:24:13,793 Iter 1919, Epoch 420.8333, Loss 0.1823
2018-05-24 05:24:19,322 Iter 1920, Epoch 421.0526, Loss 0.1821
2018-05-24 05:24:24,842 Iter 1921, Epoch 421.2719, Loss 0.1784
2018-05-24 05:24:31,010 Iter 1922, Epoch 421.4912, Loss 0.1789
2018-05-24 05:24:36,775 Iter 1923, Epoch 421.7105, Loss 0.1863
2018-05-24 05:24:42,414 Iter 1924, Epoch 421.9298, Loss 0.1791
2018-05-24 05:24:47,674 Iter 1925, Epoch 422.1491, Loss 0.1712
2018-05-24 05:24:53,223 Iter 1926, Epoch 422.3684, Loss 0.1895
2018-05-24 05:24:58,872 Iter 1927, Epoch 422.5877, Loss 0.1877
2018-05-24 05:25:04,134 Iter 1928, Epoch 422.8070, Loss 0.1755
2018-05-24 05:25:10,288 Iter 1929, Epoch 423.0263, Loss 0.1886
2018-05-24 05:25:15,915 Iter 1930, Epoch 423.2456, Loss 0.1845
2018-05-24 05:25:21,564 Iter 1931, Epoch 423.4649, Loss 0.1932
2018-05-24 05:25:26,826 Iter 1932, Epoch 423.6842, Loss 0.1909
2018-05-24 05:25:32,968 Iter 1933, Epoch 423.9035, Loss 0.1760
2018-05-24 05:25:38,231 Iter 1934, Epoch 424.1228, Loss 0.1972
2018-05-24 05:25:44,099 Iter 1935, Epoch 424.3421, Loss 0.1770
2018-05-24 05:25:49,681 Iter 1936, Epoch 424.5614, Loss 0.1925
2018-05-24 05:25:55,146 Iter 1937, Epoch 424.7807, Loss 0.1873
2018-05-24 05:26:00,407 Iter 1938, Epoch 425.0000, Loss 0.1881
2018-05-24 05:26:06,542 Iter 1939, Epoch 425.2193, Loss 0.1784
2018-05-24 05:26:12,298 Iter 1940, Epoch 425.4386, Loss 0.2102
2018-05-24 05:26:18,025 Iter 1941, Epoch 425.6579, Loss 0.1887
2018-05-24 05:26:23,284 Iter 1942, Epoch 425.8772, Loss 0.1800
2018-05-24 05:26:28,898 Iter 1943, Epoch 426.0965, Loss 0.1906
2018-05-24 05:26:34,160 Iter 1944, Epoch 426.3158, Loss 0.1808
2018-05-24 05:26:39,880 Iter 1945, Epoch 426.5351, Loss 0.1892
2018-05-24 05:26:45,401 Iter 1946, Epoch 426.7544, Loss 0.1773
2018-05-24 05:26:51,483 Iter 1947, Epoch 426.9737, Loss 0.1940
2018-05-24 05:26:57,021 Iter 1948, Epoch 427.1930, Loss 0.1866
2018-05-24 05:27:02,283 Iter 1949, Epoch 427.4123, Loss 0.1899
2018-05-24 05:27:07,548 Iter 1950, Epoch 427.6316, Loss 0.1795
2018-05-24 05:27:13,671 Iter 1951, Epoch 427.8509, Loss 0.1741
2018-05-24 05:27:19,160 Iter 1952, Epoch 428.0702, Loss 0.2028
2018-05-24 05:27:24,786 Iter 1953, Epoch 428.2895, Loss 0.1796
2018-05-24 05:27:30,382 Iter 1954, Epoch 428.5088, Loss 0.1934
2018-05-24 05:27:35,648 Iter 1955, Epoch 428.7281, Loss 0.1692
2018-05-24 05:27:41,252 Iter 1956, Epoch 428.9474, Loss 0.1951
2018-05-24 05:27:47,168 Iter 1957, Epoch 429.1667, Loss 0.1814
2018-05-24 05:27:52,427 Iter 1958, Epoch 429.3860, Loss 0.1870
2018-05-24 05:27:58,359 Iter 1959, Epoch 429.6053, Loss 0.1762
2018-05-24 05:28:04,044 Iter 1960, Epoch 429.8246, Loss 0.1876
2018-05-24 05:28:09,306 Iter 1961, Epoch 430.0439, Loss 0.1848
2018-05-24 05:28:14,982 Iter 1962, Epoch 430.2632, Loss 0.1902
2018-05-24 05:28:20,640 Iter 1963, Epoch 430.4825, Loss 0.1693
2018-05-24 05:28:26,311 Iter 1964, Epoch 430.7018, Loss 0.1861
2018-05-24 05:28:32,312 Iter 1965, Epoch 430.9211, Loss 0.1939
2018-05-24 05:28:37,944 Iter 1966, Epoch 431.1404, Loss 0.1776
2018-05-24 05:28:43,416 Iter 1967, Epoch 431.3596, Loss 0.1846
2018-05-24 05:28:48,672 Iter 1968, Epoch 431.5789, Loss 0.1984
2018-05-24 05:28:54,317 Iter 1969, Epoch 431.7982, Loss 0.1802
2018-05-24 05:29:00,027 Iter 1970, Epoch 432.0175, Loss 0.1753
2018-05-24 05:29:06,183 Iter 1971, Epoch 432.2368, Loss 0.1972
2018-05-24 05:29:11,822 Iter 1972, Epoch 432.4561, Loss 0.2045
2018-05-24 05:29:17,085 Iter 1973, Epoch 432.6754, Loss 0.1878
2018-05-24 05:29:22,757 Iter 1974, Epoch 432.8947, Loss 0.1684
2018-05-24 05:29:28,794 Iter 1975, Epoch 433.1140, Loss 0.1984
2018-05-24 05:29:34,406 Iter 1976, Epoch 433.3333, Loss 0.2081
2018-05-24 05:29:40,195 Iter 1977, Epoch 433.5526, Loss 0.1921
2018-05-24 05:29:45,448 Iter 1978, Epoch 433.7719, Loss 0.1956
2018-05-24 05:29:51,016 Iter 1979, Epoch 433.9912, Loss 0.1643
2018-05-24 05:29:56,279 Iter 1980, Epoch 434.2105, Loss 0.1848
2018-05-24 05:30:01,891 Iter 1981, Epoch 434.4298, Loss 0.1826
2018-05-24 05:30:07,900 Iter 1982, Epoch 434.6491, Loss 0.1590
2018-05-24 05:30:13,239 Iter 1983, Epoch 434.8684, Loss 0.1788
2018-05-24 05:30:19,023 Iter 1984, Epoch 435.0877, Loss 0.1995
2018-05-24 05:30:24,284 Iter 1985, Epoch 435.3070, Loss 0.1746
2018-05-24 05:30:29,825 Iter 1986, Epoch 435.5263, Loss 0.2014
2018-05-24 05:30:35,619 Iter 1987, Epoch 435.7456, Loss 0.1643
2018-05-24 05:30:41,205 Iter 1988, Epoch 435.9649, Loss 0.1980
2018-05-24 05:30:46,920 Iter 1989, Epoch 436.1842, Loss 0.1916
2018-05-24 05:30:52,617 Iter 1990, Epoch 436.4035, Loss 0.1788
2018-05-24 05:30:57,876 Iter 1991, Epoch 436.6228, Loss 0.1912
2018-05-24 05:31:03,137 Iter 1992, Epoch 436.8421, Loss 0.1767
2018-05-24 05:31:08,640 Iter 1993, Epoch 437.0614, Loss 0.1919
2018-05-24 05:31:14,594 Iter 1994, Epoch 437.2807, Loss 0.1622
2018-05-24 05:31:20,272 Iter 1995, Epoch 437.5000, Loss 0.1986
2018-05-24 05:31:25,881 Iter 1996, Epoch 437.7193, Loss 0.1878
2018-05-24 05:31:31,598 Iter 1997, Epoch 437.9386, Loss 0.1954
2018-05-24 05:31:37,389 Iter 1998, Epoch 438.1579, Loss 0.1927
2018-05-24 05:31:42,646 Iter 1999, Epoch 438.3772, Loss 0.1693
2018-05-24 05:31:48,643 Iter 2000, Epoch 438.5965, Loss 0.1899
2018-05-24 05:31:53,903 Iter 2001, Epoch 438.8158, Loss 0.1826
2018-05-24 05:31:59,383 Iter 2002, Epoch 439.0351, Loss 0.1623
2018-05-24 05:32:04,644 Iter 2003, Epoch 439.2544, Loss 0.1929
2018-05-24 05:32:10,739 Iter 2004, Epoch 439.4737, Loss 0.1852
2018-05-24 05:32:16,362 Iter 2005, Epoch 439.6930, Loss 0.1954
2018-05-24 05:32:21,621 Iter 2006, Epoch 439.9123, Loss 0.1781
2018-05-24 05:32:27,696 Iter 2007, Epoch 440.1316, Loss 0.2104
2018-05-24 05:32:33,217 Iter 2008, Epoch 440.3509, Loss 0.1697
2018-05-24 05:32:38,478 Iter 2009, Epoch 440.5702, Loss 0.1780
2018-05-24 05:32:44,296 Iter 2010, Epoch 440.7895, Loss 0.2024
2018-05-24 05:32:50,037 Iter 2011, Epoch 441.0088, Loss 0.1724
2018-05-24 05:32:55,297 Iter 2012, Epoch 441.2281, Loss 0.1935
2018-05-24 05:33:01,587 Iter 2013, Epoch 441.4474, Loss 0.1952
2018-05-24 05:33:07,315 Iter 2014, Epoch 441.6667, Loss 0.1835
2018-05-24 05:33:12,970 Iter 2015, Epoch 441.8860, Loss 0.1663
2018-05-24 05:33:18,231 Iter 2016, Epoch 442.1053, Loss 0.1921
2018-05-24 05:33:24,418 Iter 2017, Epoch 442.3246, Loss 0.1763
2018-05-24 05:33:29,682 Iter 2018, Epoch 442.5439, Loss 0.1853
2018-05-24 05:33:35,799 Iter 2019, Epoch 442.7632, Loss 0.1831
2018-05-24 05:33:41,343 Iter 2020, Epoch 442.9825, Loss 0.1981
2018-05-24 05:33:46,605 Iter 2021, Epoch 443.2018, Loss 0.1811
2018-05-24 05:33:52,063 Iter 2022, Epoch 443.4211, Loss 0.1866
2018-05-24 05:33:58,110 Iter 2023, Epoch 443.6404, Loss 0.1866
2018-05-24 05:34:03,876 Iter 2024, Epoch 443.8596, Loss 0.1820
2018-05-24 05:34:09,466 Iter 2025, Epoch 444.0789, Loss 0.2034
2018-05-24 05:34:15,400 Iter 2026, Epoch 444.2982, Loss 0.1889
2018-05-24 05:34:20,656 Iter 2027, Epoch 444.5175, Loss 0.1795
2018-05-24 05:34:26,242 Iter 2028, Epoch 444.7368, Loss 0.1909
2018-05-24 05:34:31,827 Iter 2029, Epoch 444.9561, Loss 0.1703
2018-05-24 05:34:37,922 Iter 2030, Epoch 445.1754, Loss 0.2061
2018-05-24 05:34:43,450 Iter 2031, Epoch 445.3947, Loss 0.1862
2018-05-24 05:34:48,704 Iter 2032, Epoch 445.6140, Loss 0.1904
2018-05-24 05:34:53,965 Iter 2033, Epoch 445.8333, Loss 0.1830
2018-05-24 05:34:59,826 Iter 2034, Epoch 446.0526, Loss 0.1752
2018-05-24 05:35:05,494 Iter 2035, Epoch 446.2719, Loss 0.1682
2018-05-24 05:35:11,020 Iter 2036, Epoch 446.4912, Loss 0.1887
2018-05-24 05:35:17,094 Iter 2037, Epoch 446.7105, Loss 0.1994
2018-05-24 05:35:22,348 Iter 2038, Epoch 446.9298, Loss 0.1793
2018-05-24 05:35:27,611 Iter 2039, Epoch 447.1491, Loss 0.1882
2018-05-24 05:35:33,518 Iter 2040, Epoch 447.3684, Loss 0.1727
2018-05-24 05:35:39,259 Iter 2041, Epoch 447.5877, Loss 0.1951
2018-05-24 05:35:44,895 Iter 2042, Epoch 447.8070, Loss 0.1899
2018-05-24 05:35:50,574 Iter 2043, Epoch 448.0263, Loss 0.1816
2018-05-24 05:35:56,012 Iter 2044, Epoch 448.2456, Loss 0.1841
2018-05-24 05:36:01,726 Iter 2045, Epoch 448.4649, Loss 0.1831
2018-05-24 05:36:07,463 Iter 2046, Epoch 448.6842, Loss 0.1804
2018-05-24 05:36:13,080 Iter 2047, Epoch 448.9035, Loss 0.1866
2018-05-24 05:36:18,742 Iter 2048, Epoch 449.1228, Loss 0.1693
2018-05-24 05:36:24,218 Iter 2049, Epoch 449.3421, Loss 0.1827
2018-05-24 05:36:29,844 Iter 2050, Epoch 449.5614, Loss 0.1727
2018-05-24 05:36:35,437 Iter 2051, Epoch 449.7807, Loss 0.1750
2018-05-24 05:36:41,013 Iter 2052, Epoch 450.0000, Loss 0.1825
2018-05-24 05:36:46,274 Iter 2053, Epoch 450.2193, Loss 0.1759
2018-05-24 05:36:52,334 Iter 2054, Epoch 450.4386, Loss 0.1901
2018-05-24 05:36:58,318 Iter 2055, Epoch 450.6579, Loss 0.1763
2018-05-24 05:37:03,579 Iter 2056, Epoch 450.8772, Loss 0.1802
2018-05-24 05:37:09,134 Iter 2057, Epoch 451.0965, Loss 0.1875
2018-05-24 05:37:14,396 Iter 2058, Epoch 451.3158, Loss 0.1788
2018-05-24 05:37:20,344 Iter 2059, Epoch 451.5351, Loss 0.1977
2018-05-24 05:37:26,097 Iter 2060, Epoch 451.7544, Loss 0.1727
2018-05-24 05:37:31,830 Iter 2061, Epoch 451.9737, Loss 0.1803
2018-05-24 05:37:37,095 Iter 2062, Epoch 452.1930, Loss 0.1717
2018-05-24 05:37:42,582 Iter 2063, Epoch 452.4123, Loss 0.1774
2018-05-24 05:37:47,844 Iter 2064, Epoch 452.6316, Loss 0.1914
2018-05-24 05:37:53,744 Iter 2065, Epoch 452.8509, Loss 0.1774
2018-05-24 05:37:59,010 Iter 2066, Epoch 453.0702, Loss 0.1732
2018-05-24 05:38:05,074 Iter 2067, Epoch 453.2895, Loss 0.1779
2018-05-24 05:38:10,429 Iter 2068, Epoch 453.5088, Loss 0.1895
2018-05-24 05:38:16,040 Iter 2069, Epoch 453.7281, Loss 0.1727
2018-05-24 05:38:21,566 Iter 2070, Epoch 453.9474, Loss 0.1853
2018-05-24 05:38:27,708 Iter 2071, Epoch 454.1667, Loss 0.1723
2018-05-24 05:38:33,454 Iter 2072, Epoch 454.3860, Loss 0.1818
2018-05-24 05:38:38,714 Iter 2073, Epoch 454.6053, Loss 0.1836
2018-05-24 05:38:44,216 Iter 2074, Epoch 454.8246, Loss 0.1967
2018-05-24 05:38:49,993 Iter 2075, Epoch 455.0439, Loss 0.2032
2018-05-24 05:38:55,256 Iter 2076, Epoch 455.2632, Loss 0.1838
2018-05-24 05:39:00,906 Iter 2077, Epoch 455.4825, Loss 0.1799
2018-05-24 05:39:06,615 Iter 2078, Epoch 455.7018, Loss 0.1814
2018-05-24 05:39:12,216 Iter 2079, Epoch 455.9211, Loss 0.1897
2018-05-24 05:39:17,478 Iter 2080, Epoch 456.1404, Loss 0.1807
2018-05-24 05:39:23,179 Iter 2081, Epoch 456.3596, Loss 0.1716
2018-05-24 05:39:28,737 Iter 2082, Epoch 456.5789, Loss 0.2068
2018-05-24 05:39:34,482 Iter 2083, Epoch 456.7982, Loss 0.1863
2018-05-24 05:39:40,179 Iter 2084, Epoch 457.0175, Loss 0.1710
2018-05-24 05:39:45,764 Iter 2085, Epoch 457.2368, Loss 0.1803
2018-05-24 05:39:51,281 Iter 2086, Epoch 457.4561, Loss 0.1790
2018-05-24 05:39:56,539 Iter 2087, Epoch 457.6754, Loss 0.1862
2018-05-24 05:40:02,239 Iter 2088, Epoch 457.8947, Loss 0.1777
2018-05-24 05:40:08,211 Iter 2089, Epoch 458.1140, Loss 0.1755
2018-05-24 05:40:13,475 Iter 2090, Epoch 458.3333, Loss 0.1871
2018-05-24 05:40:19,573 Iter 2091, Epoch 458.5526, Loss 0.1802
2018-05-24 05:40:25,147 Iter 2092, Epoch 458.7719, Loss 0.1866
2018-05-24 05:40:30,408 Iter 2093, Epoch 458.9912, Loss 0.1814
2018-05-24 05:40:36,455 Iter 2094, Epoch 459.2105, Loss 0.1791
2018-05-24 05:40:42,128 Iter 2095, Epoch 459.4298, Loss 0.1790
2018-05-24 05:40:47,903 Iter 2096, Epoch 459.6491, Loss 0.2009
2018-05-24 05:40:53,672 Iter 2097, Epoch 459.8684, Loss 0.1943
2018-05-24 05:40:59,572 Iter 2098, Epoch 460.0877, Loss 0.1745
2018-05-24 05:41:04,834 Iter 2099, Epoch 460.3070, Loss 0.1631
2018-05-24 05:41:10,420 Iter 2100, Epoch 460.5263, Loss 0.1825
2018-05-24 05:41:16,003 Iter 2101, Epoch 460.7456, Loss 0.1873
2018-05-24 05:41:21,267 Iter 2102, Epoch 460.9649, Loss 0.1883
2018-05-24 05:41:27,325 Iter 2103, Epoch 461.1842, Loss 0.1785
2018-05-24 05:41:32,727 Iter 2104, Epoch 461.4035, Loss 0.1925
2018-05-24 05:41:38,248 Iter 2105, Epoch 461.6228, Loss 0.1765
2018-05-24 05:41:43,798 Iter 2106, Epoch 461.8421, Loss 0.1780
2018-05-24 05:41:49,065 Iter 2107, Epoch 462.0614, Loss 0.1888
2018-05-24 05:41:54,716 Iter 2108, Epoch 462.2807, Loss 0.1824
2018-05-24 05:42:01,112 Iter 2109, Epoch 462.5000, Loss 0.1788
2018-05-24 05:42:06,375 Iter 2110, Epoch 462.7193, Loss 0.1677
2018-05-24 05:42:11,873 Iter 2111, Epoch 462.9386, Loss 0.1734
2018-05-24 05:42:17,449 Iter 2112, Epoch 463.1579, Loss 0.1773
2018-05-24 05:42:22,982 Iter 2113, Epoch 463.3772, Loss 0.1907
2018-05-24 05:42:28,696 Iter 2114, Epoch 463.5965, Loss 0.1940
2018-05-24 05:42:34,350 Iter 2115, Epoch 463.8158, Loss 0.1738
2018-05-24 05:42:39,920 Iter 2116, Epoch 464.0351, Loss 0.1838
2018-05-24 05:42:45,485 Iter 2117, Epoch 464.2544, Loss 0.1911
2018-05-24 05:42:51,049 Iter 2118, Epoch 464.4737, Loss 0.1798
2018-05-24 05:42:56,314 Iter 2119, Epoch 464.6930, Loss 0.1788
2018-05-24 05:43:01,896 Iter 2120, Epoch 464.9123, Loss 0.1946
2018-05-24 05:43:07,833 Iter 2121, Epoch 465.1316, Loss 0.1811
2018-05-24 05:43:13,402 Iter 2122, Epoch 465.3509, Loss 0.1972
2018-05-24 05:43:18,668 Iter 2123, Epoch 465.5702, Loss 0.1646
2018-05-24 05:43:24,444 Iter 2124, Epoch 465.7895, Loss 0.1936
2018-05-24 05:43:29,706 Iter 2125, Epoch 466.0088, Loss 0.1795
2018-05-24 05:43:35,970 Iter 2126, Epoch 466.2281, Loss 0.1717
2018-05-24 05:43:41,792 Iter 2127, Epoch 466.4474, Loss 0.1870
2018-05-24 05:43:47,054 Iter 2128, Epoch 466.6667, Loss 0.1698
2018-05-24 05:43:52,699 Iter 2129, Epoch 466.8860, Loss 0.1887
2018-05-24 05:43:57,962 Iter 2130, Epoch 467.1053, Loss 0.1982
2018-05-24 05:44:03,528 Iter 2131, Epoch 467.3246, Loss 0.1749
2018-05-24 05:44:09,226 Iter 2132, Epoch 467.5439, Loss 0.1812
2018-05-24 05:44:15,256 Iter 2133, Epoch 467.7632, Loss 0.1941
2018-05-24 05:44:20,511 Iter 2134, Epoch 467.9825, Loss 0.1727
2018-05-24 05:44:25,773 Iter 2135, Epoch 468.2018, Loss 0.1952
2018-05-24 05:44:31,816 Iter 2136, Epoch 468.4211, Loss 0.1921
2018-05-24 05:44:37,371 Iter 2137, Epoch 468.6404, Loss 0.1901
2018-05-24 05:44:43,002 Iter 2138, Epoch 468.8596, Loss 0.1877
2018-05-24 05:44:48,684 Iter 2139, Epoch 469.0789, Loss 0.1770
2018-05-24 05:44:54,166 Iter 2140, Epoch 469.2982, Loss 0.1704
2018-05-24 05:44:59,429 Iter 2141, Epoch 469.5175, Loss 0.1774
2018-05-24 05:45:05,103 Iter 2142, Epoch 469.7368, Loss 0.1906
2018-05-24 05:45:10,993 Iter 2143, Epoch 469.9561, Loss 0.1876
2018-05-24 05:45:16,575 Iter 2144, Epoch 470.1754, Loss 0.1817
2018-05-24 05:45:22,166 Iter 2145, Epoch 470.3947, Loss 0.1757
2018-05-24 05:45:27,834 Iter 2146, Epoch 470.6140, Loss 0.2031
2018-05-24 05:45:33,097 Iter 2147, Epoch 470.8333, Loss 0.1949
2018-05-24 05:45:39,020 Iter 2148, Epoch 471.0526, Loss 0.1710
2018-05-24 05:45:44,663 Iter 2149, Epoch 471.2719, Loss 0.1837
2018-05-24 05:45:50,310 Iter 2150, Epoch 471.4912, Loss 0.1765
2018-05-24 05:45:55,744 Iter 2151, Epoch 471.7105, Loss 0.1995
2018-05-24 05:46:01,426 Iter 2152, Epoch 471.9298, Loss 0.1850
2018-05-24 05:46:06,691 Iter 2153, Epoch 472.1491, Loss 0.1702
2018-05-24 05:46:12,663 Iter 2154, Epoch 472.3684, Loss 0.2043
2018-05-24 05:46:18,428 Iter 2155, Epoch 472.5877, Loss 0.2013
2018-05-24 05:46:23,996 Iter 2156, Epoch 472.8070, Loss 0.1774
2018-05-24 05:46:29,351 Iter 2157, Epoch 473.0263, Loss 0.1710
2018-05-24 05:46:34,992 Iter 2158, Epoch 473.2456, Loss 0.1807
2018-05-24 05:46:40,252 Iter 2159, Epoch 473.4649, Loss 0.2068
2018-05-24 05:46:46,402 Iter 2160, Epoch 473.6842, Loss 0.1809
2018-05-24 05:46:52,005 Iter 2161, Epoch 473.9035, Loss 0.1934
2018-05-24 05:46:57,471 Iter 2162, Epoch 474.1228, Loss 0.1556
2018-05-24 05:47:03,213 Iter 2163, Epoch 474.3421, Loss 0.1910
2018-05-24 05:47:08,669 Iter 2164, Epoch 474.5614, Loss 0.1880
2018-05-24 05:47:13,932 Iter 2165, Epoch 474.7807, Loss 0.1816
2018-05-24 05:47:19,975 Iter 2166, Epoch 475.0000, Loss 0.1751
2018-05-24 05:47:25,573 Iter 2167, Epoch 475.2193, Loss 0.1857
2018-05-24 05:47:31,059 Iter 2168, Epoch 475.4386, Loss 0.1669
2018-05-24 05:47:36,656 Iter 2169, Epoch 475.6579, Loss 0.2001
2018-05-24 05:47:42,771 Iter 2170, Epoch 475.8772, Loss 0.1868
2018-05-24 05:47:48,035 Iter 2171, Epoch 476.0965, Loss 0.1803
2018-05-24 05:47:53,526 Iter 2172, Epoch 476.3158, Loss 0.1677
2018-05-24 05:47:59,209 Iter 2173, Epoch 476.5351, Loss 0.1679
2018-05-24 05:48:04,474 Iter 2174, Epoch 476.7544, Loss 0.1846
2018-05-24 05:48:10,688 Iter 2175, Epoch 476.9737, Loss 0.2082
2018-05-24 05:48:16,199 Iter 2176, Epoch 477.1930, Loss 0.1697
2018-05-24 05:48:21,465 Iter 2177, Epoch 477.4123, Loss 0.1725
2018-05-24 05:48:26,731 Iter 2178, Epoch 477.6316, Loss 0.1916
2018-05-24 05:48:32,228 Iter 2179, Epoch 477.8509, Loss 0.1845
2018-05-24 05:48:37,938 Iter 2180, Epoch 478.0702, Loss 0.1597
2018-05-24 05:48:44,035 Iter 2181, Epoch 478.2895, Loss 0.1698
2018-05-24 05:48:49,297 Iter 2182, Epoch 478.5088, Loss 0.1782
2018-05-24 05:48:54,568 Iter 2183, Epoch 478.7281, Loss 0.1838
2018-05-24 05:49:00,433 Iter 2184, Epoch 478.9474, Loss 0.1991
2018-05-24 05:49:06,057 Iter 2185, Epoch 479.1667, Loss 0.1677
2018-05-24 05:49:11,816 Iter 2186, Epoch 479.3860, Loss 0.1958
2018-05-24 05:49:17,547 Iter 2187, Epoch 479.6053, Loss 0.1734
2018-05-24 05:49:22,812 Iter 2188, Epoch 479.8246, Loss 0.1913
2018-05-24 05:49:28,389 Iter 2189, Epoch 480.0439, Loss 0.1877
2018-05-24 05:49:33,654 Iter 2190, Epoch 480.2632, Loss 0.1747
2018-05-24 05:49:39,786 Iter 2191, Epoch 480.4825, Loss 0.1891
2018-05-24 05:49:45,582 Iter 2192, Epoch 480.7018, Loss 0.1768
2018-05-24 05:49:51,427 Iter 2193, Epoch 480.9211, Loss 0.1990
2018-05-24 05:49:56,690 Iter 2194, Epoch 481.1404, Loss 0.1813
2018-05-24 05:50:02,317 Iter 2195, Epoch 481.3596, Loss 0.1647
2018-05-24 05:50:07,581 Iter 2196, Epoch 481.5789, Loss 0.1742
2018-05-24 05:50:13,702 Iter 2197, Epoch 481.7982, Loss 0.1776
2018-05-24 05:50:19,255 Iter 2198, Epoch 482.0175, Loss 0.1871
2018-05-24 05:50:24,757 Iter 2199, Epoch 482.2368, Loss 0.1808
2018-05-24 05:50:30,488 Iter 2200, Epoch 482.4561, Loss 0.1807
2018-05-24 05:50:35,752 Iter 2201, Epoch 482.6754, Loss 0.1790
2018-05-24 05:50:41,713 Iter 2202, Epoch 482.8947, Loss 0.1759
2018-05-24 05:50:47,246 Iter 2203, Epoch 483.1140, Loss 0.1745
2018-05-24 05:50:52,776 Iter 2204, Epoch 483.3333, Loss 0.1724
2018-05-24 05:50:58,358 Iter 2205, Epoch 483.5526, Loss 0.1985
2018-05-24 05:51:03,856 Iter 2206, Epoch 483.7719, Loss 0.1863
2018-05-24 05:51:09,115 Iter 2207, Epoch 483.9912, Loss 0.1857
2018-05-24 05:51:14,733 Iter 2208, Epoch 484.2105, Loss 0.1883
2018-05-24 05:51:20,634 Iter 2209, Epoch 484.4298, Loss 0.1779
2018-05-24 05:51:26,209 Iter 2210, Epoch 484.6491, Loss 0.1882
2018-05-24 05:51:31,832 Iter 2211, Epoch 484.8684, Loss 0.1948
2018-05-24 05:51:37,511 Iter 2212, Epoch 485.0877, Loss 0.1941
2018-05-24 05:51:42,773 Iter 2213, Epoch 485.3070, Loss 0.1862
2018-05-24 05:51:48,630 Iter 2214, Epoch 485.5263, Loss 0.1644
2018-05-24 05:51:54,155 Iter 2215, Epoch 485.7456, Loss 0.1792
2018-05-24 05:51:59,832 Iter 2216, Epoch 485.9649, Loss 0.1863
2018-05-24 05:52:05,471 Iter 2217, Epoch 486.1842, Loss 0.1845
2018-05-24 05:52:11,146 Iter 2218, Epoch 486.4035, Loss 0.1932
2018-05-24 05:52:16,682 Iter 2219, Epoch 486.6228, Loss 0.1789
2018-05-24 05:52:22,291 Iter 2220, Epoch 486.8421, Loss 0.1717
2018-05-24 05:52:27,839 Iter 2221, Epoch 487.0614, Loss 0.1782
2018-05-24 05:52:33,095 Iter 2222, Epoch 487.2807, Loss 0.1876
2018-05-24 05:52:39,367 Iter 2223, Epoch 487.5000, Loss 0.1839
2018-05-24 05:52:44,629 Iter 2224, Epoch 487.7193, Loss 0.1823
2018-05-24 05:52:50,323 Iter 2225, Epoch 487.9386, Loss 0.1836
2018-05-24 05:52:55,586 Iter 2226, Epoch 488.1579, Loss 0.1827
2018-05-24 05:53:01,550 Iter 2227, Epoch 488.3772, Loss 0.1811
2018-05-24 05:53:07,751 Iter 2228, Epoch 488.5965, Loss 0.1697
2018-05-24 05:53:13,012 Iter 2229, Epoch 488.8158, Loss 0.1844
2018-05-24 05:53:18,584 Iter 2230, Epoch 489.0351, Loss 0.1867
2018-05-24 05:53:24,248 Iter 2231, Epoch 489.2544, Loss 0.1856
2018-05-24 05:53:29,846 Iter 2232, Epoch 489.4737, Loss 0.1924
2018-05-24 05:53:35,497 Iter 2233, Epoch 489.6930, Loss 0.1833
2018-05-24 05:53:41,208 Iter 2234, Epoch 489.9123, Loss 0.1763
2018-05-24 05:53:46,994 Iter 2235, Epoch 490.1316, Loss 0.1571
2018-05-24 05:53:52,252 Iter 2236, Epoch 490.3509, Loss 0.1727
2018-05-24 05:53:57,806 Iter 2237, Epoch 490.5702, Loss 0.1900
2018-05-24 05:54:03,355 Iter 2238, Epoch 490.7895, Loss 0.1992
2018-05-24 05:54:09,082 Iter 2239, Epoch 491.0088, Loss 0.1755
2018-05-24 05:54:14,773 Iter 2240, Epoch 491.2281, Loss 0.1987
2018-05-24 05:54:20,659 Iter 2241, Epoch 491.4474, Loss 0.1878
2018-05-24 05:54:25,919 Iter 2242, Epoch 491.6667, Loss 0.1946
2018-05-24 05:54:31,181 Iter 2243, Epoch 491.8860, Loss 0.1996
2018-05-24 05:54:36,701 Iter 2244, Epoch 492.1053, Loss 0.1869
2018-05-24 05:54:42,694 Iter 2245, Epoch 492.3246, Loss 0.1749
2018-05-24 05:54:48,285 Iter 2246, Epoch 492.5439, Loss 0.1789
2018-05-24 05:54:54,073 Iter 2247, Epoch 492.7632, Loss 0.2068
2018-05-24 05:54:59,333 Iter 2248, Epoch 492.9825, Loss 0.1772
2018-05-24 05:55:04,593 Iter 2249, Epoch 493.2018, Loss 0.1846
2018-05-24 05:55:10,148 Iter 2250, Epoch 493.4211, Loss 0.1855
2018-05-24 05:55:16,042 Iter 2251, Epoch 493.6404, Loss 0.1938
2018-05-24 05:55:21,799 Iter 2252, Epoch 493.8596, Loss 0.1719
2018-05-24 05:55:27,376 Iter 2253, Epoch 494.0789, Loss 0.1798
2018-05-24 05:55:33,062 Iter 2254, Epoch 494.2982, Loss 0.1731
2018-05-24 05:55:38,319 Iter 2255, Epoch 494.5175, Loss 0.1821
2018-05-24 05:55:44,281 Iter 2256, Epoch 494.7368, Loss 0.1759
2018-05-24 05:55:49,956 Iter 2257, Epoch 494.9561, Loss 0.1965
2018-05-24 05:55:55,422 Iter 2258, Epoch 495.1754, Loss 0.1868
2018-05-24 05:56:00,992 Iter 2259, Epoch 495.3947, Loss 0.1901
2018-05-24 05:56:06,556 Iter 2260, Epoch 495.6140, Loss 0.1810
2018-05-24 05:56:11,819 Iter 2261, Epoch 495.8333, Loss 0.1720
2018-05-24 05:56:17,732 Iter 2262, Epoch 496.0526, Loss 0.1850
2018-05-24 05:56:23,365 Iter 2263, Epoch 496.2719, Loss 0.1857
2018-05-24 05:56:29,296 Iter 2264, Epoch 496.4912, Loss 0.1808
2018-05-24 05:56:34,550 Iter 2265, Epoch 496.7105, Loss 0.1768
2018-05-24 05:56:40,250 Iter 2266, Epoch 496.9298, Loss 0.1857
2018-05-24 05:56:45,510 Iter 2267, Epoch 497.1491, Loss 0.2026
2018-05-24 05:56:51,356 Iter 2268, Epoch 497.3684, Loss 0.1873
2018-05-24 05:56:56,616 Iter 2269, Epoch 497.5877, Loss 0.1763
2018-05-24 05:57:02,778 Iter 2270, Epoch 497.8070, Loss 0.1789
2018-05-24 05:57:08,147 Iter 2271, Epoch 498.0263, Loss 0.1768
2018-05-24 05:57:13,401 Iter 2272, Epoch 498.2456, Loss 0.1931
2018-05-24 05:57:18,852 Iter 2273, Epoch 498.4649, Loss 0.1713
2018-05-24 05:57:24,424 Iter 2274, Epoch 498.6842, Loss 0.1782
2018-05-24 05:57:29,672 Iter 2275, Epoch 498.9035, Loss 0.1868
2018-05-24 05:57:34,920 Iter 2276, Epoch 499.1228, Loss 0.2065
2018-05-24 05:57:40,165 Iter 2277, Epoch 499.3421, Loss 0.1708
2018-05-24 05:57:45,414 Iter 2278, Epoch 499.5614, Loss 0.2028
2018-05-24 05:57:50,663 Iter 2279, Epoch 499.7807, Loss 0.1997
2018-05-24 05:57:55,911 Iter 2280, Epoch 500.0000, Loss 0.1790
2018-05-24 05:57:56,490 --------------------------------------------------
2018-05-24 05:57:56,491 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:00:36,871 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:00:37,102 --------------------------------------------------
2018-05-24 08:00:37,102 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:02:06,583 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:02:06,778 --------------------------------------------------
2018-05-24 08:02:06,778 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:02:32,528 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:02:32,720 --------------------------------------------------
2018-05-24 08:02:32,721 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:03:47,301 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:03:47,564 --------------------------------------------------
2018-05-24 08:03:47,565 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:05:33,412 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:05:33,615 --------------------------------------------------
2018-05-24 08:05:33,615 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:07:17,879 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:07:18,078 --------------------------------------------------
2018-05-24 08:07:18,079 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:08:28,994 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:08:29,193 --------------------------------------------------
2018-05-24 08:08:29,193 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:09:01,856 => loaded checkpoint '/home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar' (iter 2280)
alpha: null
batch_size: 50
cfg: deepmedic_ce_50_50_c25
criterion: cross_entropy
data_dir: /home/thuyen/Data/brats17/Brats17TrainingData
dataset: DualDataM
gpu: '0'
mini_batch_size: 50
name: deepmedic_ce_50_50_c25
net: DeepMedic
net_params:
  c: 25
  m: 150
  n1: 30
  n2: 40
  n3: 50
num_epochs: 500
num_patches: 20
opt: Adam
opt_params:
  amsgrad: true
  lr: 0.001
  weight_decay: 0.0001
resume: /home/thuyen/Data/brats17/src/ckpts/deepmedic_ce_50_50_c25/model_last.tar
root: /home/thuyen/Data/brats17/
sample_size: 25
save_freq: 50
schedule:
  300: null
  400: null
seed: 2018
start_iter: 2280
sub_sample_size: 19
target_size: 9
test_data_dir: /home/thuyen/Data/brats17/Brats17ValidationData
test_transforms: TensorType((torch.float32, torch.int64, torch.int64))
train_dir: /home/thuyen/Data/brats17/src/ckpts
train_list: train_0.txt
train_transforms: Compose([ ToNumpy(), RandSelect(0.5, Flip(1)), RandSelect(0.5, Flip(2)),
  RandSelect(0.5, Flip(3)), RandSelect(0.5, Noise(dim=3, num=2)), NumpyType((np.float32,
  np.float32, np.int64, np.int64, np.int64)), ToTensor(), ])
valid_freq: 50
valid_list: valid_0.txt
workers: 6

2018-05-24 08:09:02,056 --------------------------------------------------
2018-05-24 08:09:02,057 Iter 2280, Epoch 500.0000, validate validation data
2018-05-24 08:09:18,609 Subject 1/57,  Brats17_CBICA_ASH_1, whole: 0.7499, core: 0.8945, enhancing: 0.7496
2018-05-24 08:09:29,614 Subject 2/57,   Brats17_TCIA_203_1, whole: 0.9552, core: 0.9302, enhancing: 0.8989
2018-05-24 08:09:40,589 Subject 3/57,   Brats17_TCIA_309_1, whole: 0.9195, core: 0.8735, enhancing: 0.8635
2018-05-24 08:09:51,604 Subject 4/57,  Brats17_CBICA_ASA_1, whole: 0.9439, core: 0.9588, enhancing: 0.9050
2018-05-24 08:10:02,649 Subject 5/57,   Brats17_TCIA_425_1, whole: 0.8367, core: 0.1662, enhancing: 0.1669
2018-05-24 08:10:13,717 Subject 6/57,    Brats17_2013_25_1, whole: 0.8946, core: 0.0004, enhancing: 0.0000
2018-05-24 08:10:24,731 Subject 7/57,   Brats17_TCIA_603_1, whole: 0.9576, core: 0.9226, enhancing: 0.7637
2018-05-24 08:10:35,777 Subject 8/57,  Brats17_CBICA_AOO_1, whole: 0.9420, core: 0.9307, enhancing: 0.8736
2018-05-24 08:10:46,879 Subject 9/57,   Brats17_TCIA_499_1, whole: 0.8855, core: 0.9524, enhancing: 0.9176
2018-05-24 08:10:57,957 Subject 10/57,  Brats17_CBICA_ASW_1, whole: 0.9269, core: 0.9468, enhancing: 0.8958
2018-05-24 08:11:08,899 Subject 11/57,   Brats17_TCIA_390_1, whole: 0.8749, core: 0.7668, enhancing: 0.8472
2018-05-24 08:11:19,864 Subject 12/57,   Brats17_TCIA_117_1, whole: 0.9362, core: 0.9440, enhancing: 0.8865
2018-05-24 08:11:30,843 Subject 13/57,   Brats17_TCIA_478_1, whole: 0.9584, core: 0.9476, enhancing: 0.8820
2018-05-24 08:11:41,849 Subject 14/57,  Brats17_CBICA_AUN_1, whole: 0.9372, core: 0.9584, enhancing: 0.8884
2018-05-24 08:11:52,859 Subject 15/57,  Brats17_CBICA_ATB_1, whole: 0.9056, core: 0.9283, enhancing: 0.8812
2018-05-24 08:12:03,868 Subject 16/57,   Brats17_TCIA_257_1, whole: 0.8670, core: 0.9027, enhancing: 0.8324
2018-05-24 08:12:14,861 Subject 17/57,  Brats17_CBICA_BFB_1, whole: 0.9328, core: 0.9289, enhancing: 0.8296
2018-05-24 08:12:25,874 Subject 18/57,   Brats17_TCIA_280_1, whole: 0.9311, core: 0.9157, enhancing: 0.9094
2018-05-24 08:12:36,855 Subject 19/57,  Brats17_CBICA_ATF_1, whole: 0.7710, core: 0.6964, enhancing: 0.5689
2018-05-24 08:12:47,847 Subject 20/57,   Brats17_TCIA_131_1, whole: 0.9324, core: 0.9563, enhancing: 0.9436
2018-05-24 08:12:58,793 Subject 21/57,   Brats17_TCIA_460_1, whole: 0.8921, core: 0.8544, enhancing: 0.8232
2018-05-24 08:13:09,823 Subject 22/57,   Brats17_TCIA_274_1, whole: 0.9349, core: 0.9357, enhancing: 0.8965
2018-05-24 08:13:20,857 Subject 23/57,   Brats17_TCIA_265_1, whole: 0.9470, core: 0.8793, enhancing: 0.7128
2018-05-24 08:13:31,833 Subject 24/57,   Brats17_TCIA_394_1, whole: 0.9071, core: 0.8334, enhancing: 0.7906
2018-05-24 08:13:42,855 Subject 25/57,   Brats17_TCIA_332_1, whole: 0.9441, core: 0.8472, enhancing: 0.8014
2018-05-24 08:13:53,879 Subject 26/57,    Brats17_2013_27_1, whole: 0.9264, core: 0.9216, enhancing: 0.7980
2018-05-24 08:14:04,910 Subject 27/57,  Brats17_CBICA_AQZ_1, whole: 0.9409, core: 0.9121, enhancing: 0.7809
2018-05-24 08:14:15,932 Subject 28/57,  Brats17_CBICA_AXJ_1, whole: 0.8925, core: 0.2416, enhancing: 0.8028
2018-05-24 08:14:26,911 Subject 29/57,  Brats17_CBICA_AQO_1, whole: 0.9516, core: 0.9494, enhancing: 0.9138
2018-05-24 08:14:37,961 Subject 30/57,   Brats17_TCIA_338_1, whole: 0.8243, core: 0.8654, enhancing: 0.9073
2018-05-24 08:14:49,068 Subject 31/57,  Brats17_CBICA_AZH_1, whole: 0.9304, core: 0.9558, enhancing: 0.8964
2018-05-24 08:15:00,063 Subject 32/57,    Brats17_2013_18_1, whole: 0.9150, core: 0.9065, enhancing: 0.9109
2018-05-24 08:15:11,038 Subject 33/57,   Brats17_TCIA_226_1, whole: 0.9301, core: 0.8475, enhancing: 0.8501
2018-05-24 08:15:22,028 Subject 34/57,   Brats17_TCIA_321_1, whole: 0.9205, core: 0.9567, enhancing: 0.9449
2018-05-24 08:15:33,051 Subject 35/57,   Brats17_TCIA_283_1, whole: 0.7651, core: 0.8758, enhancing: 0.8155
2018-05-24 08:15:44,093 Subject 36/57,  Brats17_CBICA_AXW_1, whole: 0.9145, core: 0.9499, enhancing: 0.8849
2018-05-24 08:15:55,137 Subject 37/57,   Brats17_TCIA_411_1, whole: 0.7572, core: 0.5264, enhancing: 0.2954
2018-05-24 08:16:06,186 Subject 38/57,  Brats17_CBICA_AXM_1, whole: 0.9234, core: 0.9477, enhancing: 0.8384
2018-05-24 08:16:17,272 Subject 39/57,  Brats17_CBICA_ASN_1, whole: 0.9327, core: 0.9593, enhancing: 0.8575
2018-05-24 08:16:28,326 Subject 40/57,  Brats17_CBICA_ASO_1, whole: 0.9316, core: 0.9396, enhancing: 0.8827
2018-05-24 08:16:39,413 Subject 41/57,   Brats17_TCIA_444_1, whole: 0.8752, core: 0.8380, enhancing: 0.6385
2018-05-24 08:16:50,489 Subject 42/57,   Brats17_TCIA_179_1, whole: 0.9255, core: 0.9312, enhancing: 0.9002
2018-05-24 08:17:01,604 Subject 43/57,   Brats17_TCIA_152_1, whole: 0.9616, core: 0.5981, enhancing: 0.8333
2018-05-24 08:17:12,609 Subject 44/57,   Brats17_TCIA_615_1, whole: 0.9176, core: 0.5647, enhancing: 0.3401
2018-05-24 08:17:23,635 Subject 45/57,   Brats17_TCIA_451_1, whole: 0.9569, core: 0.9497, enhancing: 0.0000
2018-05-24 08:17:34,654 Subject 46/57,   Brats17_TCIA_480_1, whole: 0.9554, core: 0.7483, enhancing: 0.1626
2018-05-24 08:17:45,605 Subject 47/57,   Brats17_TCIA_624_1, whole: 0.9695, core: 0.5044, enhancing: 0.8576
2018-05-24 08:17:56,588 Subject 48/57,    Brats17_2013_16_1, whole: 0.9069, core: 0.1895, enhancing: 0.0000
2018-05-24 08:18:07,661 Subject 49/57,   Brats17_TCIA_402_1, whole: 0.9543, core: 0.9233, enhancing: 0.0000
2018-05-24 08:18:18,833 Subject 50/57,   Brats17_TCIA_255_1, whole: 0.9256, core: 0.8613, enhancing: 0.4143
2018-05-24 08:18:29,925 Subject 51/57,   Brats17_TCIA_449_1, whole: 0.9701, core: 0.9224, enhancing: 0.9453
2018-05-24 08:18:40,949 Subject 52/57,    Brats17_2013_29_1, whole: 0.9431, core: 0.6355, enhancing: 0.0000
2018-05-24 08:18:51,950 Subject 53/57,   Brats17_TCIA_620_1, whole: 0.7464, core: 0.6661, enhancing: 0.8855
2018-05-24 08:19:02,952 Subject 54/57,   Brats17_TCIA_642_1, whole: 0.9519, core: 0.7395, enhancing: 0.1585
2018-05-24 08:19:13,983 Subject 55/57,   Brats17_TCIA_653_1, whole: 0.9569, core: 0.6367, enhancing: 0.8628
2018-05-24 08:19:24,929 Subject 56/57,   Brats17_TCIA_177_1, whole: 0.7639, core: 0.0000, enhancing: 0.0000
2018-05-24 08:19:35,905 Subject 57/57,   Brats17_TCIA_623_1, whole: 0.9089, core: 0.8211, enhancing: 0.0000
2018-05-24 08:19:35,906 Average scores: whole: 0.9075, core: 0.7922, enhancing: 0.6826
2018-05-24 08:19:36,000 --------------------------------------------------
2018-05-24 08:19:36,001 Iter 2280, Epoch 500.0000, validate training data
2018-05-24 09:01:31,760 Average scores: whole: 0.9149, core: 0.8932, enhancing: 0.7673
2018-05-24 09:01:31,817 total time: 52.4993 minutes
